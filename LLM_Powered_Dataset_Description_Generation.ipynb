{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xdpikaboo/CSGY_6033_Final_Project/blob/main/LLM_Powered_Dataset_Description_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "from datetime import datetime\n",
        "from google.colab import files\n",
        "from typing import Dict, Any, List, Tuple\n",
        "from openai import OpenAI\n",
        "import getpass"
      ],
      "metadata": {
        "id": "GrGBueMohvph"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DOWNLOAD TEST DATASET"
      ],
      "metadata": {
        "id": "_Rx7OiAwY_04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- 1) Load CSV ----------\n",
        "uploaded = files.upload()\n",
        "if not uploaded:\n",
        "    raise RuntimeError(\"No file uploaded.\")\n",
        "csv_name = list(uploaded.keys())[0]\n",
        "csv_bytes = uploaded[csv_name]\n",
        "df = pd.read_csv(io.BytesIO(csv_bytes), low_memory=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "e0s2TY4zXRtv",
        "outputId": "19e898e9-c946-4c3a-9250-4d80c48a439c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7b96c916-d271-4088-a792-ace6a1cdbc8a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7b96c916-d271-4088-a792-ace6a1cdbc8a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving NYC_Parks_Monuments.csv to NYC_Parks_Monuments (1).csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ENTER OpenAI API KEY"
      ],
      "metadata": {
        "id": "tg9jTqFH3RCa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t25550uY3PS9",
        "outputId": "5ed48da2-11a3-4507-ca54-3584a45b75c7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OpenAI API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DATA PROFILER"
      ],
      "metadata": {
        "id": "OM9jKlPxZIWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "=== Helper functions ===\n",
        "This cell defines small utility functions used by the profiler:\n",
        "- _try_to_datetime: safely attempts to convert a column to datetime\n",
        "- _safe_quantiles: compute quantiles for numeric columns (handles errors gracefully)\n",
        "- _histogram: produce histogram bin centers + counts for numeric data\n",
        "- _top_values: returns the most frequent categorical values + percentages\n",
        "- _text_stats: computes text length statistics for text-like columns\n",
        "\"\"\"\n",
        "\n",
        "def _try_to_datetime(series: pd.Series) -> Tuple[pd.Series, bool]:\n",
        "    if pd.api.types.is_datetime64_any_dtype(series):\n",
        "        return series, True\n",
        "    try:\n",
        "        s = pd.to_datetime(series, errors=\"raise\", infer_datetime_format=True)\n",
        "        return s, True\n",
        "    except Exception:\n",
        "        return series, False\n",
        "\n",
        "def _safe_quantiles(s: pd.Series, qs=(0.05, 0.25, 0.5, 0.75, 0.95)) -> Dict[str, float]:\n",
        "    try:\n",
        "        q = s.quantile(qs)\n",
        "        return {f\"q{int(p*100)}\": float(v) for p, v in q.items() if pd.notna(v)}\n",
        "    except Exception:\n",
        "        return {}\n",
        "\n",
        "def _histogram(s: pd.Series, bins: int = 10) -> Dict[str, Any]:\n",
        "    arr = pd.to_numeric(s, errors=\"coerce\").dropna().to_numpy()\n",
        "    if arr.size == 0 or np.nanmax(arr) == np.nanmin(arr):\n",
        "        return {\"bins\": [], \"counts\": []}\n",
        "    counts, edges = np.histogram(arr, bins=bins)\n",
        "    centers = (edges[:-1] + edges[1:]) / 2.0\n",
        "    return {\"bins\": centers.tolist(), \"counts\": counts.astype(int).tolist()}\n",
        "\n",
        "def _top_values(s: pd.Series, k: int = 10) -> List[Dict[str, Any]]:\n",
        "    vc = s.value_counts(dropna=True).head(k)\n",
        "    total = int(s.notna().sum())\n",
        "    return [{\"value\": str(v), \"count\": int(c), \"pct\": float(c/total) if total else 0.0}\n",
        "            for v, c in vc.items()]\n",
        "\n",
        "def _text_stats(s: pd.Series) -> Dict[str, Any]:\n",
        "    s2 = s.dropna().astype(str)\n",
        "    if s2.empty:\n",
        "        return {\"avg_length\": None, \"q25_length\": None, \"q50_length\": None, \"q75_length\": None}\n",
        "    lens = s2.str.len()\n",
        "    return {\n",
        "        \"avg_length\": float(lens.mean()),\n",
        "        \"q25_length\": float(lens.quantile(0.25)),\n",
        "        \"q50_length\": float(lens.quantile(0.50)),\n",
        "        \"q75_length\": float(lens.quantile(0.75)),\n",
        "    }\n"
      ],
      "metadata": {
        "id": "ZsjeGedLXVH5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "=== Semantic type inference ===\n",
        "This cell defines `_semantic_type`, which labels each column as:\n",
        "  boolean, integer, float, datetime, categorical, or text.\n",
        "The function uses dtype checks + heuristics (e.g., 80% successful datetime parsing).\n",
        "The output is more meaningful than raw pandas dtypes.\n",
        "\"\"\"\n",
        "\n",
        "def _semantic_type(series: pd.Series) -> str:\n",
        "    if pd.api.types.is_bool_dtype(series): return \"boolean\"\n",
        "    if pd.api.types.is_numeric_dtype(series):\n",
        "        return \"integer\" if pd.api.types.is_integer_dtype(series) else \"float\"\n",
        "    if pd.api.types.is_datetime64_any_dtype(series): return \"datetime\"\n",
        "\n",
        "    # try a light datetime guess on a small sample\n",
        "    sample = series.dropna().astype(str).head(50)\n",
        "    parsed = pd.to_datetime(sample, errors=\"coerce\", infer_datetime_format=True)\n",
        "    if parsed.notna().mean() >= 0.8:\n",
        "        return \"datetime\"\n",
        "\n",
        "    nunique = series.nunique(dropna=True)\n",
        "    ratio = nunique / max(len(series), 1)\n",
        "    return \"categorical\" if nunique <= 50 and ratio <= 0.5 else \"text\""
      ],
      "metadata": {
        "id": "ieJxqlyoUi4C"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "=== Geospatial + Key Detection ===\n",
        "This cell provides:\n",
        "  - _is_lat / _is_lon: detect latitude/longitude columns\n",
        "  - _geospatial_hints: determine whether dataset has lat/lon pair\n",
        "  - _primary_key_candidates: detect columns that are unique + non-null\n",
        "Used later to enrich dataset-level metadata.\n",
        "\"\"\"\n",
        "\n",
        "_LAT = {\"lat\", \"latitude\"}; _LON = {\"lon\", \"lng\", \"long\", \"longitude\"}\n",
        "\n",
        "def _is_lat(name: str, s: pd.Series) -> bool:\n",
        "    name = name.lower()\n",
        "    if name in _LAT and pd.api.types.is_numeric_dtype(s): return True\n",
        "    return pd.api.types.is_numeric_dtype(s) and s.min(skipna=True) >= -90 and s.max(skipna=True) <= 90\n",
        "\n",
        "def _is_lon(name: str, s: pd.Series) -> bool:\n",
        "    name = name.lower()\n",
        "    if name in _LON and pd.api.types.is_numeric_dtype(s): return True\n",
        "    return pd.api.types.is_numeric_dtype(s) and s.min(skipna=True) >= -180 and s.max(skipna=True) <= 180\n",
        "\n",
        "def _geospatial_hints(df: pd.DataFrame) -> Dict[str, Any]:\n",
        "    lat = next((c for c in df.columns if _is_lat(c, df[c])), None)\n",
        "    lon = next((c for c in df.columns if _is_lon(c, df[c]) and c != lat), None)\n",
        "    return {\"lat_column\": lat, \"lon_column\": lon, \"has_latlon_pair\": lat is not None and lon is not None}\n",
        "\n",
        "def _primary_key_candidates(df: pd.DataFrame) -> List[str]:\n",
        "    return [c for c in df.columns if df[c].isna().mean() == 0 and df[c].is_unique]"
      ],
      "metadata": {
        "id": "ktdQ0OQhVR81"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "=== Full Data Profiler ===\n",
        "This cell defines `profile_dataframe(df)` which:\n",
        "  - Profiles each column (stats, missingness, semantic type, distributions, etc.)\n",
        "  - Computes dataset-level metadata (row count, column count, pk candidates, geospatial hints)\n",
        "  - Returns everything as a structured JSON-friendly dictionary.\n",
        "This is the main profiling engine.\n",
        "\"\"\"\n",
        "def profile_dataframe(\n",
        "    df: pd.DataFrame,\n",
        "    *,\n",
        "    sample_rows: int = 1_000_000,\n",
        "    hist_bins: int = 10,\n",
        "    topk_categorical: int = 10,\n",
        ") -> Dict[str, Any]:\n",
        "    # downsample (row-wise) for very large inputs\n",
        "    if len(df) > sample_rows:\n",
        "        df = df.sample(sample_rows, random_state=42)\n",
        "\n",
        "    n_rows, n_cols = df.shape\n",
        "    mem_bytes = int(df.memory_usage(deep=True).sum())\n",
        "\n",
        "    column_summaries = {}\n",
        "    type_counts: Dict[str, int] = {}\n",
        "\n",
        "    for c in df.columns:\n",
        "        s = df[c]\n",
        "        sem = _semantic_type(s)\n",
        "        type_counts[sem] = type_counts.get(sem, 0) + 1\n",
        "\n",
        "        non_null = int(s.notna().sum())\n",
        "        null_frac = float(s.isna().mean())\n",
        "        nunique = int(s.nunique(dropna=True))\n",
        "        is_unique = s.is_unique and null_frac == 0.0\n",
        "        is_const = nunique == 1\n",
        "\n",
        "        info: Dict[str, Any] = {\n",
        "            \"dtype\": str(s.dtype),\n",
        "            \"semantic_type\": sem,\n",
        "            \"non_null_count\": non_null,\n",
        "            \"null_fraction\": null_frac,\n",
        "            \"n_unique\": nunique,\n",
        "            \"is_unique\": bool(is_unique),\n",
        "            \"is_constant\": bool(is_const),\n",
        "            \"example_values\": [str(v) for v in s.dropna().unique()[:5]],\n",
        "        }\n",
        "\n",
        "        if sem in {\"integer\", \"float\"}:\n",
        "            s_num = pd.to_numeric(s, errors=\"coerce\")\n",
        "            info.update({\n",
        "                \"min\": None if s_num.dropna().empty else float(s_num.min()),\n",
        "                \"max\": None if s_num.dropna().empty else float(s_num.max()),\n",
        "                \"mean\": None if s_num.dropna().empty else float(s_num.mean()),\n",
        "                \"std\": None if s_num.dropna().empty else float(s_num.std(ddof=1)),\n",
        "                \"quantiles\": _safe_quantiles(s_num),\n",
        "                \"histogram\": _histogram(s_num, bins=hist_bins),\n",
        "            })\n",
        "\n",
        "        elif sem == \"datetime\":\n",
        "            s_dt, ok = _try_to_datetime(s)\n",
        "            if ok and not s_dt.dropna().empty:\n",
        "                info.update({\n",
        "                    \"min\": s_dt.min(),\n",
        "                    \"max\": s_dt.max(),\n",
        "                    \"is_monotonic_increasing\": bool(s_dt.is_monotonic_increasing),\n",
        "                    \"is_monotonic_decreasing\": bool(s_dt[::-1].is_monotonic_increasing),\n",
        "                })\n",
        "\n",
        "        elif sem == \"categorical\":\n",
        "            info[\"top_values\"] = _top_values(s.astype(\"string\"), k=topk_categorical)\n",
        "\n",
        "        elif sem == \"text\":\n",
        "            info[\"text_stats\"] = _text_stats(s)\n",
        "\n",
        "        column_summaries[c] = info\n",
        "\n",
        "    # dataset-level\n",
        "    geohints = _geospatial_hints(df)\n",
        "    pk = _primary_key_candidates(df)\n",
        "\n",
        "    time_min, time_max = None, None\n",
        "    for c, info in column_summaries.items():\n",
        "        if info[\"semantic_type\"] == \"datetime\" and info.get(\"min\") is not None:\n",
        "            tmin, tmax = info[\"min\"], info[\"max\"]\n",
        "            time_min = tmin if time_min is None or (tmin is not None and tmin < time_min) else time_min\n",
        "            time_max = tmax if time_max is None or (tmax is not None and tmax > time_max) else time_max\n",
        "\n",
        "    return {\n",
        "        \"num_rows\": int(n_rows),\n",
        "        \"num_columns\": int(n_cols),\n",
        "        \"memory_usage_bytes\": mem_bytes,\n",
        "        \"duplicate_row_fraction\": float((n_rows - len(df.drop_duplicates())) / n_rows) if n_rows else 0.0,\n",
        "        \"type_counts\": type_counts,\n",
        "        \"time_coverage\": {\"min\": time_min, \"max\": time_max},\n",
        "        \"geospatial_hints\": geohints,\n",
        "        \"primary_key_candidates\": pk,\n",
        "        \"columns\": column_summaries,\n",
        "    }"
      ],
      "metadata": {
        "id": "s7nkjHsyVYMz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA SIGNAL"
      ],
      "metadata": {
        "id": "YJtArOz4ZRzt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "=== Build Data Signals ===\n",
        "This cell defines `build_data_signals(profile)` which:\n",
        "  • Takes the raw profiler output\n",
        "  • Aggregates it into simplified dataset-level signals\n",
        "    (semantic type counts, missingness fraction, PKs, lat/lon presence, etc.)\n",
        "  • Produces a compact summary dictionary intended for LLM input\n",
        "    BEFORE generating natural-language summaries.\n",
        "\"\"\"\n",
        "def build_data_signals(profile: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Turn the raw profiler output into higher-level 'data signals'\n",
        "    that are easier for an LLM to consume.\n",
        "    \"\"\"\n",
        "    columns = profile.get(\"columns\", {})\n",
        "    dataset = profile.get(\"dataset\", {})\n",
        "\n",
        "    # Handle both naming variants just in case\n",
        "    n_rows = dataset.get(\"n_rows\") or dataset.get(\"num_rows\")\n",
        "    n_cols = dataset.get(\"n_cols\") or dataset.get(\"num_columns\")\n",
        "\n",
        "    # ---- Dataset-level signals ----\n",
        "    # Count semantic types\n",
        "    type_counts: Dict[str, int] = {}\n",
        "    total_missing = 0\n",
        "    for name, info in columns.items():\n",
        "        sem = info.get(\"semantic_type\")\n",
        "        if sem:\n",
        "            type_counts[sem] = type_counts.get(sem, 0) + 1\n",
        "        total_missing += int(info.get(\"n_missing\", 0))\n",
        "\n",
        "    total_cells = n_rows * n_cols if (n_rows is not None and n_cols is not None) else None\n",
        "    overall_missing_fraction = (\n",
        "        float(total_missing / total_cells) if total_cells and total_cells > 0 else None\n",
        "    )\n",
        "\n",
        "    # geospatial + primary keys may or may not exist depending on your profiler version\n",
        "    geospatial = dataset.get(\"geospatial\") or dataset.get(\"geospatial_hints\") or {}\n",
        "    pk_candidates = dataset.get(\"primary_keys\") or dataset.get(\"primary_key_candidates\") or []\n",
        "\n",
        "    dataset_signals: Dict[str, Any] = {\n",
        "        \"n_rows\": n_rows,\n",
        "        \"n_cols\": n_cols,\n",
        "        \"type_counts\": type_counts,                      # how many numeric / text / datetime / etc\n",
        "        \"overall_missing_fraction\": overall_missing_fraction,\n",
        "        \"has_geospatial\": bool(geospatial.get(\"has_latlon_pair\")),\n",
        "        \"lat_column\": geospatial.get(\"lat_column\"),\n",
        "        \"lon_column\": geospatial.get(\"lon_column\"),\n",
        "        \"primary_key_candidates\": pk_candidates,\n",
        "    }\n",
        "\n",
        "    # ---- Per-column signals ----\n",
        "    column_signals: List[Dict[str, Any]] = []\n",
        "\n",
        "    for name, info in columns.items():\n",
        "        sem = info.get(\"semantic_type\")\n",
        "        n_missing = int(info.get(\"n_missing\", 0))\n",
        "        nunique = int(info.get(\"nunique\") or info.get(\"n_unique\", 0))\n",
        "\n",
        "        missing_fraction = float(n_missing / n_rows) if n_rows and n_rows > 0 else 0.0\n",
        "\n",
        "        # example values from top_values (if present)\n",
        "        top_vals = info.get(\"top_values\") or []\n",
        "        example_values = [tv.get(\"value\") for tv in top_vals[:3]] if isinstance(top_vals, list) else []\n",
        "\n",
        "        # use 5% and 95% quantiles as an approximate range, if numeric\n",
        "        quantiles = info.get(\"quantiles\") or {}\n",
        "        approx_min = quantiles.get(\"0.05\")\n",
        "        approx_max = quantiles.get(\"0.95\")\n",
        "\n",
        "        # simple heuristics for \"id-like\" and \"binary\" columns\n",
        "        is_id_like = bool(n_rows and nunique == n_rows and missing_fraction == 0.0)\n",
        "        is_binary = bool(\n",
        "            nunique == 2\n",
        "            and sem in {\"integer\", \"float\", \"boolean\", \"categorical\"}\n",
        "        )\n",
        "\n",
        "        column_signals.append(\n",
        "            {\n",
        "                \"name\": name,\n",
        "                \"semantic_type\": sem,\n",
        "                \"missing_fraction\": missing_fraction,\n",
        "                \"n_unique\": nunique,\n",
        "                \"is_id_like\": is_id_like,           # good candidate for primary key / identifier\n",
        "                \"is_binary\": is_binary,             # good for yes/no flags\n",
        "                \"approx_min\": approx_min,           # for numeric/datetime: rough range\n",
        "                \"approx_max\": approx_max,\n",
        "                \"example_values\": example_values,   # a few sample values as strings\n",
        "            }\n",
        "        )\n",
        "\n",
        "    return {\n",
        "        \"dataset_signals\": dataset_signals,\n",
        "        \"column_signals\": column_signals,\n",
        "    }"
      ],
      "metadata": {
        "id": "qHajMvViXXNj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SUMMARY GENERATOR (LLM)"
      ],
      "metadata": {
        "id": "yWjDFWHcWBZj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_dataset_with_llm(\n",
        "    profile: Dict[str, Any],\n",
        "    signals: Dict[str, Any],\n",
        "    model: str = \"gpt-4o-mini\"\n",
        ") -> str:\n",
        "\n",
        "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "    # ---- Convert dict → JSON ----\n",
        "    dataset_summary_json = json.dumps(\n",
        "        {\n",
        "            \"dataset_signals\": profile,\n",
        "            \"column_signals\": signals.get(\"column_signals\", [])\n",
        "        },\n",
        "        indent=2,\n",
        "        default=str\n",
        "    )\n",
        "\n",
        "    # ---- Dataset-card prompt ----\n",
        "    user_prompt = f\"\"\"\n",
        "        You are a data documentation assistant.\n",
        "\n",
        "        A machine-generated content summary of a tabular dataset in this JSON format:\n",
        "\n",
        "        {dataset_summary_json}\n",
        "\n",
        "        The JSON has keys such as:\n",
        "        - \"dataset_signals\" (n_rows, n_cols, type_counts, overall_missing_fraction, has_geospatial, etc.)\n",
        "        - \"column_signals\": a list of columns with fields like name, semantic_type, missing_fraction, n_unique, example_values, etc.\n",
        "\n",
        "        Your job is to write a VERY concise “dataset card” style description that:\n",
        "        - Uses the JSON only to infer what the data contains and how it can be used.\n",
        "        - Uses the user fields to tailor the framing to their interest.\n",
        "        - Does NOT invent specific values (dates, row counts, monetary amounts, percentages) if they are not clearly implied by the JSON.\n",
        "\n",
        "        =====================\n",
        "        OUTPUT REQUIREMENTS\n",
        "        =====================\n",
        "\n",
        "        You MUST output in JSON format and follow this exact structure and HARD length caps:\n",
        "\n",
        "        1) Section: \"Key facts\"\n",
        "           - Output 3–5 bullet points.\n",
        "           - Each bullet MUST be ≤ 18 words.\n",
        "           - Focus on: what each row roughly represents, main entities, key columns, important datatypes (dates, geo, text), and granularity.\n",
        "           - If counts (rows, columns) are missing or null, speak qualitatively (e.g., \"many records\") instead of making them up.\n",
        "        - for column names, only output column names separated by commas, nothing else\n",
        "\n",
        "        2) Section: \"Use-cases\"\n",
        "           - Output 3–5 bullet points.\n",
        "           - Each bullet MUST be ≤ 18 words.\n",
        "           - Tailor to TOPIC and POTENTIAL_ANALYSES.\n",
        "           - Include at least one analytical idea and one operational / business question.\n",
        "\n",
        "        3) Section: \"Caveats\"\n",
        "           - Output 3–5 bullet points.\n",
        "           - Each bullet MUST be ≤ 18 words.\n",
        "           - Mention schema limitations, missingness, lack of true coordinates, time coverage uncertainty, and any obvious biases from the columns.\n",
        "           - If TIME_AND_GEO_COVERAGE is broader than what the schema suggests, flag that as a potential mismatch.\n",
        "\n",
        "        4) Section: \"Overview\"\n",
        "           - A single paragraph (NO bullets).\n",
        "           - The paragraph MUST be between 40 and 80 words.\n",
        "           - Summarize: what the dataset is about, likely time/geo scope (using TIME_AND_GEO_COVERAGE where appropriate), and typical analyses.\n",
        "           - Mention TOPIC naturally, but do not restate all details from above bullet sections.\n",
        "\n",
        "        =====================\n",
        "        STYLE & CONSTRAINTS\n",
        "        =====================\n",
        "\n",
        "        - Do NOT exceed any of the word caps, even if you must omit information.\n",
        "        - Do NOT mention internal JSON structure, keys, or field names like \"dataset_signals\" or \"column_signals\".\n",
        "        - Do NOT say \"the JSON says\" or \"the schema indicates\"; write as a normal human-facing description.\n",
        "        - Do NOT fabricate: if something is unknown or ambiguous from the JSON, either omit it or state it as uncertain.\n",
        "        - Be clear and non-technical; assume an analytically literate but non-expert reader.\n",
        "        - Output ONLY the four sections in this order, with these exact headings:\n",
        "\n",
        "        Key facts\n",
        "        Use-cases\n",
        "        Caveats\n",
        "        Overview\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "    response = client.responses.create(\n",
        "        model=model,\n",
        "        input=user_prompt,\n",
        "    )\n",
        "\n",
        "    return response.output_text"
      ],
      "metadata": {
        "id": "eTYV10_IVR4H"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "profile = profile_dataframe(df)\n",
        "signals = build_data_signals(profile)\n",
        "summary = summarize_dataset_with_llm(profile, signals)\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqzWqyxfK-ne",
        "outputId": "e343f5ee-b0ee-4cd7-9595-98ac83b971c8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-977247162.py:17: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
            "  parsed = pd.to_datetime(sample, errors=\"coerce\", infer_datetime_format=True)\n",
            "/tmp/ipython-input-977247162.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  parsed = pd.to_datetime(sample, errors=\"coerce\", infer_datetime_format=True)\n",
            "/tmp/ipython-input-977247162.py:17: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
            "  parsed = pd.to_datetime(sample, errors=\"coerce\", infer_datetime_format=True)\n",
            "/tmp/ipython-input-977247162.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  parsed = pd.to_datetime(sample, errors=\"coerce\", infer_datetime_format=True)\n",
            "/tmp/ipython-input-977247162.py:17: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
            "  parsed = pd.to_datetime(sample, errors=\"coerce\", infer_datetime_format=True)\n",
            "/tmp/ipython-input-977247162.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  parsed = pd.to_datetime(sample, errors=\"coerce\", infer_datetime_format=True)\n",
            "/tmp/ipython-input-977247162.py:17: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
            "  parsed = pd.to_datetime(sample, errors=\"coerce\", infer_datetime_format=True)\n",
            "/tmp/ipython-input-977247162.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  parsed = pd.to_datetime(sample, errors=\"coerce\", infer_datetime_format=True)\n",
            "/tmp/ipython-input-977247162.py:17: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
            "  parsed = pd.to_datetime(sample, errors=\"coerce\", infer_datetime_format=True)\n",
            "/tmp/ipython-input-977247162.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  parsed = pd.to_datetime(sample, errors=\"coerce\", infer_datetime_format=True)\n",
            "/tmp/ipython-input-977247162.py:17: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
            "  parsed = pd.to_datetime(sample, errors=\"coerce\", infer_datetime_format=True)\n",
            "/tmp/ipython-input-977247162.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  parsed = pd.to_datetime(sample, errors=\"coerce\", infer_datetime_format=True)\n",
            "/tmp/ipython-input-977247162.py:17: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
            "  parsed = pd.to_datetime(sample, errors=\"coerce\", infer_datetime_format=True)\n",
            "/tmp/ipython-input-977247162.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  parsed = pd.to_datetime(sample, errors=\"coerce\", infer_datetime_format=True)\n",
            "/tmp/ipython-input-977247162.py:17: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
            "  parsed = pd.to_datetime(sample, errors=\"coerce\", infer_datetime_format=True)\n",
            "/tmp/ipython-input-977247162.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  parsed = pd.to_datetime(sample, errors=\"coerce\", infer_datetime_format=True)\n",
            "/tmp/ipython-input-977247162.py:17: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
            "  parsed = pd.to_datetime(sample, errors=\"coerce\", infer_datetime_format=True)\n",
            "/tmp/ipython-input-977247162.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  parsed = pd.to_datetime(sample, errors=\"coerce\", infer_datetime_format=True)\n",
            "/tmp/ipython-input-3160278674.py:15: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
            "  s = pd.to_datetime(series, errors=\"raise\", infer_datetime_format=True)\n",
            "/tmp/ipython-input-3160278674.py:15: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  s = pd.to_datetime(series, errors=\"raise\", infer_datetime_format=True)\n",
            "/tmp/ipython-input-977247162.py:17: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
            "  parsed = pd.to_datetime(sample, errors=\"coerce\", infer_datetime_format=True)\n",
            "/tmp/ipython-input-977247162.py:17: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
            "  parsed = pd.to_datetime(sample, errors=\"coerce\", infer_datetime_format=True)\n",
            "/tmp/ipython-input-977247162.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  parsed = pd.to_datetime(sample, errors=\"coerce\", infer_datetime_format=True)\n",
            "/tmp/ipython-input-977247162.py:17: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
            "  parsed = pd.to_datetime(sample, errors=\"coerce\", infer_datetime_format=True)\n",
            "/tmp/ipython-input-977247162.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  parsed = pd.to_datetime(sample, errors=\"coerce\", infer_datetime_format=True)\n",
            "/tmp/ipython-input-977247162.py:17: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
            "  parsed = pd.to_datetime(sample, errors=\"coerce\", infer_datetime_format=True)\n",
            "/tmp/ipython-input-977247162.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  parsed = pd.to_datetime(sample, errors=\"coerce\", infer_datetime_format=True)\n",
            "/tmp/ipython-input-977247162.py:17: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
            "  parsed = pd.to_datetime(sample, errors=\"coerce\", infer_datetime_format=True)\n",
            "/tmp/ipython-input-977247162.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  parsed = pd.to_datetime(sample, errors=\"coerce\", infer_datetime_format=True)\n",
            "/tmp/ipython-input-977247162.py:17: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
            "  parsed = pd.to_datetime(sample, errors=\"coerce\", infer_datetime_format=True)\n",
            "/tmp/ipython-input-977247162.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  parsed = pd.to_datetime(sample, errors=\"coerce\", infer_datetime_format=True)\n",
            "/tmp/ipython-input-977247162.py:17: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
            "  parsed = pd.to_datetime(sample, errors=\"coerce\", infer_datetime_format=True)\n",
            "/tmp/ipython-input-977247162.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  parsed = pd.to_datetime(sample, errors=\"coerce\", infer_datetime_format=True)\n",
            "/tmp/ipython-input-977247162.py:17: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
            "  parsed = pd.to_datetime(sample, errors=\"coerce\", infer_datetime_format=True)\n",
            "/tmp/ipython-input-977247162.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  parsed = pd.to_datetime(sample, errors=\"coerce\", infer_datetime_format=True)\n",
            "/tmp/ipython-input-977247162.py:17: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
            "  parsed = pd.to_datetime(sample, errors=\"coerce\", infer_datetime_format=True)\n",
            "/tmp/ipython-input-977247162.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  parsed = pd.to_datetime(sample, errors=\"coerce\", infer_datetime_format=True)\n",
            "/tmp/ipython-input-977247162.py:17: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
            "  parsed = pd.to_datetime(sample, errors=\"coerce\", infer_datetime_format=True)\n",
            "/tmp/ipython-input-977247162.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  parsed = pd.to_datetime(sample, errors=\"coerce\", infer_datetime_format=True)\n",
            "/tmp/ipython-input-977247162.py:17: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
            "  parsed = pd.to_datetime(sample, errors=\"coerce\", infer_datetime_format=True)\n",
            "/tmp/ipython-input-977247162.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  parsed = pd.to_datetime(sample, errors=\"coerce\", infer_datetime_format=True)\n",
            "/tmp/ipython-input-977247162.py:17: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
            "  parsed = pd.to_datetime(sample, errors=\"coerce\", infer_datetime_format=True)\n",
            "/tmp/ipython-input-977247162.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  parsed = pd.to_datetime(sample, errors=\"coerce\", infer_datetime_format=True)\n",
            "/tmp/ipython-input-977247162.py:17: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
            "  parsed = pd.to_datetime(sample, errors=\"coerce\", infer_datetime_format=True)\n",
            "/tmp/ipython-input-977247162.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  parsed = pd.to_datetime(sample, errors=\"coerce\", infer_datetime_format=True)\n",
            "/tmp/ipython-input-977247162.py:17: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
            "  parsed = pd.to_datetime(sample, errors=\"coerce\", infer_datetime_format=True)\n",
            "/tmp/ipython-input-977247162.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  parsed = pd.to_datetime(sample, errors=\"coerce\", infer_datetime_format=True)\n",
            "/tmp/ipython-input-977247162.py:17: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
            "  parsed = pd.to_datetime(sample, errors=\"coerce\", infer_datetime_format=True)\n",
            "/tmp/ipython-input-977247162.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  parsed = pd.to_datetime(sample, errors=\"coerce\", infer_datetime_format=True)\n",
            "/tmp/ipython-input-977247162.py:17: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
            "  parsed = pd.to_datetime(sample, errors=\"coerce\", infer_datetime_format=True)\n",
            "/tmp/ipython-input-3160278674.py:15: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
            "  s = pd.to_datetime(series, errors=\"raise\", infer_datetime_format=True)\n",
            "/tmp/ipython-input-977247162.py:17: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
            "  parsed = pd.to_datetime(sample, errors=\"coerce\", infer_datetime_format=True)\n",
            "/tmp/ipython-input-3160278674.py:15: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
            "  s = pd.to_datetime(series, errors=\"raise\", infer_datetime_format=True)\n",
            "/tmp/ipython-input-977247162.py:17: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
            "  parsed = pd.to_datetime(sample, errors=\"coerce\", infer_datetime_format=True)\n",
            "/tmp/ipython-input-977247162.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  parsed = pd.to_datetime(sample, errors=\"coerce\", infer_datetime_format=True)\n",
            "/tmp/ipython-input-977247162.py:17: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
            "  parsed = pd.to_datetime(sample, errors=\"coerce\", infer_datetime_format=True)\n",
            "/tmp/ipython-input-977247162.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  parsed = pd.to_datetime(sample, errors=\"coerce\", infer_datetime_format=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "{\n",
            "  \"Key facts\": [\n",
            "    \"Dataset contains many records related to parks and sculptures.\",\n",
            "    \"Key columns include name, borough, parkname, and location.\",\n",
            "    \"Dates and text fields dominate, with some numerical data.\",\n",
            "    \"Geospatial information is present but lacks precise coordinates.\",\n",
            "    \"Time coverage spans from the 1930s to 2021.\"\n",
            "  ],\n",
            "  \"Use-cases\": [\n",
            "    \"Analyze park distribution across boroughs over time.\",\n",
            "    \"Examine correlations between materials used and sculpture types.\",\n",
            "    \"Identify trends in park maintenance and funding.\",\n",
            "    \"Assess public engagement through community boards.\",\n",
            "    \"Explore historical significance of sculptures in different neighborhoods.\"\n",
            "  ],\n",
            "  \"Caveats\": [\n",
            "    \"High fraction of missing data in several columns.\",\n",
            "    \"Not all geographic locations have true coordinates.\",\n",
            "    \"Time coverage has some uncertainty regarding earlier records.\",\n",
            "    \"Potential bias in sculpture representations across boroughs.\",\n",
            "    \"Some columns may have overlapping categories or definitions.\"\n",
            "  ],\n",
            "  \"Overview\": \"This dataset focuses on parks and sculptures, reflecting diverse historical and cultural contexts within boroughs. Spanning from the 1930s to 2021, the data includes many attributes like names, materials, and boroughs, enabling various analyses of trends, maintenance, and public engagement. However, some entries lack precise geospatial data, and several columns exhibit high missingness.\"\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    }
  ]
}