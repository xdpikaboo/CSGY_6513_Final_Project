{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-fbtv8hOqQlp",
    "outputId": "6cae727c-c721-483d-ca84-6588abf63bb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: boto3 in ./.local/lib/python3.11/site-packages (1.42.5)\n",
      "Requirement already satisfied: botocore<1.43.0,>=1.42.5 in ./.local/lib/python3.11/site-packages (from boto3) (1.42.5)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in ./.local/lib/python3.11/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.17.0,>=0.16.0 in ./.local/lib/python3.11/site-packages (from boto3) (0.16.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.11/site-packages (from botocore<1.43.0,>=1.42.5->boto3) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/conda/lib/python3.11/site-packages (from botocore<1.43.0,>=1.42.5->boto3) (2.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.43.0,>=1.42.5->boto3) (1.17.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: bert_score in ./.local/lib/python3.11/site-packages (0.3.13)\n",
      "Requirement already satisfied: torch>=1.0.0 in ./.local/lib/python3.11/site-packages (from bert_score) (2.9.1)\n",
      "Requirement already satisfied: pandas>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from bert_score) (2.1.4)\n",
      "Requirement already satisfied: transformers>=3.0.0 in ./.local/lib/python3.11/site-packages (from bert_score) (4.57.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from bert_score) (1.26.4)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from bert_score) (2.32.2)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in /opt/conda/lib/python3.11/site-packages (from bert_score) (4.67.1)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.11/site-packages (from bert_score) (3.8.4)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.11/site-packages (from bert_score) (25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.0.1->bert_score) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.0.1->bert_score) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.0.1->bert_score) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.17.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (1.13.3)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (2023.12.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./.local/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./.local/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./.local/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./.local/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./.local/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./.local/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./.local/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./.local/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./.local/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./.local/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in ./.local/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in ./.local/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./.local/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./.local/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./.local/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in ./.local/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (3.5.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=1.0.0->bert_score) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./.local/lib/python3.11/site-packages (from transformers>=3.0.0->bert_score) (0.36.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers>=3.0.0->bert_score) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers>=3.0.0->bert_score) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./.local/lib/python3.11/site-packages (from transformers>=3.0.0->bert_score) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.local/lib/python3.11/site-packages (from transformers>=3.0.0->bert_score) (0.7.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.local/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=3.0.0->bert_score) (1.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.0.0->bert_score) (3.0.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib->bert_score) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib->bert_score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib->bert_score) (4.59.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib->bert_score) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.11/site-packages (from matplotlib->bert_score) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib->bert_score) (3.2.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->bert_score) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->bert_score) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->bert_score) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->bert_score) (2025.8.3)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in ./.local/lib/python3.11/site-packages (2.9.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.11/site-packages (from openai) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.local/lib/python3.11/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in ./.local/lib/python3.11/site-packages (from openai) (0.12.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./.local/lib/python3.11/site-packages (from openai) (2.12.4)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.11/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /opt/conda/lib/python3.11/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/conda/lib/python3.11/site-packages (from openai) (4.14.1)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in ./.local/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./.local/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in ./.local/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in ./.local/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gradio in ./.local/lib/python3.11/site-packages (6.0.2)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in /opt/conda/lib/python3.11/site-packages (from gradio) (22.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /opt/conda/lib/python3.11/site-packages (from gradio) (3.7.1)\n",
      "Requirement already satisfied: brotli>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from gradio) (1.1.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in ./.local/lib/python3.11/site-packages (from gradio) (0.124.0)\n",
      "Requirement already satisfied: ffmpy in ./.local/lib/python3.11/site-packages (from gradio) (1.0.0)\n",
      "Requirement already satisfied: gradio-client==2.0.1 in ./.local/lib/python3.11/site-packages (from gradio) (2.0.1)\n",
      "Requirement already satisfied: groovy~=0.1 in ./.local/lib/python3.11/site-packages (from gradio) (0.1.2)\n",
      "Requirement already satisfied: httpx<1.0,>=0.24.1 in ./.local/lib/python3.11/site-packages (from gradio) (0.28.1)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in ./.local/lib/python3.11/site-packages (from gradio) (0.36.0)\n",
      "Requirement already satisfied: jinja2<4.0 in /opt/conda/lib/python3.11/site-packages (from gradio) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in /opt/conda/lib/python3.11/site-packages (from gradio) (3.0.2)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from gradio) (1.26.4)\n",
      "Requirement already satisfied: orjson~=3.0 in ./.local/lib/python3.11/site-packages (from gradio) (3.11.5)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from gradio) (25.0)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from gradio) (2.1.4)\n",
      "Requirement already satisfied: pillow<13.0,>=8.0 in /opt/conda/lib/python3.11/site-packages (from gradio) (11.3.0)\n",
      "Requirement already satisfied: pydantic<=2.12.4,>=2.11.10 in ./.local/lib/python3.11/site-packages (from gradio) (2.12.4)\n",
      "Requirement already satisfied: pydub in ./.local/lib/python3.11/site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in ./.local/lib/python3.11/site-packages (from gradio) (0.0.20)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /opt/conda/lib/python3.11/site-packages (from gradio) (6.0.2)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.7 in ./.local/lib/python3.11/site-packages (from gradio) (0.1.7)\n",
      "Requirement already satisfied: semantic-version~=2.0 in ./.local/lib/python3.11/site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in ./.local/lib/python3.11/site-packages (from gradio) (0.50.0)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /opt/conda/lib/python3.11/site-packages (from gradio) (0.13.3)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in ./.local/lib/python3.11/site-packages (from gradio) (0.20.0)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /opt/conda/lib/python3.11/site-packages (from gradio) (4.14.1)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in ./.local/lib/python3.11/site-packages (from gradio) (0.38.0)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from gradio-client==2.0.1->gradio) (2023.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.11/site-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.11/site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in ./.local/lib/python3.11/site-packages (from fastapi<1.0,>=0.115.2->gradio) (0.0.4)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from httpx<1.0,>=0.24.1->gradio) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in ./.local/lib/python3.11/site-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./.local/lib/python3.11/site-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (3.19.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (2.32.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (4.67.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.local/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.11/site-packages (from pydantic<=2.12.4,>=2.11.10->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in ./.local/lib/python3.11/site-packages (from pydantic<=2.12.4,>=2.11.10->gradio) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in ./.local/lib/python3.11/site-packages (from pydantic<=2.12.4,>=2.11.10->gradio) (0.4.2)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.11/site-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in ./.local/lib/python3.11/site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.11/site-packages (from typer<1.0,>=0.12->gradio) (14.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (2.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install boto3\n",
    "!pip install rouge-score evaluate -q\n",
    "!pip install bert_score\n",
    "!pip -q install faiss-cpu sentence-transformers\n",
    "!pip install openai\n",
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "GrGBueMohvph"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "from openai import OpenAI\n",
    "import getpass\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fJ7vxRaB-CEu"
   },
   "source": [
    "#AWS S3 ACCESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0g33gWcULsyb"
   },
   "source": [
    "Our S3 bucket, `nyu-tandon-cs-gy-6513-project`, contains approximately 1,300+ datasets - including 200+ datasets downloaded via Auctus and 1,100+ datasets from NYC Open Data.\n",
    "\n",
    "The bucket is organized into two top-level directories: `auctus` and `nyc_open_data`.\n",
    "\n",
    "\n",
    "**Auctus directory**\n",
    "\n",
    "The auctus directory contains datasets downloaded from the Auctus platform and is divided into two subdirectories:\n",
    "\n",
    "- `auctus/dataset_no_description` ‚Äî datasets that do not have descriptions (because descriptions were not available via Auctus).\n",
    "\n",
    "- `auctus/dataset_has_description` ‚Äî datasets with descriptions. Each dataset in this directory has a corresponding JSON file containing its description summary, using the same base filename as the dataset.\n",
    "\n",
    "**NYC Open Data directory**\n",
    "\n",
    "The `nyc_open_data` directory contains all datasets downloaded from NYC Open Data. These datasets all include descriptions, which are consolidated in a single file named `dataset_catalog.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"\"\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "os.environ[\"HF_TOKEN\"] = \"\"\n",
    "\n",
    "openaimodel = \"gpt-4.1-2025-04-14\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9bEDukWY_sHL"
   },
   "source": [
    "API KEYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FxEaRBVr95dr",
    "outputId": "786e61e3-8446-4d56-d909-07340d6c16f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWS_ACCESS_KEY_ID already set in environment.\n",
      "AWS_SECRET_ACCESS_KEY already set in environment.\n",
      "OPENAI_API_KEY already set in environment.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    HAVE_DOTENV = True\n",
    "except ImportError:\n",
    "    HAVE_DOTENV = False\n",
    "\n",
    "\n",
    "def ensure_env_local(env_name, prompt_text=None):\n",
    "    \"\"\"\n",
    "    Ensure an environment variable is set for local notebooks:\n",
    "    1) If already in os.environ, keep it.\n",
    "    2) Else, if python-dotenv is available, attempt to load from .env.\n",
    "    3) Else, prompt user securely via getpass.\n",
    "    \"\"\"\n",
    "    if os.environ.get(env_name):\n",
    "        print(f\"{env_name} already set in environment.\")\n",
    "        return\n",
    "\n",
    "    # If dotenv exists, it may have been loaded above.\n",
    "    if HAVE_DOTENV and os.environ.get(env_name):\n",
    "        print(f\"üîê Loaded {env_name} from .env.\")\n",
    "        return\n",
    "\n",
    "    if prompt_text is None:\n",
    "        prompt_text = f\"Enter {env_name}: \"\n",
    "\n",
    "    os.environ[env_name] = getpass.getpass(prompt_text)\n",
    "    print(f\"Set {env_name} from manual input.\")\n",
    "\n",
    "\n",
    "ensure_env_local(\"AWS_ACCESS_KEY_ID\")\n",
    "ensure_env_local(\"AWS_SECRET_ACCESS_KEY\")\n",
    "ensure_env_local(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Model selection\n",
    "openaimodel = \"gpt-4.1-2025-04-14\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "i02XoNwP89N7"
   },
   "outputs": [],
   "source": [
    "# Initialize S3 client\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    aws_access_key_id=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "    aws_secret_access_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "    region_name=\"us-east-1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UIg9_Z-R-xKy",
    "outputId": "2d0f33c1-22f2-432b-e1be-c76cb029059e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auctus/\n",
      "auctus/dataset_descriptions/2007_-_2008_School_Progress_Reports_-_All_Schools_description.json\n",
      "auctus/dataset_descriptions/Atomic_Polygons_description.json\n",
      "auctus/dataset_descriptions/Ballfields_9K_description.json\n",
      "auctus/dataset_descriptions/Bike_Ridership_-_Edmonton_Insight_Community_description.json\n",
      "auctus/dataset_descriptions/Chicago_Park_District_Activities_description.json\n",
      "auctus/dataset_descriptions/Chicago_Park_District__Movies_in_the_Parks_2014_description.json\n",
      "auctus/dataset_descriptions/Chicago_Park_District__Movies_in_the_Parks_2016_description.json\n",
      "auctus/dataset_descriptions/Chicago_Park_District__Movies_in_the_Parks_2017_description.json\n",
      "auctus/dataset_descriptions/Chicago_Park_District__Movies_in_the_Parks_2018_description.json\n",
      "auctus/dataset_descriptions/Chicago_Public_Schools_-_School_Admissions_Information_SY1819_description.json\n",
      "auctus/dataset_descriptions/Chicago_Public_Schools_-_School_Admissions_Information_SY2324_description.json\n",
      "auctus/dataset_descriptions/Chicago_Public_Schools_-_School_Locations_SY2122_description.json\n",
      "auctus/dataset_descriptions/Chicago_Public_Schools_-_School_Locations_SY2324_description.json\n",
      "auctus/dataset_descriptions/Child_Friendly_Downtown_-_Edmonton_Insight_Community_description.json\n",
      "auctus/dataset_descriptions/Citywide_Marketing_Directory_description.json\n",
      "auctus/dataset_descriptions/Dallas_Animal_Shelter_Data_Fiscal_Year_2020_-2021_description.json\n",
      "auctus/dataset_descriptions/Edmonton_Public_School_Board___School_Building_Footprints_description.json\n",
      "auctus/dataset_has_description/\n",
      "auctus/dataset_has_description/2007_-_2008_School_Progress_Reports_-_All_Schools.csv\n",
      "auctus/dataset_has_description/2007_-_2008_School_Progress_Reports_-_All_Schools.json\n",
      "auctus/dataset_has_description/Atomic_Polygons.csv\n",
      "auctus/dataset_has_description/Atomic_Polygons.json\n",
      "auctus/dataset_has_description/Ballfields_9K.csv\n",
      "auctus/dataset_has_description/Ballfields_9K.json\n",
      "auctus/dataset_has_description/Bike_Ridership_-_Edmonton_Insight_Community.csv\n",
      "auctus/dataset_has_description/Bike_Ridership_-_Edmonton_Insight_Community.json\n",
      "auctus/dataset_has_description/Chicago_Park_District_Activities.csv\n",
      "auctus/dataset_has_description/Chicago_Park_District_Activities.json\n",
      "auctus/dataset_has_description/Chicago_Park_District__Movies_in_the_Parks_2014.csv\n",
      "auctus/dataset_has_description/Chicago_Park_District__Movies_in_the_Parks_2014.json\n",
      "auctus/dataset_has_description/Chicago_Park_District__Movies_in_the_Parks_2016.csv\n",
      "auctus/dataset_has_description/Chicago_Park_District__Movies_in_the_Parks_2016.json\n",
      "auctus/dataset_has_description/Chicago_Park_District__Movies_in_the_Parks_2017.csv\n",
      "auctus/dataset_has_description/Chicago_Park_District__Movies_in_the_Parks_2017.json\n",
      "auctus/dataset_has_description/Chicago_Park_District__Movies_in_the_Parks_2018.csv\n",
      "auctus/dataset_has_description/Chicago_Park_District__Movies_in_the_Parks_2018.json\n",
      "auctus/dataset_has_description/Chicago_Public_Schools_-_School_Admissions_Information_SY1819.csv\n",
      "auctus/dataset_has_description/Chicago_Public_Schools_-_School_Admissions_Information_SY1819.json\n",
      "auctus/dataset_has_description/Chicago_Public_Schools_-_School_Admissions_Information_SY2324.csv\n",
      "auctus/dataset_has_description/Chicago_Public_Schools_-_School_Admissions_Information_SY2324.json\n",
      "auctus/dataset_has_description/Chicago_Public_Schools_-_School_Locations_SY2122.csv\n",
      "auctus/dataset_has_description/Chicago_Public_Schools_-_School_Locations_SY2122.json\n",
      "auctus/dataset_has_description/Chicago_Public_Schools_-_School_Locations_SY2324.csv\n",
      "auctus/dataset_has_description/Chicago_Public_Schools_-_School_Locations_SY2324.json\n",
      "auctus/dataset_has_description/Chicago_Public_Schools_-_School_Progress_Reports_SY2223.csv\n",
      "auctus/dataset_has_description/Chicago_Public_Schools_-_School_Progress_Reports_SY2223.json\n",
      "auctus/dataset_has_description/Child_Friendly_Downtown_-_Edmonton_Insight_Community.csv\n",
      "auctus/dataset_has_description/Child_Friendly_Downtown_-_Edmonton_Insight_Community.json\n",
      "auctus/dataset_has_description/Citywide_Marketing_Directory.csv\n",
      "auctus/dataset_has_description/Citywide_Marketing_Directory.json\n",
      "auctus/dataset_has_description/Dallas_Animal_Shelter_Data_Fiscal_Year_2020_-2021.csv\n",
      "auctus/dataset_has_description/Dallas_Animal_Shelter_Data_Fiscal_Year_2020_-2021.json\n",
      "auctus/dataset_has_description/Edmonton_Public_School_Board___School_Building_Footprints.csv\n",
      "auctus/dataset_has_description/Edmonton_Public_School_Board___School_Building_Footprints.json\n",
      "auctus/dataset_has_description/Energy_and_Water_Data_Disclosure_for_Local_Law_84_2014__Data_for_Calendar_Year_2013_.csv\n",
      "auctus/dataset_has_description/Energy_and_Water_Data_Disclosure_for_Local_Law_84_2014__Data_for_Calendar_Year_2013_.json\n",
      "auctus/dataset_has_description/Energy_and_Water_Data_Disclosure_for_Local_Law_84_2015__Data_for_Calendar_Year_2014_.csv\n",
      "auctus/dataset_has_description/Energy_and_Water_Data_Disclosure_for_Local_Law_84_2015__Data_for_Calendar_Year_2014_.json\n",
      "auctus/dataset_has_description/Energy_and_Water_Data_Disclosure_for_Local_Law_84_2016__Data_for_Calendar_Year_2015_.csv\n",
      "auctus/dataset_has_description/Energy_and_Water_Data_Disclosure_for_Local_Law_84_2016__Data_for_Calendar_Year_2015_.json\n",
      "auctus/dataset_has_description/Energy_and_Water_Data_Disclosure_for_Local_Law_84_2022__Data_for_Calendar_Year_2021_.csv\n",
      "auctus/dataset_has_description/Energy_and_Water_Data_Disclosure_for_Local_Law_84_2022__Data_for_Calendar_Year_2021_.json\n",
      "auctus/dataset_has_description/Expansive_Rock_and_Soil_data.csv\n",
      "auctus/dataset_has_description/Expansive_Rock_and_Soil_data.json\n",
      "auctus/dataset_has_description/Farmers_Markets_-_2012.csv\n",
      "auctus/dataset_has_description/Farmers_Markets_-_2012.json\n",
      "auctus/dataset_has_description/Farmers_Markets_-_2013.csv\n",
      "auctus/dataset_has_description/Farmers_Markets_-_2013.json\n",
      "auctus/dataset_has_description/Farmers_Markets_-_2015.csv\n",
      "auctus/dataset_has_description/Farmers_Markets_-_2015.json\n",
      "auctus/dataset_has_description/Financial_Services_for_NYCHA_Residents_by_Development_-_Local_Law_163.csv\n",
      "auctus/dataset_has_description/Financial_Services_for_NYCHA_Residents_by_Development_-_Local_Law_163.json\n",
      "auctus/dataset_has_description/Forestry_Inspections.csv\n",
      "auctus/dataset_has_description/Forestry_Inspections.json\n",
      "auctus/dataset_has_description/Forestry_Planting_Spaces.csv\n",
      "auctus/dataset_has_description/Forestry_Planting_Spaces.json\n",
      "auctus/dataset_has_description/Forestry_Risk_Assessments.csv\n",
      "auctus/dataset_has_description/Forestry_Risk_Assessments.json\n",
      "auctus/dataset_has_description/Forestry_Tree_Points.csv\n",
      "auctus/dataset_has_description/Forestry_Tree_Points.json\n",
      "auctus/dataset_has_description/Forestry_Work_Orders.csv\n",
      "auctus/dataset_has_description/Forestry_Work_Orders.json\n",
      "auctus/dataset_has_description/Grants_for_Weatherization_Services.csv\n",
      "auctus/dataset_has_description/Grants_for_Weatherization_Services.json\n",
      "auctus/dataset_has_description/Grocery_Stores.csv\n",
      "auctus/dataset_has_description/Grocery_Stores.json\n",
      "auctus/dataset_has_description/Hurricane_City_Police_Data.csv\n",
      "auctus/dataset_has_description/Hurricane_City_Police_Data.json\n",
      "auctus/dataset_has_description/Hurricane_Evacuation_Centers.csv\n",
      "auctus/dataset_has_description/Hurricane_Evacuation_Centers.json\n",
      "auctus/dataset_has_description/Independent_Theaters__Live_Event_Venues__and_Live_Event_Support_Grants__FY2023.csv\n",
      "auctus/dataset_has_description/Independent_Theaters__Live_Event_Venues__and_Live_Event_Support_Grants__FY2023.json\n",
      "auctus/dataset_has_description/Labor_Market_Analysts.csv\n",
      "auctus/dataset_has_description/Labor_Market_Analysts.json\n",
      "auctus/dataset_has_description/Labor_Market_Regions.csv\n",
      "auctus/dataset_has_description/Labor_Market_Regions.json\n",
      "auctus/dataset_has_description/Language_Access_Secret_Shopper__LASS__Ratings.csv\n",
      "auctus/dataset_has_description/Language_Access_Secret_Shopper__LASS__Ratings.json\n",
      "auctus/dataset_has_description/MTA_Subsidies__Beginning_2019.csv\n",
      "auctus/dataset_has_description/MTA_Subsidies__Beginning_2019.json\n",
      "auctus/dataset_has_description/Marketplace-events.csv\n",
      "auctus/dataset_has_description/Marketplace-events.json\n",
      "auctus/dataset_has_description/Marketplace_Providers_Comparison_Summary.csv\n",
      "auctus/dataset_has_description/Marketplace_Providers_Comparison_Summary.json\n",
      "auctus/dataset_has_description/Massachusetts_Department_of_Transportation__MassDOT__Work_Zone_Data_Exchange__WZDx__v2.0_Feed_Sample.csv\n",
      "auctus/dataset_has_description/Massachusetts_Department_of_Transportation__MassDOT__Work_Zone_Data_Exchange__WZDx__v2.0_Feed_Sample.json\n",
      "auctus/dataset_has_description/Member_Health_Subsidy.csv\n",
      "auctus/dataset_has_description/Member_Health_Subsidy.json\n",
      "auctus/dataset_has_description/Micro-Market_Recovery_Program_-_Permits.csv\n",
      "auctus/dataset_has_description/Micro-Market_Recovery_Program_-_Permits.json\n",
      "auctus/dataset_has_description/Missouri_Farmers__Markets.csv\n",
      "auctus/dataset_has_description/Missouri_Farmers__Markets.json\n",
      "auctus/dataset_has_description/NYC_Farmers_Markets.csv\n",
      "auctus/dataset_has_description/NYC_Farmers_Markets.json\n",
      "auctus/dataset_has_description/Nominee_Certificates_Issued_by_Stream.csv\n",
      "auctus/dataset_has_description/Nominee_Certificates_Issued_by_Stream.json\n",
      "auctus/dataset_has_description/Number_of_businesses_in_the_State.csv\n",
      "auctus/dataset_has_description/Number_of_businesses_in_the_State.json\n",
      "auctus/dataset_has_description/PCHEES_Dashboard_-_Market_Penetration.csv\n",
      "auctus/dataset_has_description/PCHEES_Dashboard_-_Market_Penetration.json\n",
      "auctus/dataset_has_description/Parks_Special_Events.csv\n",
      "auctus/dataset_has_description/Parks_Special_Events.json\n",
      "auctus/dataset_has_description/Port_Planning___Marketing_Fund.csv\n",
      "auctus/dataset_has_description/Port_Planning___Marketing_Fund.json\n",
      "auctus/dataset_has_description/Restaurant_and_Market_Health_Inspections.csv\n",
      "auctus/dataset_has_description/Restaurant_and_Market_Health_Inspections.json\n",
      "auctus/dataset_has_description/Road_Weather_Demonstration_Data.csv\n",
      "auctus/dataset_has_description/Road_Weather_Demonstration_Data.json\n",
      "auctus/dataset_has_description/Root_for_Trees_Inventory.csv\n",
      "auctus/dataset_has_description/Root_for_Trees_Inventory.json\n",
      "auctus/dataset_has_description/Sandy.Parcels.csv\n",
      "auctus/dataset_has_description/Sandy.Parcels.json\n",
      "auctus/dataset_has_description/School_Attendance_by_School__2021-2022.csv\n",
      "auctus/dataset_has_description/School_Attendance_by_School__2021-2022.json\n",
      "auctus/dataset_has_description/School_Zones_2024-2025__Elementary_School_.csv\n",
      "auctus/dataset_has_description/School_Zones_2024-2025__Elementary_School_.json\n",
      "auctus/dataset_has_description/School_Zones_2024-2025__High_School_.csv\n",
      "auctus/dataset_has_description/School_Zones_2024-2025__High_School_.json\n",
      "auctus/dataset_has_description/Sidewalk_Management_Database_-_All_Tree_Damage__ATD_.csv\n",
      "auctus/dataset_has_description/Sidewalk_Management_Database_-_All_Tree_Damage__ATD_.json\n",
      "auctus/dataset_has_description/Sidewalk_Management_Database_-_Built.csv\n",
      "auctus/dataset_has_description/Sidewalk_Management_Database_-_Built.json\n",
      "auctus/dataset_has_description/State_Agency_Open_Data_Planning_Compliance.csv\n",
      "auctus/dataset_has_description/State_Agency_Open_Data_Planning_Compliance.json\n",
      "auctus/dataset_has_description/Subsidy_Uptake_Dashboard.csv\n",
      "auctus/dataset_has_description/Subsidy_Uptake_Dashboard.json\n",
      "auctus/dataset_has_description/Traffic_Signal_and_All-Way_Stop_Study_Requests.csv\n",
      "auctus/dataset_has_description/Traffic_Signal_and_All-Way_Stop_Study_Requests.json\n",
      "auctus/dataset_has_description/Tree_Contract_Work_Details.csv\n",
      "auctus/dataset_has_description/Tree_Contract_Work_Details.json\n",
      "auctus/dataset_has_description/Tree_Inventory_Denver.csv\n",
      "auctus/dataset_has_description/Tree_Inventory_Denver.json\n",
      "auctus/dataset_has_description/Trees.csv\n",
      "auctus/dataset_has_description/Trees.json\n",
      "auctus/dataset_has_description/Unemployment_Rate_in_Northeast_Census_Region.csv\n",
      "auctus/dataset_has_description/Unemployment_Rate_in_Northeast_Census_Region.json\n",
      "auctus/dataset_has_description/Utah_Census_Data_Cities_2009-2013.csv\n",
      "auctus/dataset_has_description/Utah_Census_Data_Cities_2009-2013.json\n",
      "auctus/dataset_has_description/WDFW-Creel_Analysis_Effort_Count.csv\n",
      "auctus/dataset_has_description/WDFW-Creel_Analysis_Effort_Count.json\n",
      "auctus/dataset_has_description/WDFW-Creel_Analysis_Interview.csv\n",
      "auctus/dataset_has_description/WDFW-Creel_Analysis_Interview.json\n",
      "auctus/dataset_has_description/Waterlines_1K.csv\n",
      "auctus/dataset_has_description/Waterlines_1K.json\n",
      "auctus/dataset_has_description/Watershed_Water_Quality_-_Hydrology_Qualifiers.csv\n",
      "auctus/dataset_has_description/Watershed_Water_Quality_-_Hydrology_Qualifiers.json\n",
      "auctus/dataset_has_description/Weather_Data__Daily__-_Environment_Canada.csv\n",
      "auctus/dataset_has_description/Weather_Data__Daily__-_Environment_Canada.json\n",
      "auctus/dataset_has_description/Wholesale_Markets.csv\n",
      "auctus/dataset_has_description/Wholesale_Markets.json\n",
      "auctus/dataset_has_description/ethiopia-market-centers.csv\n",
      "auctus/dataset_has_description/ethiopia-market-centers.json\n",
      "auctus/dataset_has_description/external-debt_pse.csv\n",
      "auctus/dataset_has_description/external-debt_pse.json\n",
      "auctus/dataset_has_description/ffa24-grd-projects.csv\n",
      "auctus/dataset_has_description/ffa24-grd-projects.json\n",
      "auctus/dataset_has_description/financial-sector_blz.csv\n",
      "auctus/dataset_has_description/financial-sector_blz.json\n",
      "auctus/dataset_has_description/financial-sector_brn.csv\n",
      "auctus/dataset_has_description/financial-sector_brn.json\n",
      "auctus/dataset_has_description/financial-sector_btn.csv\n",
      "auctus/dataset_has_description/financial-sector_btn.json\n",
      "auctus/dataset_has_description/financial-sector_geo.csv\n",
      "auctus/dataset_has_description/financial-sector_geo.json\n",
      "auctus/dataset_has_description/financial-sector_mne.csv\n",
      "auctus/dataset_has_description/financial-sector_mne.json\n",
      "auctus/dataset_has_description/financial-sector_ssd.csv\n",
      "auctus/dataset_has_description/financial-sector_ssd.json\n",
      "auctus/dataset_has_description/financial-sector_tls.csv\n",
      "auctus/dataset_has_description/financial-sector_tls.json\n",
      "auctus/dataset_has_description/financial-sector_uzb.csv\n",
      "auctus/dataset_has_description/financial-sector_uzb.json\n",
      "auctus/dataset_has_description/financial-sector_vct.csv\n",
      "auctus/dataset_has_description/financial-sector_vct.json\n",
      "auctus/dataset_has_description/private-sector_pse.csv\n",
      "auctus/dataset_has_description/private-sector_pse.json\n",
      "auctus/dataset_has_description/qc_aid-effectiveness_pse.csv\n",
      "auctus/dataset_has_description/qc_aid-effectiveness_pse.json\n",
      "auctus/dataset_has_description/qc_climate-change_pse.csv\n",
      "auctus/dataset_has_description/qc_climate-change_pse.json\n",
      "auctus/dataset_has_description/qc_external-debt_pse.csv\n",
      "auctus/dataset_has_description/qc_external-debt_pse.json\n",
      "auctus/dataset_has_description/qc_financial-sector_prk.csv\n",
      "auctus/dataset_has_description/qc_financial-sector_prk.json\n",
      "auctus/dataset_has_description/qc_gender_pse.csv\n",
      "auctus/dataset_has_description/qc_gender_pse.json\n",
      "auctus/dataset_has_description/qc_health_pse.csv\n",
      "auctus/dataset_has_description/qc_health_pse.json\n",
      "auctus/dataset_has_description/qc_infrastructure_pse.csv\n",
      "auctus/dataset_has_description/qc_infrastructure_pse.json\n",
      "auctus/dataset_has_description/qc_public-sector_pse.csv\n",
      "auctus/dataset_has_description/qc_public-sector_pse.json\n",
      "auctus/dataset_has_description/qc_trade_pse.csv\n",
      "auctus/dataset_has_description/qc_trade_pse.json\n",
      "auctus/dataset_has_description/qc_urban-development_pse.csv\n",
      "auctus/dataset_has_description/qc_urban-development_pse.json\n",
      "auctus/dataset_has_description/trade_pse.csv\n",
      "auctus/dataset_has_description/trade_pse.json\n",
      "auctus/dataset_no_description/\n",
      "auctus/dataset_no_description/2017_Commodity_Flow_Survey_Public_Use_File__CFS_PUF_.csv\n",
      "auctus/dataset_no_description/2017_Diversity_Report_Pre-Kindergarten.csv\n",
      "auctus/dataset_no_description/AFF_-_GA_Inventory.csv\n",
      "auctus/dataset_no_description/AFF_-_GA_Performance.csv\n",
      "auctus/dataset_no_description/Air_Quality_Testing_at_Saratoga_Village.csv\n",
      "auctus/dataset_no_description/Air_Vac.csv\n",
      "auctus/dataset_no_description/Approved_CAPS.csv\n",
      "auctus/dataset_no_description/Aqueduct_Air_Vent.csv\n",
      "auctus/dataset_no_description/BART_Real_Estate.csv\n",
      "auctus/dataset_no_description/Bachelors_Degree_Business___Business_Management_Salaries_Yrs_1-4.csv\n",
      "auctus/dataset_no_description/Backflow_Preventers.csv\n",
      "auctus/dataset_no_description/Businesses.csv\n",
      "auctus/dataset_no_description/Buy_Missouri_Businesses.csv\n",
      "auctus/dataset_no_description/CTE_Demographic_Data_by_District_2010.csv\n",
      "auctus/dataset_no_description/Capital_Projects.csv\n",
      "auctus/dataset_no_description/Capital_Projects_Point_Geometry.csv\n",
      "auctus/dataset_no_description/Cigarette___Tobacco_Tax_Revenue_Utah_1990-2017.csv\n",
      "auctus/dataset_no_description/College___Universities.csv\n",
      "auctus/dataset_no_description/Commerce_performance_measures_monthly_counts.csv\n",
      "auctus/dataset_no_description/Commercial_Development_Projects.csv\n",
      "auctus/dataset_no_description/Concrete_Locations.csv\n",
      "auctus/dataset_no_description/Coronavirus_Relief_Fund_Appropriations.csv\n",
      "auctus/dataset_no_description/CountyRoads_RRAMS_1k.csv\n",
      "auctus/dataset_no_description/County_Libraries.csv\n",
      "auctus/dataset_no_description/Covid-19-News.csv\n",
      "auctus/dataset_no_description/Covid_Vaccination_Events.csv\n",
      "auctus/dataset_no_description/Defects_and_Violations.csv\n",
      "auctus/dataset_no_description/Dept_of_Insurance_Performance_Outcome_Measures.csv\n",
      "auctus/dataset_no_description/Detention_Ponds.csv\n",
      "auctus/dataset_no_description/Development_Projects.csv\n",
      "auctus/dataset_no_description/District_EOC_scores_by_Subgroup_2013.csv\n",
      "auctus/dataset_no_description/Driver_Feedback_Signs__Radar_Boards_.csv\n",
      "auctus/dataset_no_description/East_Lone_Tree_Focus_Area_Boundary.csv\n",
      "auctus/dataset_no_description/Elderly_Housing.csv\n",
      "auctus/dataset_no_description/Environmentally_Preferable_Purchasing_FY15_-_Construction.csv\n",
      "auctus/dataset_no_description/Fire_Hydrant.csv\n",
      "auctus/dataset_no_description/Fittings__Air_Vac_.csv\n",
      "auctus/dataset_no_description/Home_Page_Stories.csv\n",
      "auctus/dataset_no_description/Hydrants.csv\n",
      "auctus/dataset_no_description/Inlets.csv\n",
      "auctus/dataset_no_description/Inspection_Needed.csv\n",
      "auctus/dataset_no_description/Irr_Air_Release.csv\n",
      "auctus/dataset_no_description/Job_Centers.csv\n",
      "auctus/dataset_no_description/Laterals.csv\n",
      "auctus/dataset_no_description/Library_Professional_Development_Grants.csv\n",
      "auctus/dataset_no_description/MZ_Test_Point_Data.csv\n",
      "auctus/dataset_no_description/Manholes.csv\n",
      "auctus/dataset_no_description/Master_Address_Points.csv\n",
      "auctus/dataset_no_description/Meter_Box.csv\n",
      "auctus/dataset_no_description/Money_Services_Businesses.csv\n",
      "auctus/dataset_no_description/Neighborhood_Cleanups.csv\n",
      "auctus/dataset_no_description/Neighborhoods.csv\n",
      "auctus/dataset_no_description/Outbreak_Data_-_Restaurant_Grading.csv\n",
      "auctus/dataset_no_description/Outfalls.csv\n",
      "auctus/dataset_no_description/PMA_Segments.csv\n",
      "auctus/dataset_no_description/PMA_Segments__Function_Class_.csv\n",
      "auctus/dataset_no_description/Parcels.csv\n",
      "auctus/dataset_no_description/Parcels_Web.csv\n",
      "auctus/dataset_no_description/Park_Envelopes.csv\n",
      "auctus/dataset_no_description/Park_Site.csv\n",
      "auctus/dataset_no_description/Parks.csv\n",
      "auctus/dataset_no_description/Part-Time_Rate.csv\n",
      "auctus/dataset_no_description/Patient_Survey_Data_Homehealth_Care_ALL.csv\n",
      "auctus/dataset_no_description/Payroll_Jobs.csv\n",
      "auctus/dataset_no_description/Plat_Hook_1K.csv\n",
      "auctus/dataset_no_description/Preferred_Source_Procurements_FY15.csv\n",
      "auctus/dataset_no_description/Project_Areas.csv\n",
      "auctus/dataset_no_description/Purchasing___Finance_Performance_Outcome_Measures.csv\n",
      "auctus/dataset_no_description/Quarterly_Grocery_Food_Sales__Q1__Calendar_2014.csv\n",
      "auctus/dataset_no_description/Rail_Crossings.csv\n",
      "auctus/dataset_no_description/Raise_The_Roof_Insurance.csv\n",
      "auctus/dataset_no_description/Real_GDP.csv\n",
      "auctus/dataset_no_description/Reentry_Housing.csv\n",
      "auctus/dataset_no_description/Residential_Development_Projects.csv\n",
      "auctus/dataset_no_description/SMART_Station.csv\n",
      "auctus/dataset_no_description/SWPPP_Sites.csv\n",
      "auctus/dataset_no_description/Sandy_Pride_Projects_-_Assigned.csv\n",
      "auctus/dataset_no_description/Senior_Farmers_Market_Nutrition_Program.csv\n",
      "auctus/dataset_no_description/Short_List_Locations.csv\n",
      "auctus/dataset_no_description/Skilled_Nursing_Facility_Costs_DataViz.csv\n",
      "auctus/dataset_no_description/Statewide_Child_Care_Subsidy_Providers.csv\n",
      "auctus/dataset_no_description/Storm_Drain_Pipes.csv\n",
      "auctus/dataset_no_description/Subsidized_Affordable_Housing.csv\n",
      "auctus/dataset_no_description/Surface_Parking.csv\n",
      "auctus/dataset_no_description/Survey_Data_-_Restaurant_Grading.csv\n",
      "auctus/dataset_no_description/T100_-_Preliminary_Estimates.csv\n",
      "auctus/dataset_no_description/TIP_Project_Lines.csv\n",
      "auctus/dataset_no_description/TIP_Project_Points.csv\n",
      "auctus/dataset_no_description/Takata_Recall___Net_Air_Bags_Remaining.csv\n",
      "auctus/dataset_no_description/Trees_Trimmed_And_Removed_Annually.csv\n",
      "auctus/dataset_no_description/Upload_--_Farm_Pads___Evacuation_Routes_05042015.csv\n",
      "auctus/dataset_no_description/Valves.csv\n",
      "auctus/dataset_no_description/WELPA_Scores_by_District_2010.csv\n",
      "auctus/dataset_no_description/WIC_Farmers_Market_Nutrition_Program.csv\n",
      "auctus/dataset_no_description/WINDSOR_COUNTY_SD__2019_DEMOGRAPHICS.csv\n",
      "auctus/dataset_no_description/WLRD_Projects.csv\n",
      "auctus/dataset_no_description/Well_House.csv\n",
      "auctus/dataset_no_description/Workforce_Services_performance_management_dashboard_Housing.csv\n",
      "auctus/dataset_no_description/Workforce_Services_performance_management_dashboard_percent.csv\n",
      "auctus/dataset_no_description/live_parcel_line.csv\n",
      "dataset_descriptions/2007_-_2008_School_Progress_Reports_-_All_Schools_description.json\n",
      "dataset_descriptions/Atomic_Polygons_description.json\n",
      "dataset_descriptions/Ballfields_9K_description.json\n",
      "dataset_descriptions/Bike_Ridership_-_Edmonton_Insight_Community_description.json\n",
      "dataset_descriptions/Chicago_Park_District_Activities_description.json\n",
      "dataset_descriptions/Chicago_Park_District__Movies_in_the_Parks_2014_description.json\n",
      "dataset_descriptions/Chicago_Park_District__Movies_in_the_Parks_2016_description.json\n",
      "dataset_descriptions/Chicago_Park_District__Movies_in_the_Parks_2017_description.json\n",
      "dataset_descriptions/Chicago_Park_District__Movies_in_the_Parks_2018_description.json\n",
      "dataset_descriptions/Chicago_Public_Schools_-_School_Admissions_Information_SY1819_description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__2007_-_2008_School_Progress_Reports_-_All_Schools__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__Atomic_Polygons__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__Ballfields_9K__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__Bike_Ridership_-_Edmonton_Insight_Community__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__Chicago_Park_District_Activities__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__Chicago_Park_District__Movies_in_the_Parks_2014__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__Chicago_Park_District__Movies_in_the_Parks_2016__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__Chicago_Park_District__Movies_in_the_Parks_2017__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__Chicago_Park_District__Movies_in_the_Parks_2018__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__Chicago_Public_Schools_-_School_Admissions_Information_SY1819__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__Chicago_Public_Schools_-_School_Admissions_Information_SY2324__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__Chicago_Public_Schools_-_School_Locations_SY2122__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__Chicago_Public_Schools_-_School_Locations_SY2324__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__Chicago_Public_Schools_-_School_Progress_Reports_SY2223__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__Child_Friendly_Downtown_-_Edmonton_Insight_Community__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__Citywide_Marketing_Directory__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__Dallas_Animal_Shelter_Data_Fiscal_Year_2020_-2021__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__Edmonton_Public_School_Board___School_Building_Footprints__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__Energy_and_Water_Data_Disclosure_for_Local_Law_84_2014__Data_for_Calendar_Year_2013___description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__Energy_and_Water_Data_Disclosure_for_Local_Law_84_2015__Data_for_Calendar_Year_2014___description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__Energy_and_Water_Data_Disclosure_for_Local_Law_84_2016__Data_for_Calendar_Year_2015___description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__Energy_and_Water_Data_Disclosure_for_Local_Law_84_2022__Data_for_Calendar_Year_2021___description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__Expansive_Rock_and_Soil_data__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__Farmers_Markets_-_2012__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__Farmers_Markets_-_2013__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__Farmers_Markets_-_2015__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__Financial_Services_for_NYCHA_Residents_by_Development_-_Local_Law_163__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__Forestry_Risk_Assessments__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__Grants_for_Weatherization_Services__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__Grocery_Stores__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__Hurricane_City_Police_Data__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__Hurricane_Evacuation_Centers__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__Independent_Theaters__Live_Event_Venues__and_Live_Event_Support_Grants__FY2023__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__Labor_Market_Analysts__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__Labor_Market_Regions__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__Language_Access_Secret_Shopper__LASS__Ratings__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__MTA_Subsidies__Beginning_2019__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__Marketplace-events__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__Marketplace_Providers_Comparison_Summary__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__Massachusetts_Department_of_Transportation__MassDOT__Work_Zone_Data_Exchange__WZDx__v2.0_Feed_Sample__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__Member_Health_Subsidy__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__Micro-Market_Recovery_Program_-_Permits__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__Missouri_Farmers__Markets__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__NYC_Farmers_Markets__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__Nominee_Certificates_Issued_by_Stream__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__Number_of_businesses_in_the_State__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__PCHEES_Dashboard_-_Market_Penetration__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__Parks_Special_Events__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__Port_Planning___Marketing_Fund__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__Restaurant_and_Market_Health_Inspections__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__Road_Weather_Demonstration_Data__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__Root_for_Trees_Inventory__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__Sandy.Parcels__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__School_Attendance_by_School__2021-2022__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__School_Zones_2024-2025__Elementary_School___description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__School_Zones_2024-2025__High_School___description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__Sidewalk_Management_Database_-_All_Tree_Damage__ATD___description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__Sidewalk_Management_Database_-_Built__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__State_Agency_Open_Data_Planning_Compliance__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__Subsidy_Uptake_Dashboard__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__Traffic_Signal_and_All-Way_Stop_Study_Requests__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__Tree_Contract_Work_Details__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__Tree_Inventory_Denver__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__Trees__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__Unemployment_Rate_in_Northeast_Census_Region__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__Utah_Census_Data_Cities_2009-2013__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__WDFW-Creel_Analysis_Effort_Count__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__WDFW-Creel_Analysis_Interview__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__Waterlines_1K__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__Watershed_Water_Quality_-_Hydrology_Qualifiers__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__Weather_Data__Daily__-_Environment_Canada__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__Wholesale_Markets__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__ethiopia-market-centers__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__external-debt_pse__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__ffa24-grd-projects__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__financial-sector_blz__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__financial-sector_brn__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__financial-sector_btn__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__financial-sector_geo__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__financial-sector_mne__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__financial-sector_ssd__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__financial-sector_tls__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__financial-sector_uzb__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__financial-sector_vct__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__private-sector_pse__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__qc_aid-effectiveness_pse__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__qc_climate-change_pse__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__qc_external-debt_pse__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__qc_financial-sector_prk__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__qc_gender_pse__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__qc_health_pse__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__qc_infrastructure_pse__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__qc_public-sector_pse__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__qc_trade_pse__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__qc_urban-development_pse__description.json\n",
      "dataset_descriptions/auctus__dataset_has_description__trade_pse__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__2017_Commodity_Flow_Survey_Public_Use_File__CFS_PUF___description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__2017_Diversity_Report_Pre-Kindergarten__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__AFF_-_GA_Inventory__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__AFF_-_GA_Performance__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Air_Quality_Testing_at_Saratoga_Village__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Air_Vac__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Approved_CAPS__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Aqueduct_Air_Vent__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__BART_Real_Estate__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Bachelors_Degree_Business___Business_Management_Salaries_Yrs_1-4__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Backflow_Preventers__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Businesses__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Buy_Missouri_Businesses__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__CTE_Demographic_Data_by_District_2010__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Capital_Projects_Point_Geometry__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Capital_Projects__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Cigarette___Tobacco_Tax_Revenue_Utah_1990-2017__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__College___Universities__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Commerce_performance_measures_monthly_counts__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Commercial_Development_Projects__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Concrete_Locations__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Coronavirus_Relief_Fund_Appropriations__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__CountyRoads_RRAMS_1k__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__County_Libraries__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Covid-19-News__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Covid_Vaccination_Events__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Dept_of_Insurance_Performance_Outcome_Measures__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Detention_Ponds__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Development_Projects__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__District_EOC_scores_by_Subgroup_2013__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Driver_Feedback_Signs__Radar_Boards___description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__East_Lone_Tree_Focus_Area_Boundary__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Elderly_Housing__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Environmentally_Preferable_Purchasing_FY15_-_Construction__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Fire_Hydrant__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Fittings__Air_Vac___description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Home_Page_Stories__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Hydrants__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Inlets__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Inspection_Needed__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Irr_Air_Release__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Job_Centers__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Laterals__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Library_Professional_Development_Grants__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Manholes__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Master_Address_Points__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Meter_Box__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Money_Services_Businesses__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Neighborhood_Cleanups__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Neighborhoods__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Outbreak_Data_-_Restaurant_Grading__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Outfalls__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__PMA_Segments__Function_Class___description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__PMA_Segments__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Parcels_Web__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Parcels__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Park_Envelopes__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Park_Site__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Parks__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Part-Time_Rate__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Patient_Survey_Data_Homehealth_Care_ALL__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Payroll_Jobs__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Plat_Hook_1K__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Preferred_Source_Procurements_FY15__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Project_Areas__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Purchasing___Finance_Performance_Outcome_Measures__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Quarterly_Grocery_Food_Sales__Q1__Calendar_2014__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Rail_Crossings__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Raise_The_Roof_Insurance__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Real_GDP__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Reentry_Housing__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Residential_Development_Projects__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__SMART_Station__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__SWPPP_Sites__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Sandy_Pride_Projects_-_Assigned__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Senior_Farmers_Market_Nutrition_Program__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Short_List_Locations__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Skilled_Nursing_Facility_Costs_DataViz__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Statewide_Child_Care_Subsidy_Providers__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Storm_Drain_Pipes__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Subsidized_Affordable_Housing__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Surface_Parking__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Survey_Data_-_Restaurant_Grading__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__T100_-_Preliminary_Estimates__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__TIP_Project_Lines__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__TIP_Project_Points__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Takata_Recall___Net_Air_Bags_Remaining__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Trees_Trimmed_And_Removed_Annually__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Upload_--_Farm_Pads___Evacuation_Routes_05042015__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Valves__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__WELPA_Scores_by_District_2010__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__WIC_Farmers_Market_Nutrition_Program__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__WINDSOR_COUNTY_SD__2019_DEMOGRAPHICS__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__WLRD_Projects__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Workforce_Services_performance_management_dashboard_Housing__description.json\n",
      "dataset_descriptions/auctus__dataset_no_description__Workforce_Services_performance_management_dashboard_percent__description.json\n",
      "dataset_descriptions/nyc_open_data__2006-2007 School Progress Report__description.json\n",
      "dataset_descriptions/nyc_open_data__2007 - 2008 School Progress Reports - All Schools__description.json\n",
      "dataset_descriptions/nyc_open_data__2007-08 Class Size - School-level Detail__description.json\n",
      "dataset_descriptions/nyc_open_data__2009 - 2010 Class Size - School-level Detail__description.json\n",
      "dataset_descriptions/nyc_open_data__2009 - 2010 Graduation Outcomes - Regents-based Math- ELA APM - School Level__description.json\n",
      "dataset_descriptions/nyc_open_data__2009 District 75 Survey Data__description.json\n",
      "dataset_descriptions/nyc_open_data__2009 Public Data File Students D75__description.json\n",
      "dataset_descriptions/nyc_open_data__2009-2012 Historical Daily Attendance By School__description.json\n",
      "dataset_descriptions/nyc_open_data__2010 - 2011 School Progress Report__description.json\n",
      "dataset_descriptions/nyc_open_data__2010 - 2011 School Progress Reports - All Schools__description.json\n",
      "dataset_descriptions/nyc_open_data__2010 AP College Board School Level Results__description.json\n",
      "dataset_descriptions/nyc_open_data__2010 Census Blocks water areas included__description.json\n",
      "dataset_descriptions/nyc_open_data__2010 Census Blocks__description.json\n",
      "dataset_descriptions/nyc_open_data__2010 Census Tracts water areas included__description.json\n",
      "dataset_descriptions/nyc_open_data__2010 Census Tracts__description.json\n",
      "dataset_descriptions/nyc_open_data__2010 Gen Ed Survey Data__description.json\n",
      "dataset_descriptions/nyc_open_data__2010 Public Use Microdata Areas PUMAs__description.json\n",
      "dataset_descriptions/nyc_open_data__2011 - 2012 YABC Progress Report__description.json\n",
      "dataset_descriptions/nyc_open_data__2011-2012 NYC Family Guides__description.json\n",
      "dataset_descriptions/nyc_open_data__2011-2012 School Progress Report - All Schools__description.json\n",
      "dataset_descriptions/nyc_open_data__2012 - 2013 School Locations__description.json\n",
      "dataset_descriptions/nyc_open_data__2012 - 2017 Historical Monthly Grade Level Attendance By School__description.json\n",
      "dataset_descriptions/nyc_open_data__2012 AP Results__description.json\n",
      "dataset_descriptions/nyc_open_data__2012 NYC District 75 Schools Survey__description.json\n",
      "dataset_descriptions/nyc_open_data__2012 NYC General Education School Survey__description.json\n",
      "dataset_descriptions/nyc_open_data__2013 - 2014 School Survey Data__description.json\n",
      "dataset_descriptions/nyc_open_data__2013 - 2018 Demographic Snapshot Borough__description.json\n",
      "dataset_descriptions/nyc_open_data__2013 - 2018 Demographic Snapshot District__description.json\n",
      "dataset_descriptions/nyc_open_data__2013 NYC School Survey__description.json\n",
      "dataset_descriptions/nyc_open_data__2013-2015 School Closure Discharge Reporting__description.json\n",
      "dataset_descriptions/nyc_open_data__2014 - 2015 Co-Location Report__description.json\n",
      "dataset_descriptions/nyc_open_data__2014 - 2015 Student School Survey Data__description.json\n",
      "dataset_descriptions/nyc_open_data__2014 -15 Guidance Counselor Reporting - Post Secondary Planning__description.json\n",
      "dataset_descriptions/nyc_open_data__2014-2015 DOE High School Performance-Directory__description.json\n",
      "dataset_descriptions/nyc_open_data__2014-2015 Diversity Report - K-8 Grades 9-12 District Schools Special Programs Diversity Efforts__description.json\n",
      "dataset_descriptions/nyc_open_data__2015 - 16 School Safety Report__description.json\n",
      "dataset_descriptions/nyc_open_data__2015 - 2016 Arts Report__description.json\n",
      "dataset_descriptions/nyc_open_data__2015 - 2016 Audited Register Data__description.json\n",
      "dataset_descriptions/nyc_open_data__2015 - 2016 Career Technical Education Report__description.json\n",
      "dataset_descriptions/nyc_open_data__2015 - 2016 Demographic Data - Pre-Kindergarten__description.json\n",
      "dataset_descriptions/nyc_open_data__2015 - 2016 Final Class Size Report Middle High School__description.json\n",
      "dataset_descriptions/nyc_open_data__2015 - 2016 Guidance Counselor Report__description.json\n",
      "dataset_descriptions/nyc_open_data__2015 - 2016 School Locations__description.json\n",
      "dataset_descriptions/nyc_open_data__2015 2016 Local Law 60 Bathroom Non- Instructional Space__description.json\n",
      "dataset_descriptions/nyc_open_data__2015 2016 Local Law 60 Room Detail__description.json\n",
      "dataset_descriptions/nyc_open_data__2015 2016 Student Discipline Annual Report - D Code__description.json\n",
      "dataset_descriptions/nyc_open_data__2015 2016 Student Discipline Annual Report - ELL__description.json\n",
      "dataset_descriptions/nyc_open_data__2015 2016 Student Discipline Annual Report - R P S TOTALS__description.json\n",
      "dataset_descriptions/nyc_open_data__2015 2016 Student Discipline Annual Report - RACE__description.json\n",
      "dataset_descriptions/nyc_open_data__2015 2016 Student Discipline Annual Report - Students In Temporary Housing__description.json\n",
      "dataset_descriptions/nyc_open_data__2015-16 Demographic Data - Admissions Methods__description.json\n",
      "dataset_descriptions/nyc_open_data__2015-16 Guidance Counselor Reporting - Demographic Data__description.json\n",
      "dataset_descriptions/nyc_open_data__2015-16 Guidance Counselor Reporting - Guidance Counselor Data__description.json\n",
      "dataset_descriptions/nyc_open_data__2015-16 Guidance Counselor Reporting - Social Worker Data__description.json\n",
      "dataset_descriptions/nyc_open_data__2015-16 Local Law 15 Health Data - Licensed Health Instructors__description.json\n",
      "dataset_descriptions/nyc_open_data__2015-2016 Demographic Data - Diversity Efforts__description.json\n",
      "dataset_descriptions/nyc_open_data__2015-2016 Demographic Data - Grades 9-12 School__description.json\n",
      "dataset_descriptions/nyc_open_data__2015-2016 Demographic Data - Grades K-8 District__description.json\n",
      "dataset_descriptions/nyc_open_data__2015-2016 Diversity Report - K-8 Grades 9-12 District Schools Special Programs Diversity Efforts Admissions Methods__description.json\n",
      "dataset_descriptions/nyc_open_data__2015-2016 Physical Education - PE Space - Certified PE Teachers__description.json\n",
      "dataset_descriptions/nyc_open_data__2015-2016 Physical Education - PE Space - School Level__description.json\n",
      "dataset_descriptions/nyc_open_data__2015-2016 Physical Education - PE Space__description.json\n",
      "dataset_descriptions/nyc_open_data__2015-2016 SHSAT Admissions Test Offers By Sending School__description.json\n",
      "dataset_descriptions/nyc_open_data__2015-2018 Historical Daily Attendance By School__description.json\n",
      "dataset_descriptions/nyc_open_data__2015-2018 School Closure Discharge Reporting__description.json\n",
      "dataset_descriptions/nyc_open_data__2016 - 2017 Arts Survey data__description.json\n",
      "dataset_descriptions/nyc_open_data__2016 - 2017 Computer Science Report__description.json\n",
      "dataset_descriptions/nyc_open_data__2016 - 2017 Health Education Instructor Report__description.json\n",
      "dataset_descriptions/nyc_open_data__2016 - 2017 School Safety Report__description.json\n",
      "dataset_descriptions/nyc_open_data__2016 DOE High School Performance Directory__description.json\n",
      "dataset_descriptions/nyc_open_data__2016 Public Data File Parent__description.json\n",
      "dataset_descriptions/nyc_open_data__2016 Public Data File Student__description.json\n",
      "dataset_descriptions/nyc_open_data__2016 Public Data File Teacher__description.json\n",
      "dataset_descriptions/nyc_open_data__2016-17 - 2020-21 District End-of-Year Attendance and Chronic Absenteeism Data__description.json\n",
      "dataset_descriptions/nyc_open_data__2016-17 Physical Education - PE Instruction - Borough Level__description.json\n",
      "dataset_descriptions/nyc_open_data__2016-17 Physical Education - PE Instruction - School Level__description.json\n",
      "dataset_descriptions/nyc_open_data__2016-2017 Arts Report__description.json\n",
      "dataset_descriptions/nyc_open_data__2016-2017 Monthly Attendance__description.json\n",
      "dataset_descriptions/nyc_open_data__2016-2017 Student Discipline Annual Report - AGE__description.json\n",
      "dataset_descriptions/nyc_open_data__2016-2017 Student Discipline Annual Report - ELL__description.json\n",
      "dataset_descriptions/nyc_open_data__2016-2017 Student Discipline Annual Report - GRADE LEVEL__description.json\n",
      "dataset_descriptions/nyc_open_data__2016-2017 Student Discipline Annual Report - RACE__description.json\n",
      "dataset_descriptions/nyc_open_data__2016-2017 Student Discipline Annual Report - RPS TOTALS__description.json\n",
      "dataset_descriptions/nyc_open_data__2016-2017 Student Discipline Annual Report - TRANSFERS__description.json\n",
      "dataset_descriptions/nyc_open_data__2017 - 2018 Demographic Snapshot 3- K For All__description.json\n",
      "dataset_descriptions/nyc_open_data__2017 - 2018 Diversity Report Admission Methods__description.json\n",
      "dataset_descriptions/nyc_open_data__2017 - 2018 November2017 Pct Class Size Borough K8 - Open Data Portal__description.json\n",
      "dataset_descriptions/nyc_open_data__2017 - 2018 November2017 Pct Class Size City K8 - Open Data Portal__description.json\n",
      "dataset_descriptions/nyc_open_data__2017 - 2018 Pct Class Size Borough MSHS - Open Data Portal__description.json\n",
      "dataset_descriptions/nyc_open_data__2017 - 2018 Quality Review Schools List__description.json\n",
      "dataset_descriptions/nyc_open_data__2017 - 2018 Schools NYPD Crime Data Report__description.json\n",
      "dataset_descriptions/nyc_open_data__2017 - 2020 Monthly Grade Level Attendance by School__description.json\n",
      "dataset_descriptions/nyc_open_data__2017 -2018 Diversity Report 9 - 12 School__description.json\n",
      "dataset_descriptions/nyc_open_data__2017 2018 Anticipated Bilingual Education Programs__description.json\n",
      "dataset_descriptions/nyc_open_data__2017 DOE High School Directory__description.json\n",
      "dataset_descriptions/nyc_open_data__2017 Diversity Report 9-12 - District__description.json\n",
      "dataset_descriptions/nyc_open_data__2017 Diversity Report 9-12 - School__description.json\n",
      "dataset_descriptions/nyc_open_data__2017 Diversity Report Admissions Methods__description.json\n",
      "dataset_descriptions/nyc_open_data__2017 Diversity Report K-8 - District__description.json\n",
      "dataset_descriptions/nyc_open_data__2017 Diversity Report K-8 - School__description.json\n",
      "dataset_descriptions/nyc_open_data__2017 Public Data File Parents__description.json\n",
      "dataset_descriptions/nyc_open_data__2017 School Food Report LL215__description.json\n",
      "dataset_descriptions/nyc_open_data__2017- 2018 Class Size Report City K-8 Average Class Size__description.json\n",
      "dataset_descriptions/nyc_open_data__2017- 2018 Class Size Report City Middle And High School Class Size Distribution__description.json\n",
      "dataset_descriptions/nyc_open_data__2017- 2018 Class Size Report City Middle School And High School Core Average Class Size__description.json\n",
      "dataset_descriptions/nyc_open_data__2017- 2018 Class Size Report District Middle And High School Class Size Distribution__description.json\n",
      "dataset_descriptions/nyc_open_data__2017-18 - 2021-22 Demographic Snapshot__description.json\n",
      "dataset_descriptions/nyc_open_data__2017-2018 Class Size Report Borough K-8 Average Class Size__description.json\n",
      "dataset_descriptions/nyc_open_data__2017-2018 Computer Science Report LL177__description.json\n",
      "dataset_descriptions/nyc_open_data__2017-2018 EMS Transport Final Race__description.json\n",
      "dataset_descriptions/nyc_open_data__2017-2018 EMS Transport Final Students With Disabilities__description.json\n",
      "dataset_descriptions/nyc_open_data__2017-2018 Health Education Report MS 54 - District__description.json\n",
      "dataset_descriptions/nyc_open_data__2017-2018 Health Education Report MS 54 - School__description.json\n",
      "dataset_descriptions/nyc_open_data__2017-2018 Local Law Licensed Health Instructor - School__description.json\n",
      "dataset_descriptions/nyc_open_data__2017-2018 Monthly Attendance__description.json\n",
      "dataset_descriptions/nyc_open_data__2017-2018 Physical Education - PE Space__description.json\n",
      "dataset_descriptions/nyc_open_data__2017-2018 SCHOOL LEVEL CLASS SIZE REPORT__description.json\n",
      "dataset_descriptions/nyc_open_data__2017-2018 Student Discipline Annual Report - AGE__description.json\n",
      "dataset_descriptions/nyc_open_data__2017-2018 Student Discipline Annual Report - Grade Level__description.json\n",
      "dataset_descriptions/nyc_open_data__2017-2018 Student Discipline Annual Report - Students In Temporary Housing__description.json\n",
      "dataset_descriptions/nyc_open_data__2017-2018 Student Discipline Annual Report - Students in Temporary Housing_n88k-5bs4__description.json\n",
      "dataset_descriptions/nyc_open_data__2017-2018 Students in Temporary Housing - School__description.json\n",
      "dataset_descriptions/nyc_open_data__2018 - 2019 Average Class Size City K8__description.json\n",
      "dataset_descriptions/nyc_open_data__2018 - 2019 Career Technical Education Report__description.json\n",
      "dataset_descriptions/nyc_open_data__2018 - 2019 Class Size Pct City K8__description.json\n",
      "dataset_descriptions/nyc_open_data__2018 - 2019 Students in Temporary Housing Additional Sections Report__description.json\n",
      "dataset_descriptions/nyc_open_data__2018 - 2020 CLASS and ECERS-R Data__description.json\n",
      "dataset_descriptions/nyc_open_data__2018 -2019 SQG EC__description.json\n",
      "dataset_descriptions/nyc_open_data__2018 DOE High School Directory__description.json\n",
      "dataset_descriptions/nyc_open_data__2018 DOE Middle School Directory__description.json\n",
      "dataset_descriptions/nyc_open_data__2018 Diversity Report - Grades 9-12 District__description.json\n",
      "dataset_descriptions/nyc_open_data__2018 Diversity Report - Grades 9-12 School__description.json\n",
      "dataset_descriptions/nyc_open_data__2018 Diversity Report - Grades K-8 District__description.json\n",
      "dataset_descriptions/nyc_open_data__2018 Diversity Report - Grades K-8 School__description.json\n",
      "dataset_descriptions/nyc_open_data__2018 Diversity Report - Pre-Kindergarten__description.json\n",
      "dataset_descriptions/nyc_open_data__2018 Pre-K School Directory__description.json\n",
      "dataset_descriptions/nyc_open_data__2018-19 Guidance Counselor Report - Post Secondary Planning__description.json\n",
      "dataset_descriptions/nyc_open_data__2018-2019 Average Class Size City - Middle High School__description.json\n",
      "dataset_descriptions/nyc_open_data__2018-2019 Citywide Demographic Snapshot__description.json\n",
      "dataset_descriptions/nyc_open_data__2018-2019 Class Size Report City K-8 Class Size Distribution__description.json\n",
      "dataset_descriptions/nyc_open_data__2018-2019 Class Size Report District Middle High School Class Size Distribution__description.json\n",
      "dataset_descriptions/nyc_open_data__2018-2019 Daily Attendance__description.json\n",
      "dataset_descriptions/nyc_open_data__2018-2019 Prelim Percentage Class Size Borough - K-8TH__description.json\n",
      "dataset_descriptions/nyc_open_data__2018-2019 Prelim Percentage Class Size City - K-8TH__description.json\n",
      "dataset_descriptions/nyc_open_data__2018-2019 Students in Temporary Housing - School__description.json\n",
      "dataset_descriptions/nyc_open_data__2019 - 2020 School Year Local Law 226 Report for the Demographics of School Staff - Ethnicity__description.json\n",
      "dataset_descriptions/nyc_open_data__2019 -2020 Local Law 174 CTE Report__description.json\n",
      "dataset_descriptions/nyc_open_data__2019 NYC School Survey - Parents__description.json\n",
      "dataset_descriptions/nyc_open_data__2019 Public Data File - Students__description.json\n",
      "dataset_descriptions/nyc_open_data__2019 Public Data File - Teachers__description.json\n",
      "dataset_descriptions/nyc_open_data__2019 Subway Rider Survey__description.json\n",
      "dataset_descriptions/nyc_open_data__2019- 2020 Submission MF LL231 7142020__description.json\n",
      "dataset_descriptions/nyc_open_data__2019-20 Demographic Data In NYC Public Schools Suppressed - Pre-K K-8 9-12 Grades__description.json\n",
      "dataset_descriptions/nyc_open_data__2019-20 Demographic Snapshot - School__description.json\n",
      "dataset_descriptions/nyc_open_data__2019-2020 Co-Location Reporting__description.json\n",
      "dataset_descriptions/nyc_open_data__2019-2020 Local Law 102 Physical Education Report -Final__description.json\n",
      "dataset_descriptions/nyc_open_data__2019-2020 PSAL Report - December 2020__description.json\n",
      "dataset_descriptions/nyc_open_data__2020 Census Blocks water areas included__description.json\n",
      "dataset_descriptions/nyc_open_data__2020 Census Blocks__description.json\n",
      "dataset_descriptions/nyc_open_data__2020 Census Tracts water areas included__description.json\n",
      "dataset_descriptions/nyc_open_data__2020 Census Tracts__description.json\n",
      "dataset_descriptions/nyc_open_data__2020 Community District Tabulation Areas CDTAs__description.json\n",
      "dataset_descriptions/nyc_open_data__2020 DOE Gifted and Talented Admissions Guide__description.json\n",
      "dataset_descriptions/nyc_open_data__2020 DOE High School Directory__description.json\n",
      "dataset_descriptions/nyc_open_data__2020 Neighborhood Tabulation Areas NTAs__description.json\n",
      "dataset_descriptions/nyc_open_data__2020 Public Use Microdata Areas PUMAs__description.json\n",
      "dataset_descriptions/nyc_open_data__2020-2021 Demographic Snapshot Borough__description.json\n",
      "dataset_descriptions/nyc_open_data__2020-2021 Demographic Snapshot Citywide__description.json\n",
      "dataset_descriptions/nyc_open_data__2020-2021 Demographic Snapshot District__description.json\n",
      "dataset_descriptions/nyc_open_data__2020-2021 EMS Transport Full Year - Race SWD YOB__description.json\n",
      "dataset_descriptions/nyc_open_data__2020-2021 Local Law 231 Training Data__description.json\n",
      "dataset_descriptions/nyc_open_data__2020-2021 Suspension Report NYPD Contacts - All__description.json\n",
      "dataset_descriptions/nyc_open_data__2020-21 Guidance CounselorSocial Worker Data - Demographic Post Secondary__description.json\n",
      "dataset_descriptions/nyc_open_data__2020s Mean Monthly High Water__description.json\n",
      "dataset_descriptions/nyc_open_data__2021 - 2022 Average Class Size by Borough__description.json\n",
      "dataset_descriptions/nyc_open_data__2021 - 2022 Bilingual Program List__description.json\n",
      "dataset_descriptions/nyc_open_data__2021 -2022 Guidance Counselor__description.json\n",
      "dataset_descriptions/nyc_open_data__2021 City Council - September Attendance Report__description.json\n",
      "dataset_descriptions/nyc_open_data__2021 City Council April Attendance__description.json\n",
      "dataset_descriptions/nyc_open_data__2021 City Council March Attendance__description.json\n",
      "dataset_descriptions/nyc_open_data__2021 DOE High School Directory__description.json\n",
      "dataset_descriptions/nyc_open_data__2021 LL51 Spring Semester DL FINAL REDACT__description.json\n",
      "dataset_descriptions/nyc_open_data__2021 Public Data File - Parent__description.json\n",
      "dataset_descriptions/nyc_open_data__2021 Public Data File - Teacher__description.json\n",
      "dataset_descriptions/nyc_open_data__2021_Prelim_Distribution_ClassSize_Borough__description.json\n",
      "dataset_descriptions/nyc_open_data__2050s Mean Monthly High Water__description.json\n",
      "dataset_descriptions/nyc_open_data__2050s Sea Level Rise Projections__description.json\n",
      "dataset_descriptions/nyc_open_data__2080s Mean Monthly High Water__description.json\n",
      "dataset_descriptions/nyc_open_data__2080s Mean Sea Level Rise Projections__description.json\n",
      "dataset_descriptions/nyc_open_data__2100s Mean Sea Level Rise Projections__description.json\n",
      "dataset_descriptions/nyc_open_data__311 Customer Satisfaction Survey__description.json\n",
      "dataset_descriptions/nyc_open_data__311 Interpreter Wait Time__description.json\n",
      "dataset_descriptions/nyc_open_data__311 Resolution Satisfaction Survey__description.json\n",
      "dataset_descriptions/nyc_open_data__485-x Affordable Neighborhoods for New Yorkers Registrations of Prospective Applicants for Tax Benefits__description.json\n",
      "dataset_descriptions/nyc_open_data__911 End-to-End Data__description.json\n",
      "dataset_descriptions/nyc_open_data__911 Open Data Local Law 119__description.json\n",
      "dataset_descriptions/nyc_open_data__ACRIS - Country Codes__description.json\n",
      "dataset_descriptions/nyc_open_data__ACRIS - Document Control Codes__description.json\n",
      "dataset_descriptions/nyc_open_data__ACRIS - Property Types Codes__description.json\n",
      "dataset_descriptions/nyc_open_data__ACRIS - State Codes__description.json\n",
      "dataset_descriptions/nyc_open_data__ACRIS - UCC Collateral Codes__description.json\n",
      "dataset_descriptions/nyc_open_data__AIR_LOT_POLYGON__description.json\n",
      "dataset_descriptions/nyc_open_data__ARCHIVE - Greenstreets__description.json\n",
      "dataset_descriptions/nyc_open_data__Accessible Pedestrian Signal Locations__description.json\n",
      "dataset_descriptions/nyc_open_data__Active NYC Health Code Regulated Child Care Programs__description.json\n",
      "dataset_descriptions/nyc_open_data__Active Projects Under Construction__description.json\n",
      "dataset_descriptions/nyc_open_data__Active Projects__description.json\n",
      "dataset_descriptions/nyc_open_data__Added Projects__description.json\n",
      "dataset_descriptions/nyc_open_data__Adding Chemicals to Water Supply Permits__description.json\n",
      "dataset_descriptions/nyc_open_data__Additional Costs Allocation__description.json\n",
      "dataset_descriptions/nyc_open_data__AddressPoint__description.json\n",
      "dataset_descriptions/nyc_open_data__Adult Protective Services - Ineligible Referral__description.json\n",
      "dataset_descriptions/nyc_open_data__Adult Protective Services - Refer to Close by Referral Source__description.json\n",
      "dataset_descriptions/nyc_open_data__Adult Protective Services - Refer to Close__description.json\n",
      "dataset_descriptions/nyc_open_data__Advanced Projects__description.json\n",
      "dataset_descriptions/nyc_open_data__Affordable Housing Production by Building__description.json\n",
      "dataset_descriptions/nyc_open_data__Affordable Housing Production by Project__description.json\n",
      "dataset_descriptions/nyc_open_data__Agency Spending by Budget Function__description.json\n",
      "dataset_descriptions/nyc_open_data__Agency Voter Registration Activity__description.json\n",
      "dataset_descriptions/nyc_open_data__Aggregate Employee Statistics__description.json\n",
      "dataset_descriptions/nyc_open_data__Air Quality__description.json\n",
      "dataset_descriptions/nyc_open_data__Algorithmic Tools Compliance Report__description.json\n",
      "dataset_descriptions/nyc_open_data__Annual Examination Schedule of Each Fiscal Year__description.json\n",
      "dataset_descriptions/nyc_open_data__Annual Overweight Load AOL Permits Stipulations__description.json\n",
      "dataset_descriptions/nyc_open_data__Annual Overweight Load AOL Permits__description.json\n",
      "dataset_descriptions/nyc_open_data__Annual Report on Domestic Violence Initiatives Indicators and Factors__description.json\n",
      "dataset_descriptions/nyc_open_data__Annual Report on Family Justice Center Client Satisfaction Surveys__description.json\n",
      "dataset_descriptions/nyc_open_data__Anticipated RFP__description.json\n",
      "dataset_descriptions/nyc_open_data__Aquatics Programming 2021 to current__description.json\n",
      "dataset_descriptions/nyc_open_data__Archaeology Reports Database__description.json\n",
      "dataset_descriptions/nyc_open_data__Archived DCWP Charges__description.json\n",
      "dataset_descriptions/nyc_open_data__Article 730 Transfer Waitlist__description.json\n",
      "dataset_descriptions/nyc_open_data__Asbestos Control Program ACP7__description.json\n",
      "dataset_descriptions/nyc_open_data__Assessment Actions__description.json\n",
      "dataset_descriptions/nyc_open_data__Asset Management Executive Summary - Agency Assets by Project Type and Class__description.json\n",
      "dataset_descriptions/nyc_open_data__Asset Management Executive Summary - State of Good Repair Needs Capital Eligible__description.json\n",
      "dataset_descriptions/nyc_open_data__Asset Management Executive Summary - State of Good Repair Needs Expense__description.json\n",
      "dataset_descriptions/nyc_open_data__Asset Management Parks System AMPS Assets__description.json\n",
      "dataset_descriptions/nyc_open_data__Asset Management Parks System AMPS Comments__description.json\n",
      "dataset_descriptions/nyc_open_data__Asset Management Parks System AMPS Transactions__description.json\n",
      "dataset_descriptions/nyc_open_data__Associated Address by Borough and Community District__description.json\n",
      "dataset_descriptions/nyc_open_data__Athletic Facilities__description.json\n",
      "dataset_descriptions/nyc_open_data__Atomic Polygons__description.json\n",
      "dataset_descriptions/nyc_open_data__Attendance Results 2013-2019__description.json\n",
      "dataset_descriptions/nyc_open_data__Automated Traffic Volume Counts__description.json\n",
      "dataset_descriptions/nyc_open_data__Automatic Public Toilets__description.json\n",
      "dataset_descriptions/nyc_open_data__Average Class Size 2017-2019 Preliminary__description.json\n",
      "dataset_descriptions/nyc_open_data__BIC Complaints Inquiries__description.json\n",
      "dataset_descriptions/nyc_open_data__BOUNDARY__description.json\n",
      "dataset_descriptions/nyc_open_data__BUILDING_HISTORIC_P__description.json\n",
      "dataset_descriptions/nyc_open_data__BUILDING_HISTORIC__description.json\n",
      "dataset_descriptions/nyc_open_data__BUILDING_P__description.json\n",
      "dataset_descriptions/nyc_open_data__Beach Water Samples__description.json\n",
      "dataset_descriptions/nyc_open_data__Beaches__description.json\n",
      "dataset_descriptions/nyc_open_data__Bedbug Reporting__description.json\n",
      "dataset_descriptions/nyc_open_data__Benefits Access Center Wait Times__description.json\n",
      "dataset_descriptions/nyc_open_data__Bi-Annual Pedestrian Counts__description.json\n",
      "dataset_descriptions/nyc_open_data__Bicycle Counters__description.json\n",
      "dataset_descriptions/nyc_open_data__Bicycle Parking__description.json\n",
      "dataset_descriptions/nyc_open_data__Big Apple Connect BAC Enrollment Statistics__description.json\n",
      "dataset_descriptions/nyc_open_data__Bike Share Inspections__description.json\n",
      "dataset_descriptions/nyc_open_data__Bikes in Buildings Requests__description.json\n",
      "dataset_descriptions/nyc_open_data__Block Planting__description.json\n",
      "dataset_descriptions/nyc_open_data__Block Pruning__description.json\n",
      "dataset_descriptions/nyc_open_data__Board of Standards and Appeals BSA Applications Status__description.json\n",
      "dataset_descriptions/nyc_open_data__Borough Boundaries water areas included__description.json\n",
      "dataset_descriptions/nyc_open_data__Borough Boundaries__description.json\n",
      "dataset_descriptions/nyc_open_data__Borough Engineering Tracking System__description.json\n",
      "dataset_descriptions/nyc_open_data__Borough Enrollment Offices__description.json\n",
      "dataset_descriptions/nyc_open_data__BoroughCommunity District Report__description.json\n",
      "dataset_descriptions/nyc_open_data__Bridge Hold Location Stipulations__description.json\n",
      "dataset_descriptions/nyc_open_data__Bronx Borough President Capital Funding__description.json\n",
      "dataset_descriptions/nyc_open_data__Bronx Community Boards__description.json\n",
      "dataset_descriptions/nyc_open_data__Buildings Selected for the Alternative Enforcement Program AEP__description.json\n",
      "dataset_descriptions/nyc_open_data__Buildings Selected for the Heat Sensor Program HSP__description.json\n",
      "dataset_descriptions/nyc_open_data__Buildings Selected for the Underlying Conditions Program__description.json\n",
      "dataset_descriptions/nyc_open_data__Buildings Subject to HPD Jurisdiction__description.json\n",
      "dataset_descriptions/nyc_open_data__Buildings by Borough and Community District__description.json\n",
      "dataset_descriptions/nyc_open_data__Bureau of Fire Investigations - Fire Causes__description.json\n",
      "dataset_descriptions/nyc_open_data__Bus Breakdown and Delays__description.json\n",
      "dataset_descriptions/nyc_open_data__Bus Lanes - Local Streets__description.json\n",
      "dataset_descriptions/nyc_open_data__Bus Pad Tracking__description.json\n",
      "dataset_descriptions/nyc_open_data__Bus Stop Shelter__description.json\n",
      "dataset_descriptions/nyc_open_data__Business Improvement District BID Trends Report FY24__description.json\n",
      "dataset_descriptions/nyc_open_data__Business Improvement Districts__description.json\n",
      "dataset_descriptions/nyc_open_data__CDBG-DR Program Categories__description.json\n",
      "dataset_descriptions/nyc_open_data__CDBG-DR Reimbursements Over Time__description.json\n",
      "dataset_descriptions/nyc_open_data__CEQR Project Locations__description.json\n",
      "dataset_descriptions/nyc_open_data__CEQR Project Milestones__description.json\n",
      "dataset_descriptions/nyc_open_data__CEQR Projects__description.json\n",
      "dataset_descriptions/nyc_open_data__CONDO_AREA__description.json\n",
      "dataset_descriptions/nyc_open_data__COVID-19 DOE Active Testing Locations as of Feb 25__description.json\n",
      "dataset_descriptions/nyc_open_data__COVID-19 Daily Counts of Cases Hospitalizations and Deaths__description.json\n",
      "dataset_descriptions/nyc_open_data__COVID-19 Free Meals Locations__description.json\n",
      "dataset_descriptions/nyc_open_data__COVID-19 LoanGrant Report Historical__description.json\n",
      "dataset_descriptions/nyc_open_data__COVID-19 Outcomes by Testing Cohorts Cases Hospitalizations and Deaths__description.json\n",
      "dataset_descriptions/nyc_open_data__COVID-19 Revenue Report Historical__description.json\n",
      "dataset_descriptions/nyc_open_data__CSC Appeals Filed in Calendar Year__description.json\n",
      "dataset_descriptions/nyc_open_data__CUNY Community College Expenditures By Source__description.json\n",
      "dataset_descriptions/nyc_open_data__CURRENT BASES__description.json\n",
      "dataset_descriptions/nyc_open_data__Campaign Contributions__description.json\n",
      "dataset_descriptions/nyc_open_data__Campaign Expenditures__description.json\n",
      "dataset_descriptions/nyc_open_data__Campaign Financial Analysis__description.json\n",
      "dataset_descriptions/nyc_open_data__Campaign Intermediaries__description.json\n",
      "dataset_descriptions/nyc_open_data__Campaign Public Funds Payments__description.json\n",
      "dataset_descriptions/nyc_open_data__Cancelled Projects__description.json\n",
      "dataset_descriptions/nyc_open_data__Canine Waste Dispensers__description.json\n",
      "dataset_descriptions/nyc_open_data__Capacity Projects by School__description.json\n",
      "dataset_descriptions/nyc_open_data__Capacity Projects in Process Site Locations__description.json\n",
      "dataset_descriptions/nyc_open_data__Capital Budget__description.json\n",
      "dataset_descriptions/nyc_open_data__Capital Commitment Actuals__description.json\n",
      "dataset_descriptions/nyc_open_data__Capital Commitment Plan__description.json\n",
      "dataset_descriptions/nyc_open_data__Capital Plan Funding Source__description.json\n",
      "dataset_descriptions/nyc_open_data__Capital Project Schedules and Budgets__description.json\n",
      "dataset_descriptions/nyc_open_data__Capital Project Tracker__description.json\n",
      "dataset_descriptions/nyc_open_data__Capital Projects Dashboard - Citywide Budget Spend History and Variance__description.json\n",
      "dataset_descriptions/nyc_open_data__Capital Projects Dashboard - Citywide Budget and Schedule__description.json\n",
      "dataset_descriptions/nyc_open_data__Capital Projects Dashboard - Citywide Budget and Spend by Fiscal Year__description.json\n",
      "dataset_descriptions/nyc_open_data__Capital Projects Dashboard - Citywide Schedule History and Variance__description.json\n",
      "dataset_descriptions/nyc_open_data__Capital Projects Database CPDB - Commitments__description.json\n",
      "dataset_descriptions/nyc_open_data__Capital Projects Database CPDB - Projects Points__description.json\n",
      "dataset_descriptions/nyc_open_data__Capital Projects Database CPDB - Projects Polygons__description.json\n",
      "dataset_descriptions/nyc_open_data__Capital Projects Database CPDB - Projects__description.json\n",
      "dataset_descriptions/nyc_open_data__Capital Projects__description.json\n",
      "dataset_descriptions/nyc_open_data__Carshare Locations - Curbside__description.json\n",
      "dataset_descriptions/nyc_open_data__Carshare Locations - Municipal Garages and Lots__description.json\n",
      "dataset_descriptions/nyc_open_data__Case-Related Information About Civil Litigation__description.json\n",
      "dataset_descriptions/nyc_open_data__Cash Assistance Applications for Youth Heads of Household Ages 16 - 20__description.json\n",
      "dataset_descriptions/nyc_open_data__Cash Assistance Emergency Assistance Requests__description.json\n",
      "dataset_descriptions/nyc_open_data__Cash Assistance Engagement Report__description.json\n",
      "dataset_descriptions/nyc_open_data__Cash Assistance and SNAP Cases Closed__description.json\n",
      "dataset_descriptions/nyc_open_data__Cash Assistance and SNAP Cases Rejections__description.json\n",
      "dataset_descriptions/nyc_open_data__Cash Assistance and SNAP Cases Reopenings with a Missed Benefits Cycle__description.json\n",
      "dataset_descriptions/nyc_open_data__Cash Assistance and SNAP Cases Reopenings__description.json\n",
      "dataset_descriptions/nyc_open_data__Centerline__description.json\n",
      "dataset_descriptions/nyc_open_data__Certification of No Harassment CONH Pilot Building List__description.json\n",
      "dataset_descriptions/nyc_open_data__Certified Asbestos Investigators__description.json\n",
      "dataset_descriptions/nyc_open_data__Charter Mandated Quarterly Report on Provisionals__description.json\n",
      "dataset_descriptions/nyc_open_data__Child Abuse Liaisons__description.json\n",
      "dataset_descriptions/nyc_open_data__Child Support Caseload and Collections__description.json\n",
      "dataset_descriptions/nyc_open_data__Childrens Play Areas CPAs__description.json\n",
      "dataset_descriptions/nyc_open_data__City Clerk eLobbyist Data__description.json\n",
      "dataset_descriptions/nyc_open_data__City Council Districts Water Areas Included_872g-cjhh__description.json\n",
      "dataset_descriptions/nyc_open_data__City Council Districts Water Areas Included__description.json\n",
      "dataset_descriptions/nyc_open_data__City Council January 2021 Attendance__description.json\n",
      "dataset_descriptions/nyc_open_data__City Council Legislation Bills and Local Laws__description.json\n",
      "dataset_descriptions/nyc_open_data__City Owned and Leased Property COLP__description.json\n",
      "dataset_descriptions/nyc_open_data__CityStore - The Official Store of the City of New York__description.json\n",
      "dataset_descriptions/nyc_open_data__Citywide Auto Fringe Benefits__description.json\n",
      "dataset_descriptions/nyc_open_data__Citywide Catch Basins__description.json\n",
      "dataset_descriptions/nyc_open_data__Citywide HRA- Administered Medicaid Enrollees__description.json\n",
      "dataset_descriptions/nyc_open_data__Citywide Marketing Directory__description.json\n",
      "dataset_descriptions/nyc_open_data__Citywide Public Computer Centers__description.json\n",
      "dataset_descriptions/nyc_open_data__Civil List__description.json\n",
      "dataset_descriptions/nyc_open_data__Civil Service Exams for NYCHA Residents - Local Law 163__description.json\n",
      "dataset_descriptions/nyc_open_data__Civil Service List Active__description.json\n",
      "dataset_descriptions/nyc_open_data__Civil Service List Terminated__description.json\n",
      "dataset_descriptions/nyc_open_data__Civilian Complaint Review Board Allegations Against Police Officers__description.json\n",
      "dataset_descriptions/nyc_open_data__Civilian Complaint Review Board Complaints Against Police Officers__description.json\n",
      "dataset_descriptions/nyc_open_data__Civilian Complaint Review Board Penalties__description.json\n",
      "dataset_descriptions/nyc_open_data__Civilian Complaint Review Board Police Officers__description.json\n",
      "dataset_descriptions/nyc_open_data__Claims Report - Underlying Settlements and Claims Filed Data__description.json\n",
      "dataset_descriptions/nyc_open_data__Class Size Report 2006-2007__description.json\n",
      "dataset_descriptions/nyc_open_data__Class Size Report Detail Summary 2010-2015__description.json\n",
      "dataset_descriptions/nyc_open_data__Class Size Report Distribution Summary 2010-2015__description.json\n",
      "dataset_descriptions/nyc_open_data__Clean Air Tracking System CATS Permits__description.json\n",
      "dataset_descriptions/nyc_open_data__Clergy Parking Permits__description.json\n",
      "dataset_descriptions/nyc_open_data__Commercial Bicycle Unit Inspections__description.json\n",
      "dataset_descriptions/nyc_open_data__Commissioners Correspondence__description.json\n",
      "dataset_descriptions/nyc_open_data__CommonPlace__description.json\n",
      "dataset_descriptions/nyc_open_data__Community Boards Geographic Report__description.json\n",
      "dataset_descriptions/nyc_open_data__Community Development Block Grant CDBG Eligibility by Census Tract - CSV__description.json\n",
      "dataset_descriptions/nyc_open_data__Community Districts Water areas included__description.json\n",
      "dataset_descriptions/nyc_open_data__Community Districts__description.json\n",
      "dataset_descriptions/nyc_open_data__Community Food Connection Quarterly Report__description.json\n",
      "dataset_descriptions/nyc_open_data__Compliance at-the-tap Lead and Copper Data__description.json\n",
      "dataset_descriptions/nyc_open_data__Congressional Districts water areas included__description.json\n",
      "dataset_descriptions/nyc_open_data__Congressional Districts__description.json\n",
      "dataset_descriptions/nyc_open_data__Construction-Related Incidents__description.json\n",
      "dataset_descriptions/nyc_open_data__Contract Budget by Category__description.json\n",
      "dataset_descriptions/nyc_open_data__Cooking Gas Consumption And Cost 2010 - May 2025__description.json\n",
      "dataset_descriptions/nyc_open_data__Council Members__description.json\n",
      "dataset_descriptions/nyc_open_data__CoursesTraining Provider Listing__description.json\n",
      "dataset_descriptions/nyc_open_data__Curb Metal Data__description.json\n",
      "dataset_descriptions/nyc_open_data__Current RFP__description.json\n",
      "dataset_descriptions/nyc_open_data__Current Reservoir Levels__description.json\n",
      "dataset_descriptions/nyc_open_data__DARP-ROTOW Enrollment Status of Active Tow Truck Companies__description.json\n",
      "dataset_descriptions/nyc_open_data__DCLA Programs Funding__description.json\n",
      "dataset_descriptions/nyc_open_data__DCWP Charges__description.json\n",
      "dataset_descriptions/nyc_open_data__DCWP Consumer Complaints__description.json\n",
      "dataset_descriptions/nyc_open_data__DCWP Inspections__description.json\n",
      "dataset_descriptions/nyc_open_data__DCWP Licensed Vehicles__description.json\n",
      "dataset_descriptions/nyc_open_data__DCWP Payments Received__description.json\n",
      "dataset_descriptions/nyc_open_data__DEP - Cryptosporidium And Giardia Data Set__description.json\n",
      "dataset_descriptions/nyc_open_data__DHS Daily Report Historical__description.json\n",
      "dataset_descriptions/nyc_open_data__DHS Daily Report__description.json\n",
      "dataset_descriptions/nyc_open_data__DHS Data Dashboard__description.json\n",
      "dataset_descriptions/nyc_open_data__DOB Certificate Of Occupancy__description.json\n",
      "dataset_descriptions/nyc_open_data__DOB Disciplinary Actions__description.json\n",
      "dataset_descriptions/nyc_open_data__DOB License Info__description.json\n",
      "dataset_descriptions/nyc_open_data__DOB NOW Build Elevator Device Details__description.json\n",
      "dataset_descriptions/nyc_open_data__DOB NOW Build Elevator Permit Applications__description.json\n",
      "dataset_descriptions/nyc_open_data__DOB NOW Build Limited Alteration Applications__description.json\n",
      "dataset_descriptions/nyc_open_data__DOB NOW Certificate of Occupancy__description.json\n",
      "dataset_descriptions/nyc_open_data__DOB NOW Electrical Permit Details__description.json\n",
      "dataset_descriptions/nyc_open_data__DOB NOW Elevator Safety Compliance__description.json\n",
      "dataset_descriptions/nyc_open_data__DOB NOW Safety Boiler__description.json\n",
      "dataset_descriptions/nyc_open_data__DOB NOW Safety Facades Compliance Filings__description.json\n",
      "dataset_descriptions/nyc_open_data__DOB Safety Violations__description.json\n",
      "dataset_descriptions/nyc_open_data__DOB Stalled Construction Sites__description.json\n",
      "dataset_descriptions/nyc_open_data__DOE Building Space Usage__description.json\n",
      "dataset_descriptions/nyc_open_data__DOE Funding Sources__description.json\n",
      "dataset_descriptions/nyc_open_data__DOE High School Directory 2013-2014__description.json\n",
      "dataset_descriptions/nyc_open_data__DOF Refund List - Real Property Tax__description.json\n",
      "dataset_descriptions/nyc_open_data__DOF Scofftow Case Information__description.json\n",
      "dataset_descriptions/nyc_open_data__DOF Summary of Neighborhood Sales by Neighborhood Citywide by Borough__description.json\n",
      "dataset_descriptions/nyc_open_data__DOHMH Call Center Summary__description.json\n",
      "dataset_descriptions/nyc_open_data__DOHMH Dog Bite Data__description.json\n",
      "dataset_descriptions/nyc_open_data__DOHMH HIV Service Directory__description.json\n",
      "dataset_descriptions/nyc_open_data__DOHMH HIVAIDS Annual Report__description.json\n",
      "dataset_descriptions/nyc_open_data__DOHMH Indoor Environmental Complaints__description.json\n",
      "dataset_descriptions/nyc_open_data__DOHMH New York City Restaurant Inspection Results__description.json\n",
      "dataset_descriptions/nyc_open_data__DOHMH School Cafeteria Inspections 2020-Present__description.json\n",
      "dataset_descriptions/nyc_open_data__DOP Adult Case Closings By Reason__description.json\n",
      "dataset_descriptions/nyc_open_data__DOP Adult Case Closings By Type__description.json\n",
      "dataset_descriptions/nyc_open_data__DOP Adult Case Count By Type__description.json\n",
      "dataset_descriptions/nyc_open_data__DOP Adult Investigations Ordered__description.json\n",
      "dataset_descriptions/nyc_open_data__DOP Adult Probationers Rearrested As A Percentage Of NYPD Arrest Report Monthly Average__description.json\n",
      "dataset_descriptions/nyc_open_data__DOP Adult Rearrest Rate Monthly Average__description.json\n",
      "dataset_descriptions/nyc_open_data__DOP Juvenile Case Closings By Reason__description.json\n",
      "dataset_descriptions/nyc_open_data__DOP Juvenile Case Closings By Type__description.json\n",
      "dataset_descriptions/nyc_open_data__DOP Juvenile Case Count By Type__description.json\n",
      "dataset_descriptions/nyc_open_data__DOP Juvenile Investigations Assigned__description.json\n",
      "dataset_descriptions/nyc_open_data__DOP Juvenile Probationers Rearrested As A Percentage Of NYPD Arrest Report Monthly Average__description.json\n",
      "dataset_descriptions/nyc_open_data__DOP Juvenile Rearrest Rate Monthly Average__description.json\n",
      "dataset_descriptions/nyc_open_data__DSNY Commercial Waste Zones__description.json\n",
      "dataset_descriptions/nyc_open_data__DSNY Disposal Facilities Used by Year__description.json\n",
      "dataset_descriptions/nyc_open_data__DSNY Disposal Sites Used by Facilities by Year__description.json\n",
      "dataset_descriptions/nyc_open_data__DSNY Disposal Vendors__description.json\n",
      "dataset_descriptions/nyc_open_data__DSNY Districts With Disposal Vendor Assignments__description.json\n",
      "dataset_descriptions/nyc_open_data__DSNY DonateNYC Directory__description.json\n",
      "dataset_descriptions/nyc_open_data__DSNY Garages__description.json\n",
      "dataset_descriptions/nyc_open_data__DSNY Litter Basket Inventory__description.json\n",
      "dataset_descriptions/nyc_open_data__DSNY Monthly Tonnage Data__description.json\n",
      "dataset_descriptions/nyc_open_data__DSNY Organics Collection Participation__description.json\n",
      "dataset_descriptions/nyc_open_data__DSNY Other Organics Collection Tonnages__description.json\n",
      "dataset_descriptions/nyc_open_data__DSNY Salt Usage__description.json\n",
      "dataset_descriptions/nyc_open_data__DSNY Special Waste Drop-off Sites__description.json\n",
      "dataset_descriptions/nyc_open_data__DYCD Participant Demographics__description.json\n",
      "dataset_descriptions/nyc_open_data__DYCD Program Sites__description.json\n",
      "dataset_descriptions/nyc_open_data__DYCD Runaway and Homeless Youth RHY Demographics and Services Local Law 86__description.json\n",
      "dataset_descriptions/nyc_open_data__Daily Inmates In Custody__description.json\n",
      "dataset_descriptions/nyc_open_data__Debt Affordability Statement Schedules__description.json\n",
      "dataset_descriptions/nyc_open_data__Debt Burden__description.json\n",
      "dataset_descriptions/nyc_open_data__Debt Service Major Indicators__description.json\n",
      "dataset_descriptions/nyc_open_data__Deed Restriction Database__description.json\n",
      "dataset_descriptions/nyc_open_data__Denied TW and Wholesale Market Companies__description.json\n",
      "dataset_descriptions/nyc_open_data__Detention Admissions by Community District__description.json\n",
      "dataset_descriptions/nyc_open_data__Dewatered Solids and Biosolids Allocations per Biosolids Management Practice Site__description.json\n",
      "dataset_descriptions/nyc_open_data__Digital Tax Map Air Lots__description.json\n",
      "dataset_descriptions/nyc_open_data__Digital Tax Map Condominium Units__description.json\n",
      "dataset_descriptions/nyc_open_data__Digital Tax Map Condominiums__description.json\n",
      "dataset_descriptions/nyc_open_data__Digital Tax Map Real Estate of Utility Company REUC Lots__description.json\n",
      "dataset_descriptions/nyc_open_data__Digital Tax Map Subterranean Lots__description.json\n",
      "dataset_descriptions/nyc_open_data__Directory Of Homeless Drop- In Centers__description.json\n",
      "dataset_descriptions/nyc_open_data__Directory of Benefits Access Centers__description.json\n",
      "dataset_descriptions/nyc_open_data__Directory of NYCHA Community Facilities__description.json\n",
      "dataset_descriptions/nyc_open_data__Directory of SNAP Centers__description.json\n",
      "dataset_descriptions/nyc_open_data__Distribution Sites LCR Monitoring Results__description.json\n",
      "dataset_descriptions/nyc_open_data__Dog Runs__description.json\n",
      "dataset_descriptions/nyc_open_data__Doing Business Search - Entities__description.json\n",
      "dataset_descriptions/nyc_open_data__Doing Business Search - People__description.json\n",
      "dataset_descriptions/nyc_open_data__Donations Received by City Agencies__description.json\n",
      "dataset_descriptions/nyc_open_data__Donations to Not-for-Profit Organizations Affiliated with Elected Officials__description.json\n",
      "dataset_descriptions/nyc_open_data__Drinking Water Quality Distribution Monitoring Data__description.json\n",
      "dataset_descriptions/nyc_open_data__Drivers and Attendants__description.json\n",
      "dataset_descriptions/nyc_open_data__E-Designations__description.json\n",
      "dataset_descriptions/nyc_open_data__E-Designations_hxm3-23vy__description.json\n"
     ]
    }
   ],
   "source": [
    "response = s3.list_objects_v2(Bucket=\"nyu-tandon-cs-gy-6513-project\")\n",
    "\n",
    "for obj in response.get(\"Contents\", []):\n",
    "    print(obj[\"Key\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OM9jKlPxZIWU"
   },
   "source": [
    "#DATA PROFILER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZsjeGedLXVH5"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "=== Helper functions ===\n",
    "This cell defines small utility functions used by the profiler:\n",
    "\"\"\"\n",
    "\n",
    "import warnings\n",
    "\n",
    "def _try_to_datetime(series: pd.Series) -> Tuple[pd.Series, bool]:\n",
    "    if pd.api.types.is_datetime64_any_dtype(series):\n",
    "        return series, True\n",
    "    try:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\",\n",
    "                message=\"Could not infer format, so each element will be parsed individually.*\",\n",
    "                category=UserWarning,\n",
    "            )\n",
    "            s = pd.to_datetime(series, errors=\"raise\")\n",
    "        return s, True\n",
    "    except Exception:\n",
    "        return series, False\n",
    "\n",
    "def _safe_quantiles(s: pd.Series, qs=(0.05, 0.25, 0.5, 0.75, 0.95)) -> Dict[str, float]:\n",
    "    try:\n",
    "        q = s.quantile(qs)\n",
    "        return {f\"q{int(p*100)}\": float(v) for p, v in q.items() if pd.notna(v)}\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "def _histogram(s: pd.Series, bins: int = 10) -> Dict[str, Any]:\n",
    "    arr = pd.to_numeric(s, errors=\"coerce\").dropna().to_numpy()\n",
    "    if arr.size == 0 or np.nanmax(arr) == np.nanmin(arr):\n",
    "        return {\"bins\": [], \"counts\": []}\n",
    "    counts, edges = np.histogram(arr, bins=bins)\n",
    "    centers = (edges[:-1] + edges[1:]) / 2.0\n",
    "    return {\"bins\": centers.tolist(), \"counts\": counts.astype(int).tolist()}\n",
    "\n",
    "def _top_values(s: pd.Series, k: int = 10) -> List[Dict[str, Any]]:\n",
    "    vc = s.value_counts(dropna=True).head(k)\n",
    "    total = int(s.notna().sum())\n",
    "    return [{\"value\": str(v), \"count\": int(c), \"pct\": float(c/total) if total else 0.0}\n",
    "            for v, c in vc.items()]\n",
    "\n",
    "def _text_stats(s: pd.Series) -> Dict[str, Any]:\n",
    "    s2 = s.dropna().astype(str)\n",
    "    if s2.empty:\n",
    "        return {\"avg_length\": None, \"q25_length\": None, \"q50_length\": None, \"q75_length\": None}\n",
    "    lens = s2.str.len()\n",
    "    return {\n",
    "        \"avg_length\": float(lens.mean()),\n",
    "        \"q25_length\": float(lens.quantile(0.25)),\n",
    "        \"q50_length\": float(lens.quantile(0.50)),\n",
    "        \"q75_length\": float(lens.quantile(0.75)),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UWhmVb6CMT3O"
   },
   "outputs": [],
   "source": [
    "# --- JIT-accelerated numeric stats for profiling ---\n",
    "from numba import njit\n",
    "import numpy as np\n",
    "\n",
    "@njit\n",
    "def _nanmean_numba(x):\n",
    "    total = 0.0\n",
    "    count = 0\n",
    "    for i in range(x.shape[0]):\n",
    "        v = x[i]\n",
    "        if not np.isnan(v):\n",
    "            total += v\n",
    "            count += 1\n",
    "    if count == 0:\n",
    "        return np.nan\n",
    "    return total / count\n",
    "\n",
    "@njit\n",
    "def _nanstd_numba(x):\n",
    "    mean = _nanmean_numba(x)\n",
    "    if np.isnan(mean):\n",
    "        return np.nan\n",
    "    total = 0.0\n",
    "    count = 0\n",
    "    for i in range(x.shape[0]):\n",
    "        v = x[i]\n",
    "        if not np.isnan(v):\n",
    "            diff = v - mean\n",
    "            total += diff * diff\n",
    "            count += 1\n",
    "    if count <= 1:\n",
    "        return np.nan\n",
    "    return np.sqrt(total / (count - 1))\n",
    "\n",
    "@njit\n",
    "def _nanquantile_sorted(sorted_x, q):\n",
    "    n = sorted_x.shape[0]\n",
    "    n_non_nan = n\n",
    "    # ignore trailing NaNs in a sorted array\n",
    "    for i in range(n - 1, -1, -1):\n",
    "        if np.isnan(sorted_x[i]):\n",
    "            n_non_nan -= 1\n",
    "        else:\n",
    "            break\n",
    "    if n_non_nan == 0:\n",
    "        return np.nan\n",
    "\n",
    "    pos = q * (n_non_nan - 1)\n",
    "    lo = int(np.floor(pos))\n",
    "    hi = int(np.ceil(pos))\n",
    "    if lo == hi:\n",
    "        return sorted_x[lo]\n",
    "    w = pos - lo\n",
    "    return sorted_x[lo] * (1.0 - w) + sorted_x[hi] * w\n",
    "\n",
    "@njit\n",
    "def _nan_quantiles_numba(x):\n",
    "    arr = x.copy()\n",
    "    arr.sort()\n",
    "    q25 = _nanquantile_sorted(arr, 0.25)\n",
    "    q50 = _nanquantile_sorted(arr, 0.50)\n",
    "    q75 = _nanquantile_sorted(arr, 0.75)\n",
    "    return q25, q50, q75\n",
    "\n",
    "@njit\n",
    "def _histogram_numba(x, bins):\n",
    "    # simple histogram ignoring NaNs\n",
    "    min_val = np.inf\n",
    "    max_val = -np.inf\n",
    "    for i in range(x.shape[0]):\n",
    "        v = x[i]\n",
    "        if not np.isnan(v):\n",
    "            if v < min_val:\n",
    "                min_val = v\n",
    "            if v > max_val:\n",
    "                max_val = v\n",
    "\n",
    "    if not np.isfinite(min_val) or not np.isfinite(max_val):\n",
    "        return np.empty(0, dtype=np.float64), np.empty(0, dtype=np.int64)\n",
    "\n",
    "    edges = np.linspace(min_val, max_val, bins + 1)\n",
    "    counts = np.zeros(bins, dtype=np.int64)\n",
    "\n",
    "    for i in range(x.shape[0]):\n",
    "        v = x[i]\n",
    "        if np.isnan(v):\n",
    "            continue\n",
    "        if v == max_val:\n",
    "            idx = bins - 1\n",
    "        else:\n",
    "            idx = int((v - min_val) / (max_val - min_val) * bins)\n",
    "            if idx < 0:\n",
    "                idx = 0\n",
    "            elif idx >= bins:\n",
    "                idx = bins - 1\n",
    "        counts[idx] += 1\n",
    "\n",
    "    centers = (edges[:-1] + edges[1:]) * 0.5\n",
    "    return centers, counts\n",
    "\n",
    "@njit\n",
    "def _numeric_summary_numba(x, bins):\n",
    "    min_val = np.inf\n",
    "    max_val = -np.inf\n",
    "    for i in range(x.shape[0]):\n",
    "        v = x[i]\n",
    "        if not np.isnan(v):\n",
    "            if v < min_val:\n",
    "                min_val = v\n",
    "            if v > max_val:\n",
    "                max_val = v\n",
    "    if not np.isfinite(min_val) or not np.isfinite(max_val):\n",
    "        min_val = np.nan\n",
    "        max_val = np.nan\n",
    "\n",
    "    mean_val = _nanmean_numba(x)\n",
    "    std_val = _nanstd_numba(x)\n",
    "    q25, q50, q75 = _nan_quantiles_numba(x)\n",
    "    centers, counts = _histogram_numba(x, bins)\n",
    "    return min_val, max_val, mean_val, std_val, q25, q50, q75, centers, counts\n",
    "\n",
    "def numeric_stats_from_series(s: pd.Series, hist_bins: int = 10) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Wrapper that bridges pandas -> jitted NumPy, and back into a JSON-serializable dict.\n",
    "    \"\"\"\n",
    "    s_num = pd.to_numeric(s, errors=\"coerce\")\n",
    "    arr = s_num.to_numpy(dtype=np.float64)\n",
    "\n",
    "    min_v, max_v, mean_v, std_v, q25, q50, q75, centers, counts = _numeric_summary_numba(\n",
    "        arr, hist_bins\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"min\": None if np.isnan(min_v) else float(min_v),\n",
    "        \"max\": None if np.isnan(max_v) else float(max_v),\n",
    "        \"mean\": None if np.isnan(mean_v) else float(mean_v),\n",
    "        \"std\": None if np.isnan(std_v) else float(std_v),\n",
    "        \"quantiles\": {\n",
    "            \"q25\": None if np.isnan(q25) else float(q25),\n",
    "            \"q50\": None if np.isnan(q50) else float(q50),\n",
    "            \"q75\": None if np.isnan(q75) else float(q75),\n",
    "        },\n",
    "        \"histogram\": {\n",
    "            \"bin_centers\": [float(c) for c in centers],\n",
    "            \"counts\": [int(c) for c in counts],\n",
    "        },\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ieJxqlyoUi4C"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "=== Semantic type inference ===\n",
    "This cell defines `_semantic_type`, which labels each column as:\n",
    "  boolean, integer, float, datetime, categorical, or text.\n",
    "The function uses dtype checks + heuristics (e.g., 80% successful datetime parsing).\n",
    "The output is more meaningful than raw pandas dtypes.\n",
    "\"\"\"\n",
    "\n",
    "import warnings\n",
    "\n",
    "def _semantic_type(series: pd.Series) -> str:\n",
    "    if pd.api.types.is_bool_dtype(series):\n",
    "        return \"boolean\"\n",
    "    if pd.api.types.is_numeric_dtype(series):\n",
    "        return \"integer\" if pd.api.types.is_integer_dtype(series) else \"float\"\n",
    "    if pd.api.types.is_datetime64_any_dtype(series):\n",
    "        return \"datetime\"\n",
    "\n",
    "    # try a light datetime guess on a small sample\n",
    "    sample = series.dropna().astype(str).head(50)\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\n",
    "            \"ignore\",\n",
    "            message=\"Could not infer format, so each element will be parsed individually.*\",\n",
    "            category=UserWarning,\n",
    "        )\n",
    "        parsed = pd.to_datetime(sample, errors=\"coerce\")\n",
    "\n",
    "    if parsed.notna().mean() >= 0.8:\n",
    "        return \"datetime\"\n",
    "\n",
    "    nunique = series.nunique(dropna=True)\n",
    "    ratio = nunique / max(len(series), 1)\n",
    "    return \"categorical\" if nunique <= 50 and ratio <= 0.5 else \"text\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ktdQ0OQhVR81"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "=== Geospatial + Key Detection ===\n",
    "This cell provides:\n",
    "  - _is_lat / _is_lon: detect latitude/longitude columns\n",
    "  - _geospatial_hints: determine whether dataset has lat/lon pair\n",
    "  - _primary_key_candidates: detect columns that are unique + non-null\n",
    "Used later to enrich dataset-level metadata.\n",
    "\"\"\"\n",
    "\n",
    "_LAT = {\"lat\", \"latitude\"}; _LON = {\"lon\", \"lng\", \"long\", \"longitude\"}\n",
    "\n",
    "def _is_lat(name: str, s: pd.Series) -> bool:\n",
    "    name = name.lower()\n",
    "    if name in _LAT and pd.api.types.is_numeric_dtype(s): return True\n",
    "    return pd.api.types.is_numeric_dtype(s) and s.min(skipna=True) >= -90 and s.max(skipna=True) <= 90\n",
    "\n",
    "def _is_lon(name: str, s: pd.Series) -> bool:\n",
    "    name = name.lower()\n",
    "    if name in _LON and pd.api.types.is_numeric_dtype(s): return True\n",
    "    return pd.api.types.is_numeric_dtype(s) and s.min(skipna=True) >= -180 and s.max(skipna=True) <= 180\n",
    "\n",
    "def _geospatial_hints(df: pd.DataFrame) -> Dict[str, Any]:\n",
    "    lat = next((c for c in df.columns if _is_lat(c, df[c])), None)\n",
    "    lon = next((c for c in df.columns if _is_lon(c, df[c]) and c != lat), None)\n",
    "    return {\"lat_column\": lat, \"lon_column\": lon, \"has_latlon_pair\": lat is not None and lon is not None}\n",
    "\n",
    "def _primary_key_candidates(df: pd.DataFrame) -> List[str]:\n",
    "    return [c for c in df.columns if df[c].isna().mean() == 0 and df[c].is_unique]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s7nkjHsyVYMz"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "=== Full Data Profiler ===\n",
    "This cell defines `profile_dataframe(df)` which:\n",
    "  - Profiles each column (stats, missingness, semantic type, distributions, etc.)\n",
    "  - Computes dataset-level metadata (row count, column count, pk candidates, geospatial hints)\n",
    "  - Returns everything as a structured JSON-friendly dictionary.\n",
    "This is the main profiling engine.\n",
    "\"\"\"\n",
    "def profile_dataframe(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    sample_rows: int = 1_000_000,\n",
    "    hist_bins: int = 10,\n",
    "    topk_categorical: int = 10,\n",
    ") -> Dict[str, Any]:\n",
    "    if len(df) > sample_rows:\n",
    "        df = df.sample(sample_rows, random_state=42)\n",
    "\n",
    "    n_rows, n_cols = df.shape\n",
    "    mem_bytes = int(df.memory_usage(deep=True).sum())\n",
    "\n",
    "    column_summaries = {}\n",
    "    type_counts: Dict[str, int] = {}\n",
    "\n",
    "    for c in df.columns:\n",
    "        s = df[c]\n",
    "        sem = _semantic_type(s)\n",
    "        type_counts[sem] = type_counts.get(sem, 0) + 1\n",
    "\n",
    "        non_null = int(s.notna().sum())\n",
    "        null_frac = float(s.isna().mean())\n",
    "        nunique = int(s.nunique(dropna=True))\n",
    "        is_unique = s.is_unique and null_frac == 0.0\n",
    "        is_const = nunique == 1\n",
    "\n",
    "        info: Dict[str, Any] = {\n",
    "            \"dtype\": str(s.dtype),\n",
    "            \"semantic_type\": sem,\n",
    "            \"non_null_count\": non_null,\n",
    "            \"null_fraction\": null_frac,\n",
    "            \"n_unique\": nunique,\n",
    "            \"is_unique\": bool(is_unique),\n",
    "            \"is_constant\": bool(is_const),\n",
    "            \"example_values\": [str(v) for v in s.dropna().unique()[:5]],\n",
    "        }\n",
    "\n",
    "        if sem in {\"integer\", \"float\"}:\n",
    "            # Use JIT-accelerated numeric stats\n",
    "            numeric_stats = numeric_stats_from_series(s, hist_bins=hist_bins)\n",
    "            info.update(numeric_stats)\n",
    "\n",
    "        elif sem == \"datetime\":\n",
    "            s_dt, ok = _try_to_datetime(s)\n",
    "            if ok and not s_dt.dropna().empty:\n",
    "                info.update({\n",
    "                    \"min\": s_dt.min(),\n",
    "                    \"max\": s_dt.max(),\n",
    "                    \"is_monotonic_increasing\": bool(s_dt.is_monotonic_increasing),\n",
    "                    \"is_monotonic_decreasing\": bool(s_dt[::-1].is_monotonic_increasing),\n",
    "                })\n",
    "\n",
    "        elif sem == \"categorical\":\n",
    "            info[\"top_values\"] = _top_values(s.astype(\"string\"), k=topk_categorical)\n",
    "\n",
    "        elif sem == \"text\":\n",
    "            info[\"text_stats\"] = _text_stats(s)\n",
    "\n",
    "        column_summaries[c] = info\n",
    "\n",
    "    # dataset-level\n",
    "    geohints = _geospatial_hints(df)\n",
    "    pk = _primary_key_candidates(df)\n",
    "\n",
    "    time_min, time_max = None, None\n",
    "    for c, info in column_summaries.items():\n",
    "        if info[\"semantic_type\"] == \"datetime\" and info.get(\"min\") is not None:\n",
    "            tmin, tmax = info[\"min\"], info[\"max\"]\n",
    "            time_min = tmin if time_min is None or (tmin is not None and tmin < time_min) else time_min\n",
    "            time_max = tmax if time_max is None or (tmax is not None and tmax > time_max) else time_max\n",
    "\n",
    "    return {\n",
    "        \"num_rows\": int(n_rows),\n",
    "        \"num_columns\": int(n_cols),\n",
    "        \"memory_usage_bytes\": mem_bytes,\n",
    "        \"duplicate_row_fraction\": float((n_rows - len(df.drop_duplicates())) / n_rows) if n_rows else 0.0,\n",
    "        \"type_counts\": type_counts,\n",
    "        \"time_coverage\": {\"min\": time_min, \"max\": time_max},\n",
    "        \"geospatial_hints\": geohints,\n",
    "        \"primary_key_candidates\": pk,\n",
    "        \"columns\": column_summaries,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YJtArOz4ZRzt"
   },
   "source": [
    "# DATA SIGNAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qmGiDkGA0y37"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict\n",
    "from typing import Tuple\n",
    "from typing import Any\n",
    "from typing import List\n",
    "\n",
    "DOMAIN_KEYWORDS = {\n",
    "    \"cities_geo\": [\n",
    "        \"city\", \"city_name\", \"latitude\", \"longitude\",\n",
    "        \"lat\", \"lon\", \"country\", \"region\", \"state\", \"county\"\n",
    "    ],\n",
    "    \"health\": [\n",
    "        \"patient\", \"hospital\", \"diagnosis\", \"disease\",\n",
    "        \"blood_pressure\", \"cholesterol\", \"heart_rate\",\n",
    "        \"bmi\", \"hospital_id\"\n",
    "    ],\n",
    "    \"finance\": [\n",
    "        \"price\", \"revenue\", \"profit\", \"stock\", \"ticker\",\n",
    "        \"transaction\", \"amount\", \"balance\", \"account_id\"\n",
    "    ],\n",
    "    \"demographics\": [\n",
    "        \"age\", \"gender\", \"income\", \"education\", \"occupation\",\n",
    "        \"household\", \"population\"\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def infer_domain_from_profile(profile: dict) -> str:\n",
    "    \"\"\"\n",
    "    Heuristic: look at column names and pick the domain\n",
    "    with the highest keyword overlap. Returns 'generic'\n",
    "    if nothing matches.\n",
    "    \"\"\"\n",
    "    columns = profile.get(\"columns\", {})\n",
    "    col_names = [name.lower() for name in columns.keys()]\n",
    "\n",
    "    best_domain = \"generic\"\n",
    "    best_score = 0\n",
    "\n",
    "    for domain, keywords in DOMAIN_KEYWORDS.items():\n",
    "        score = 0\n",
    "        for col in col_names:\n",
    "            for kw in keywords:\n",
    "                if kw in col:\n",
    "                    score += 1\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_domain = domain\n",
    "\n",
    "    if best_score == 0:\n",
    "        return \"generic\"\n",
    "    return best_domain\n",
    "\n",
    "\n",
    "def build_data_signals(profile: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Turn the raw profiler output into higher-level 'data signals'\n",
    "    that are easier for an LLM to consume.\n",
    "    \"\"\"\n",
    "    columns = profile.get(\"columns\", {})\n",
    "    dataset = profile.get(\"dataset\", {})\n",
    "\n",
    "    # Handle both naming variants just in case\n",
    "    n_rows = dataset.get(\"n_rows\") or dataset.get(\"num_rows\")\n",
    "    n_cols = dataset.get(\"n_cols\") or dataset.get(\"num_columns\")\n",
    "\n",
    "    # Dataset-level signals\n",
    "    # Count semantic types\n",
    "    type_counts: Dict[str, int] = {}\n",
    "    total_missing = 0\n",
    "    for name, info in columns.items():\n",
    "        sem = info.get(\"semantic_type\")\n",
    "        if sem:\n",
    "            type_counts[sem] = type_counts.get(sem, 0) + 1\n",
    "        total_missing += int(info.get(\"n_missing\", 0))\n",
    "\n",
    "    total_cells = n_rows * n_cols if (n_rows is not None and n_cols is not None) else None\n",
    "    overall_missing_fraction = (\n",
    "        float(total_missing / total_cells) if total_cells and total_cells > 0 else None\n",
    "    )\n",
    "\n",
    "    # geospatial + primary keys may or may not exist depending on your profiler version\n",
    "    geospatial = dataset.get(\"geospatial\") or dataset.get(\"geospatial_hints\") or {}\n",
    "    pk_candidates = dataset.get(\"primary_keys\") or dataset.get(\"primary_key_candidates\") or []\n",
    "\n",
    "    dataset_signals: Dict[str, Any] = {\n",
    "        \"n_rows\": n_rows,\n",
    "        \"n_cols\": n_cols,\n",
    "        \"type_counts\": type_counts,                      # how many numeric / text / datetime / etc\n",
    "        \"overall_missing_fraction\": overall_missing_fraction,\n",
    "        \"has_geospatial\": bool(geospatial.get(\"has_latlon_pair\")),\n",
    "        \"lat_column\": geospatial.get(\"lat_column\"),\n",
    "        \"lon_column\": geospatial.get(\"lon_column\"),\n",
    "        \"primary_key_candidates\": pk_candidates,\n",
    "    }\n",
    "\n",
    "    # ---- Per-column signals ----\n",
    "    column_signals: List[Dict[str, Any]] = []\n",
    "\n",
    "    for name, info in columns.items():\n",
    "        sem = info.get(\"semantic_type\")\n",
    "        n_missing = int(info.get(\"n_missing\", 0))\n",
    "        nunique = int(info.get(\"nunique\") or info.get(\"n_unique\", 0))\n",
    "\n",
    "        missing_fraction = float(n_missing / n_rows) if n_rows and n_rows > 0 else 0.0\n",
    "\n",
    "        # example values from top_values (if present)\n",
    "        top_vals = info.get(\"top_values\") or []\n",
    "        example_values = [tv.get(\"value\") for tv in top_vals[:3]] if isinstance(top_vals, list) else []\n",
    "\n",
    "        # use 5% and 95% quantiles as an approximate range, if numeric\n",
    "        quantiles = info.get(\"quantiles\") or {}\n",
    "        approx_min = quantiles.get(\"0.05\")\n",
    "        approx_max = quantiles.get(\"0.95\")\n",
    "\n",
    "        # simple heuristics for \"id-like\" and \"binary\" columns\n",
    "        is_id_like = bool(n_rows and nunique == n_rows and missing_fraction == 0.0)\n",
    "        is_binary = bool(\n",
    "            nunique == 2\n",
    "            and sem in {\"integer\", \"float\", \"boolean\", \"categorical\"}\n",
    "        )\n",
    "\n",
    "        column_signals.append(\n",
    "            {\n",
    "                \"name\": name,\n",
    "                \"semantic_type\": sem,\n",
    "                \"missing_fraction\": missing_fraction,\n",
    "                \"n_unique\": nunique,\n",
    "                \"is_id_like\": is_id_like,           # good candidate for primary key / identifier\n",
    "                \"is_binary\": is_binary,             # good for yes/no flags\n",
    "                \"approx_min\": approx_min,           # for numeric/datetime: rough range\n",
    "                \"approx_max\": approx_max,\n",
    "                \"example_values\": example_values,   # a few sample values as strings\n",
    "            }\n",
    "        )\n",
    "\n",
    "    domain = infer_domain_from_profile(profile)\n",
    "\n",
    "     #  Semantic tags / topic hints \n",
    "    tags = set()\n",
    "\n",
    "    col_names = list(columns.keys())\n",
    "    col_names_lower = [name.lower() for name in col_names]\n",
    "    all_col_text = \" \".join(col_names_lower)\n",
    "\n",
    "    type_counts = dataset_signals.get(\"type_counts\", {}) or {}\n",
    "\n",
    "    # Geospatial-ish\n",
    "    if any(tok in all_col_text for tok in [\"lat\", \"lon\", \"latitude\", \"longitude\", \"city\", \"country\", \"region\", \"borough\", \"address\"]):\n",
    "        tags.add(\"geospatial\")\n",
    "\n",
    "    # Time / events\n",
    "    if any(tok in all_col_text for tok in [\"date\", \"time\", \"year\", \"month\", \"timestamp\", \"day\"]):\n",
    "        tags.add(\"temporal\")\n",
    "\n",
    "    # Finance / economics\n",
    "    if any(tok in all_col_text for tok in [\"price\", \"cost\", \"income\", \"revenue\", \"sales\", \"amount\", \"budget\"]):\n",
    "        tags.add(\"financial\")\n",
    "\n",
    "    # Demographics / people\n",
    "    if any(tok in all_col_text for tok in [\"age\", \"gender\", \"sex\", \"student\", \"employee\", \"customer\", \"user\"]):\n",
    "        tags.add(\"demographics\")\n",
    "\n",
    "    # Text-heavy datasets\n",
    "    num_text_like = sum(\n",
    "        count for t, count in type_counts.items()\n",
    "        if str(t).lower() in {\"text\", \"string\", \"categorical\"}\n",
    "    )\n",
    "    if num_text_like and num_text_like >= len(type_counts) / 2:\n",
    "        tags.add(\"text-heavy\")\n",
    "\n",
    "    # Metrics / numeric-heavy\n",
    "    num_numeric_like = sum(\n",
    "        count for t, count in type_counts.items()\n",
    "        if any(kw in str(t).lower() for kw in [\"int\", \"float\", \"number\", \"numeric\"])\n",
    "    )\n",
    "    if num_numeric_like and num_numeric_like >= len(type_counts) / 2:\n",
    "        tags.add(\"numeric-heavy\")\n",
    "\n",
    "    # Attach tags into dataset_signals and top-level signals\n",
    "    dataset_signals[\"semantic_tags\"] = sorted(tags)\n",
    "\n",
    "    return {\n",
    "        \"dataset_signals\": dataset_signals,\n",
    "        \"column_signals\": column_signals,\n",
    "        \"domain\": domain,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yWjDFWHcWBZj"
   },
   "source": [
    "# SUMMARY GENERATOR (LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "kY1UKOCp0CLR"
   },
   "outputs": [],
   "source": [
    "\n",
    "DOMAIN_PROMPT_HINTS = {\n",
    "    \"cities_geo\": (\n",
    "        \"The dataset appears to be about cities and geographic indicators \"\n",
    "        \"(e.g., air quality, population density, income, transport, happiness). \"\n",
    "        \"Emphasize how columns relate to urban quality-of-life, cross-city comparisons, \"\n",
    "        \"and geographic coverage.\"\n",
    "    ),\n",
    "    \"health\": (\n",
    "        \"The dataset appears to be about health / clinical data. \"\n",
    "        \"Emphasize what each column measures clinically, potential privacy issues, \"\n",
    "        \"and common analytical tasks like risk prediction or outcome analysis.\"\n",
    "    ),\n",
    "    \"finance\": (\n",
    "        \"The dataset appears to be about finance or transactions. \"\n",
    "        \"Emphasize monetary units, time periods, and typical tasks like forecasting, \"\n",
    "        \"risk analysis, and portfolio evaluation.\"\n",
    "    ),\n",
    "    \"demographics\": (\n",
    "        \"The dataset appears to be about demographics / population statistics. \"\n",
    "        \"Emphasize population segments, sampling, and fairness / bias considerations.\"\n",
    "    ),\n",
    "    \"generic\": (\n",
    "        \"The dataset domain is not clearly identified. \"\n",
    "        \"Give a general-purpose description suitable for data catalog documentation.\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "def summarize_dataset_with_llm(\n",
    "    profile: Dict[str, Any],\n",
    "    signals: Dict[str, Any],\n",
    "    use_domain_hint: bool = True,\n",
    "    model: str = openaimodel,\n",
    "    side_info_text: Optional[str] = None,\n",
    ") -> str:\n",
    "\n",
    "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "    # ---- Convert dict ‚Üí JSON ----\n",
    "    dataset_summary_json = json.dumps(\n",
    "        {\n",
    "            \"dataset_signals\": signals.get(\"dataset_signals\", {}),\n",
    "            \"column_signals\": signals.get(\"column_signals\", []),\n",
    "        },\n",
    "        indent=2,\n",
    "        default=str\n",
    "    )\n",
    "\n",
    "    # Semantic tags (topic hints) from signals, if present\n",
    "    semantic_tags = (\n",
    "        signals.get(\"dataset_signals\", {})\n",
    "        .get(\"semantic_tags\", [])\n",
    "        or []\n",
    "    )\n",
    "    semantic_tags_text = \", \".join(semantic_tags) if semantic_tags else \"none\"\n",
    "\n",
    "    data_signals = signals.get(\"dataset_signals\", {}) or {}\n",
    "    domain = data_signals.get(\"domain\", \"generic\")\n",
    "    domain_hint = \"\"\n",
    "\n",
    "\n",
    "    if use_domain_hint:\n",
    "        domain = signals.get(\"domain\", \"generic\")\n",
    "        domain_hint = DOMAIN_PROMPT_HINTS.get(domain, DOMAIN_PROMPT_HINTS[\"generic\"])\n",
    "    else:\n",
    "        domain = \"generic\"\n",
    "        domain_hint = (\n",
    "            \"Ignore any specific domain assumptions and write a generic dataset card \"\n",
    "            \"without mentioning a particular application area.\"\n",
    "        )\n",
    "\n",
    "    side_info_section = \"\"\n",
    "    if side_info_text:\n",
    "        side_info_section = f\"\"\"\n",
    "        Additional side information about this dataset (README or documentation snippet):\n",
    "\n",
    "        {side_info_text}\n",
    "        \"\"\"\n",
    "    # ---- Dataset-card prompt ----\n",
    "    user_prompt = f\"\"\"\n",
    "        You are a data documentation assistant.\n",
    "\n",
    "        High-level domain: {domain!r}.\n",
    "        Domain-specific guidance: {domain_hint}\n",
    "\n",
    "        A machine-generated content summary of a tabular dataset in this JSON format:\n",
    "\n",
    "        {dataset_summary_json}\n",
    "\n",
    "        In addition, here are semantic topic tags inferred from the schema:\n",
    "        {semantic_tags_text}\n",
    "\n",
    "        The JSON has keys such as:\n",
    "        - \"dataset_signals\" (n_rows, n_cols, type_counts, overall_missing_fraction, has_geospatial, etc.)\n",
    "        - \"column_signals\": a list of columns with fields like name, semantic_type, missing_fraction, n_unique, example_values, etc.\n",
    "\n",
    "        Your job is to write a VERY concise ‚Äúdataset card‚Äù style description that:\n",
    "        - Uses the JSON only to infer what the data contains and how it can be used.\n",
    "        - Uses the user fields to tailor the framing to their interest.\n",
    "        - Does NOT invent specific values (dates, row counts, monetary amounts, percentages) if they are not clearly implied by the JSON.\n",
    "\n",
    "        =====================\n",
    "        OUTPUT REQUIREMENTS\n",
    "        =====================\n",
    "\n",
    "        You MUST output in JSON format and follow this exact structure and HARD length caps:\n",
    "        1) Section: \"Columns\"\n",
    "           - A list of column names as key and types as value\n",
    "        2) Section: \"Key facts\"\\\n",
    "           - Output 3‚Äì5 bullet points.\n",
    "           - Each bullet MUST be ‚â§ 18 words.\n",
    "           - Focus on: what each row roughly represents, main entities, key columns, important datatypes (dates, geo, text), and granularity.\n",
    "           - If counts (rows, columns) are missing or null, speak qualitatively (e.g., \"many records\") instead of making them up.\n",
    "\n",
    "        3) Section: \"Use-cases\"\n",
    "           - Output 3‚Äì5 bullet points.\n",
    "           - Each bullet MUST be ‚â§ 18 words.\n",
    "           - Tailor to TOPIC and POTENTIAL_ANALYSES.\n",
    "           - Include at least one analytical idea and one operational / business question.\n",
    "\n",
    "        4) Section: \"Caveats\"\n",
    "           - Output 3‚Äì5 bullet points.\n",
    "           - Each bullet MUST be ‚â§ 18 words.\n",
    "           - Mention schema limitations, missingness, lack of true coordinates, time coverage uncertainty, and any obvious biases from the columns.\n",
    "           - If TIME_AND_GEO_COVERAGE is broader than what the schema suggests, flag that as a potential mismatch.\n",
    "\n",
    "        5) Section: \"Overview\"\n",
    "           - A single paragraph (NO bullets).\n",
    "           - The paragraph MUST be between 40 and 200 words.\n",
    "           - Add sentences that:\n",
    "             * briefly state what each row represents, and\n",
    "             * mention key columns by name (e.g., \"School Year\", \"Borough\", \"Region\", \"Program\").\n",
    "\n",
    "        =====================\n",
    "        STYLE & CONSTRAINTS\n",
    "        =====================\n",
    "\n",
    "        - Do NOT exceed any of the word caps, even if you must omit information.\n",
    "        - Do NOT mention internal JSON structure, keys, or field names like \"dataset_signals\" or \"column_signals\".\n",
    "        - Do NOT say \"the JSON says\" or \"the schema indicates\"; write as a normal human-facing description.\n",
    "        - Do NOT fabricate: if something is unknown or ambiguous from the JSON, either omit it or state it as uncertain.\n",
    "        - Be clear and non-technical; assume an analytically literate but non-expert reader.\n",
    "        - Output ONLY the five sections in this order, with these exact headings:\n",
    "        - Columns\n",
    "        - Key facts\n",
    "        - Use-cases\n",
    "        - Caveats\n",
    "        - Overview\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "    response = client.responses.create(\n",
    "        model=model,\n",
    "        input=user_prompt,\n",
    "    )\n",
    "\n",
    "    return response.output_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fb8edbd1"
   },
   "outputs": [],
   "source": [
    "def verify_column_names_and_types_deterministic(\n",
    "    ground_truth_profile: Dict[str, Any],\n",
    "    generated_summary_str: str\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Deterministically verifies column names and their semantic types.\n",
    "    \"\"\"\n",
    "    # Parse the generated_summary JSON string into a dictionary\n",
    "    generated_summary_dict = json.loads(generated_summary_str)\n",
    "\n",
    "    # Extract column names and types from the generated summary\n",
    "    generated_columns = generated_summary_dict.get(\"Columns\", {})\n",
    "\n",
    "    # Extract column names and types from the ground truth profile\n",
    "    ground_truth_columns = {\n",
    "        name: info.get(\"semantic_type\")\n",
    "        for name, info in ground_truth_profile.get(\"columns\", {}).items()\n",
    "    }\n",
    "\n",
    "    discrepancies = []\n",
    "\n",
    "    # 1. Check for columns present in ground truth but missing in generated summary\n",
    "    for gt_col_name, gt_sem_type in ground_truth_columns.items():\n",
    "        # Standardize ground truth column name for comparison )\n",
    "        standardized_gt_col_name = gt_col_name.replace('\\n', ' ')\n",
    "\n",
    "        found_in_generated = False\n",
    "        for gen_col_name, gen_sem_type in generated_columns.items():\n",
    "            if gen_col_name == gt_col_name or gen_col_name == standardized_gt_col_name:\n",
    "                found_in_generated = True\n",
    "                # Check semantic type consistency\n",
    "                if gt_sem_type != gen_sem_type:\n",
    "                    discrepancies.append(\n",
    "                        f\"Column '{gt_col_name}': Ground truth type '{gt_sem_type}' \"\n",
    "                        f\"does not match generated type '{gen_sem_type}'.\"\n",
    "                    )\n",
    "                break\n",
    "\n",
    "        if not found_in_generated:\n",
    "            discrepancies.append(\n",
    "                f\"Column '{gt_col_name}' (type: {gt_sem_type}) is in ground truth but missing from generated summary.\"\n",
    "            )\n",
    "\n",
    "    # 2. Check for columns present in generated summary but missing in ground truth\n",
    "    for gen_col_name, gen_sem_type in generated_columns.items():\n",
    "        found_in_ground_truth = False\n",
    "        for gt_col_name in ground_truth_columns.keys():\n",
    "            standardized_gt_col_name = gt_col_name.replace('\\n', ' ')\n",
    "            if gen_col_name == gt_col_name or gen_col_name == standardized_gt_col_name:\n",
    "                found_in_ground_truth = True\n",
    "                break\n",
    "        if not found_in_ground_truth:\n",
    "            discrepancies.append(\n",
    "                f\"Column '{gen_col_name}' (type: {gen_sem_type}) is in generated summary but missing from ground truth.\"\n",
    "            )\n",
    "\n",
    "    return {\n",
    "        \"column_type_discrepancies\": discrepancies,\n",
    "        \"status\": \"Passed\" if not discrepancies else \"Failed\"\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52f58f65"
   },
   "source": [
    "### Evaluation Section\n",
    "\n",
    "This section evaluates the generated summary against a ground truth summary using ROUGE-L and BERTScore metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WrvmYLqvyuZV"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Union, Dict, Any\n",
    "\n",
    "def parse_llm_summary(raw: Union[str, Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    if isinstance(raw, dict):\n",
    "        return raw\n",
    "\n",
    "    if raw is None:\n",
    "        raise ValueError(\"summary is None; did the LLM call fail?\")\n",
    "\n",
    "    text = str(raw).strip()\n",
    "\n",
    "    # Strip ```json ... ``` or ``` ... ``` fences if ever present\n",
    "    if text.startswith(\"```\"):\n",
    "        lines = text.splitlines()\n",
    "        if lines and lines[0].startswith(\"```\"):\n",
    "            lines = lines[1:]\n",
    "        if lines and lines[-1].startswith(\"```\"):\n",
    "            lines = lines[:-1]\n",
    "        text = \"\\n\".join(lines).strip()\n",
    "\n",
    "    start = text.find(\"{\")\n",
    "    end = text.rfind(\"}\")\n",
    "    if start != -1 and end != -1:\n",
    "        text = text[start:end+1]\n",
    "\n",
    "    return json.loads(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "f0081595"
   },
   "outputs": [],
   "source": [
    "from typing import Dict, Any, Optional, Union\n",
    "\n",
    "def extract_overview_from_summary(summary_json: Union[Dict[str, Any], str, None]) -> str:\n",
    "    \"\"\"Safely extract the 'Overview' field from a generated or existing summary JSON.\"\"\"\n",
    "    if summary_json is None:\n",
    "        return \"\"\n",
    "\n",
    "    if isinstance(summary_json, str):\n",
    "        try:\n",
    "            summary_json = parse_llm_summary(summary_json)\n",
    "        except Exception:\n",
    "            return str(summary_json).strip()\n",
    "\n",
    "    if isinstance(summary_json, dict):\n",
    "        return str(summary_json.get(\"Overview\", \"\")).strip()\n",
    "\n",
    "    return str(summary_json).strip()\n",
    "\n",
    "\n",
    "def evaluate_generated_overview_vs_reference(\n",
    "    generated_summary_json: Dict[str, Any],\n",
    "    reference_overview_text: str,\n",
    "    reference_label: str = \"Reference Overview\",\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Compute ROUGE-L and BERTScore between generated Overview and a reference Overview.\n",
    "\n",
    "    Returns a dict with metric values, and also prints a human-readable report.\n",
    "    \"\"\"\n",
    "    from rouge_score import rouge_scorer\n",
    "    from evaluate import load\n",
    "\n",
    "    generated_overview = extract_overview_from_summary(generated_summary_json)\n",
    "    reference_overview_text = (reference_overview_text or \"\").strip()\n",
    "\n",
    "    print(\"Generated Summary Overview:\\n\", generated_overview, \"\\n\")\n",
    "    print(f\"{reference_label}:\\n\", reference_overview_text, \"\\n\")\n",
    "\n",
    "    # Guard\n",
    "    if not reference_overview_text:\n",
    "        print(\"No reference Overview provided; skipping ROUGE/BERTScore.\")\n",
    "        return {\n",
    "            \"skipped\": True,\n",
    "            \"reason\": \"empty_reference\",\n",
    "            \"rougeL\": None,\n",
    "            \"bertscore\": None,\n",
    "        }\n",
    "\n",
    "    # ROUGE-L\n",
    "    scorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
    "    rouge_scores = scorer.score(reference_overview_text, generated_overview)\n",
    "    rougeL = rouge_scores[\"rougeL\"]\n",
    "\n",
    "    print(\"ROUGE-L Results:\")\n",
    "    print(f\"  Precision: {rougeL.precision:.4f}\")\n",
    "    print(f\"  Recall:    {rougeL.recall:.4f}\")\n",
    "    print(f\"  F1:        {rougeL.fmeasure:.4f}\")\n",
    "\n",
    "    # BERTScore\n",
    "    bertscore = load(\"bertscore\")\n",
    "    results = bertscore.compute(\n",
    "        predictions=[generated_overview],\n",
    "        references=[reference_overview_text],\n",
    "        lang=\"en\",\n",
    "    )\n",
    "\n",
    "    bert_dict = {\n",
    "        \"precision\": float(results[\"precision\"][0]),\n",
    "        \"recall\": float(results[\"recall\"][0]),\n",
    "        \"f1\": float(results[\"f1\"][0]),\n",
    "    }\n",
    "\n",
    "    print(\"\\nBERTScore Results:\")\n",
    "    print(f\"  Precision: {bert_dict['precision']:.4f}\")\n",
    "    print(f\"  Recall:    {bert_dict['recall']:.4f}\")\n",
    "    print(f\"  F1:        {bert_dict['f1']:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"skipped\": False,\n",
    "        \"rougeL\": {\n",
    "            \"precision\": float(rougeL.precision),\n",
    "            \"recall\": float(rougeL.recall),\n",
    "            \"f1\": float(rougeL.fmeasure),\n",
    "        },\n",
    "        \"bertscore\": bert_dict,\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_summary_vs_baseline(\n",
    "    generated_summary_json: Dict[str, Any],\n",
    "    baseline_text: str,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Backward-compatible wrapper for the earlier notebook flow.\"\"\"\n",
    "    return evaluate_generated_overview_vs_reference(\n",
    "        generated_summary_json=generated_summary_json,\n",
    "        reference_overview_text=baseline_text,\n",
    "        reference_label=\"Ground-truth-based Baseline Summary\",\n",
    "    )\n",
    "\n",
    "\n",
    "# Example (safe-guarded) usage:\n",
    "if \"summary\" in globals() and \"baseline_summary\" in globals():\n",
    "    try:\n",
    "        generated_summary_dict = parse_llm_summary(summary)\n",
    "        evaluate_summary_vs_baseline(generated_summary_dict, baseline_summary)\n",
    "    except Exception as e:\n",
    "        print(f\"Evaluation example skipped due to error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c585f238"
   },
   "outputs": [],
   "source": [
    "def verify_summary_with_llm(\n",
    "    ground_truth_profile: Dict[str, Any],\n",
    "    generated_summary_str: str,\n",
    "    model: str = openaimodel\n",
    ") -> str:\n",
    "\n",
    "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "    # This preprocessing step corrects common malformations. We will perform these fixes here because they are common patterns in the LLM's output for this specific task.\n",
    "    cleaned_summary_str = generated_summary_str\n",
    "    cleaned_summary_str = cleaned_summary_str.replace('},\\n  \"Use-cases\":', '],\\n  \"Use-cases\":')\n",
    "    cleaned_summary_str = cleaned_summary_str.replace('},\\n  \"Caveats\":', '],\\n  \"Caveats\":')\n",
    "    cleaned_summary_str = cleaned_summary_str.replace('},\\n  \"Overview\":', '],\\n  \"Overview\":')\n",
    "\n",
    "    # Parse the cleaned_summary_str JSON string into a dictionary\n",
    "    generated_summary_dict = json.loads(cleaned_summary_str)\n",
    "\n",
    "    verification_input = {\n",
    "        \"ground_truth_profile\": {\n",
    "            \"num_rows\": ground_truth_profile.get(\"num_rows\"),\n",
    "            \"num_columns\": ground_truth_profile.get(\"num_columns\"),\n",
    "            \"type_counts\": ground_truth_profile.get(\"type_counts\"),\n",
    "            \"duplicate_row_fraction\": ground_truth_profile.get(\"duplicate_row_fraction\"),\n",
    "            \"time_coverage\": ground_truth_profile.get(\"time_coverage\"),\n",
    "            \"geospatial_hints\": ground_truth_profile.get(\"geospatial_hints\"),\n",
    "            \"primary_key_candidates\": ground_truth_profile.get(\"primary_key_candidates\"),\n",
    "            \"column_details\": {\n",
    "                col_name: {\n",
    "                    \"semantic_type\": col_info.get(\"semantic_type\"),\n",
    "                    \"n_unique\": col_info.get(\"n_unique\"),\n",
    "                    \"null_fraction\": col_info.get(\"null_fraction\"),\n",
    "                    \"min\": col_info.get(\"min\"),\n",
    "                    \"max\": col_info.get(\"max\"),\n",
    "                    \"example_values\": col_info.get(\"example_values\", [])\n",
    "                } for col_name, col_info in ground_truth_profile.get(\"columns\", {}).items()\n",
    "            }\n",
    "        },\n",
    "        \"generated_summary\": generated_summary_dict\n",
    "    }\n",
    "\n",
    "    verification_json_input = json.dumps(verification_input, indent=2, default=str)\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "        You are an AI assistant designed to verify the factual consistency of a dataset description against ground truth data.\n",
    "\n",
    "        Here is a JSON object containing `ground_truth_profile` (from a data profiler) and a `generated_summary` (previously created by another LLM).\n",
    "\n",
    "        {verification_json_input}\n",
    "\n",
    "        Your task is to:\n",
    "        1. **Key Facts Verification**: Evaluate the accuracy and consistency of the \"Key facts\" section in `generated_summary` against the `ground_truth_profile` (e.g., num_rows, num_columns, geospatial hints, primary key candidates).\n",
    "        2. **Caveats Verification**: Assess if the \"Caveats\" section accurately reflects potential data limitations or issues present in the `ground_truth_profile` (e.g., null fractions, time coverage, geospatial info).\n",
    "        3. **Overall Assessment**: Provide a concluding statement on the general accuracy and consistency of the `generated_summary` with respect to the `ground_truth_profile`.\n",
    "\n",
    "        OUTPUT REQUIREMENTS:\n",
    "        Output your verification report in JSON format with the following keys:\n",
    "        - \"Result\": A string indicating the overall verification status, either \"Passed\" or \"Failed\". This should be determined by whether the summary is generally accurate and consistent, with no factual contradictions.\n",
    "        - \"Key Facts Consistency\": A string comment on the accuracy of the \"Key facts\" section.\n",
    "        - \"Caveats Accuracy\": A string comment on the accuracy of the \"Caveats\" section.\n",
    "        - \"Overall Verification Status\": A concluding statement on the summary's accuracy.\n",
    "        \"\"\"\n",
    "\n",
    "    response = client.responses.create(\n",
    "        model=model,\n",
    "        input=user_prompt,\n",
    "    )\n",
    "\n",
    "    return response.output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Run Over S3 Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EzeO95rgzLiV",
    "outputId": "18c5cfa8-7e2f-4e5d-a669-b24e5f0fdc38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[NO DESC] Done. Processed: 0, Failed: 0\n",
      "\n",
      "[HAS DESC] Done. Processed: 0, Failed: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'processed': 0, 'failed': 0}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Batch: run dataset description pipeline over S3 ===\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "import os\n",
    "import io\n",
    "import gc\n",
    "import json\n",
    "import inspect\n",
    "from functools import lru_cache\n",
    "from typing import List, Optional, Dict, Any, Iterable, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Config\n",
    "# -----------------------\n",
    "BUCKET_NAME = \"nyu-tandon-cs-gy-6513-project\"\n",
    "\n",
    "AUCTUS_NO_DESC_PREFIX = \"auctus/dataset_no_description/\"\n",
    "AUCTUS_HAS_DESC_PREFIX = \"auctus/dataset_has_description/\"\n",
    "\n",
    "NYC_OPEN_DATA_PREFIX = \"nyc_open_data/\"\n",
    "NYC_DATA_CATALOG_KEY = \"nyc_open_data/dataset_catalog.csv\"\n",
    "\n",
    "OUTPUT_PREFIX = \"dataset_descriptions/\"\n",
    "FAIL_PREFIX = \"dataset_descriptions_failures/\"\n",
    "\n",
    "# ---- RAM safety knobs ----\n",
    "SAMPLE_ROWS = 3000\n",
    "MAX_COLS = 200\n",
    "SKIP_IF_FILE_MB = 300\n",
    "PRINT_EVERY = 25\n",
    "\n",
    "# ---- Retry + validation knobs ----\n",
    "MAX_RETRIES = 2  # total attempts = 1 + MAX_RETRIES\n",
    "USE_LLM_JUDGE = True  \n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Utilities\n",
    "# -----------------------\n",
    "def utc_now_iso() -> str:\n",
    "    return datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "\n",
    "def _list_keys(bucket: str, prefix: str) -> List[str]:\n",
    "    keys = []\n",
    "    token = None\n",
    "    while True:\n",
    "        kwargs = {\"Bucket\": bucket, \"Prefix\": prefix}\n",
    "        if token:\n",
    "            kwargs[\"ContinuationToken\"] = token\n",
    "        resp = s3.list_objects_v2(**kwargs)\n",
    "        for obj in resp.get(\"Contents\", []):\n",
    "            keys.append(obj[\"Key\"])\n",
    "        if resp.get(\"IsTruncated\"):\n",
    "            token = resp.get(\"NextContinuationToken\")\n",
    "        else:\n",
    "            break\n",
    "    return keys\n",
    "\n",
    "\n",
    "def iter_dataset_keys(\n",
    "    bucket: str,\n",
    "    prefixes: List[str],\n",
    "    suffix: str = \".csv\",\n",
    ") -> Iterable[str]:\n",
    "    for prefix in prefixes:\n",
    "        token = None\n",
    "        while True:\n",
    "            kwargs = {\"Bucket\": bucket, \"Prefix\": prefix}\n",
    "            if token:\n",
    "                kwargs[\"ContinuationToken\"] = token\n",
    "            resp = s3.list_objects_v2(**kwargs)\n",
    "\n",
    "            for obj in resp.get(\"Contents\", []):\n",
    "                key = obj[\"Key\"]\n",
    "                if not key.lower().endswith(suffix):\n",
    "                    continue\n",
    "                if key == NYC_DATA_CATALOG_KEY:\n",
    "                    continue\n",
    "                yield key\n",
    "\n",
    "            if resp.get(\"IsTruncated\"):\n",
    "                token = resp.get(\"NextContinuationToken\")\n",
    "            else:\n",
    "                break\n",
    "\n",
    "\n",
    "def _safe_output_name(dataset_key: str) -> str:\n",
    "    no_ext = os.path.splitext(dataset_key)[0]\n",
    "    return no_ext.replace(\"/\", \"__\")\n",
    "\n",
    "\n",
    "def _replace_ext(key: str, new_ext: str) -> str:\n",
    "    base, _ = os.path.splitext(key)\n",
    "    return base + new_ext\n",
    "\n",
    "\n",
    "def _head_size_mb(bucket: str, key: str) -> float:\n",
    "    try:\n",
    "        head = s3.head_object(Bucket=bucket, Key=key)\n",
    "        return head.get(\"ContentLength\", 0) / (1024 * 1024)\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def _read_csv_sample_from_s3(\n",
    "    bucket: str,\n",
    "    key: str,\n",
    "    sample_rows: int = SAMPLE_ROWS,\n",
    "    max_cols: int = MAX_COLS,\n",
    ") -> pd.DataFrame:\n",
    "    obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "    header_df = pd.read_csv(obj[\"Body\"], nrows=0)\n",
    "    cols = list(header_df.columns)[:max_cols]\n",
    "\n",
    "    obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "    df = pd.read_csv(\n",
    "        obj[\"Body\"],\n",
    "        nrows=sample_rows,\n",
    "        usecols=cols,\n",
    "        low_memory=True\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def _cleanup(*vars_):\n",
    "    for v in vars_:\n",
    "        try:\n",
    "            del v\n",
    "        except Exception:\n",
    "            pass\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "def _extract_overview_from_any_json(d: Dict[str, Any]) -> Optional[str]:\n",
    "    for k in [\"Overview\", \"overview\", \"summary\", \"Summary\", \"description\", \"Description\"]:\n",
    "        v = d.get(k)\n",
    "        if isinstance(v, str) and v.strip():\n",
    "            return v.strip()\n",
    "\n",
    "    if isinstance(d.get(\"generated\"), dict):\n",
    "        for k in [\"Overview\", \"overview\", \"summary\", \"description\"]:\n",
    "            v = d[\"generated\"].get(k)\n",
    "            if isinstance(v, str) and v.strip():\n",
    "                return v.strip()\n",
    "    return None\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# NYC catalog loader (3-col schema)\n",
    "# -----------------------\n",
    "@lru_cache(maxsize=1)\n",
    "def _load_nyc_catalog_map(bucket: str = BUCKET_NAME) -> Dict[str, str]:\n",
    "    try:\n",
    "        obj = s3.get_object(Bucket=bucket, Key=NYC_DATA_CATALOG_KEY)\n",
    "        body = obj[\"Body\"].read()\n",
    "        df = pd.read_csv(io.BytesIO(body), header=None, low_memory=True)\n",
    "\n",
    "        if df.shape[1] < 3:\n",
    "            print(f\"Warning: NYC catalog has {df.shape[1]} columns; expected >= 3.\")\n",
    "            return {}\n",
    "\n",
    "        mp = {}\n",
    "        for _, row in df.iterrows():\n",
    "            name = str(row[1]).strip()\n",
    "            summary = str(row[2]).strip()\n",
    "\n",
    "            if not name or not summary:\n",
    "                continue\n",
    "            if not name.lower().endswith(\".csv\"):\n",
    "                name = name + \".csv\"\n",
    "\n",
    "            mp[name.lower()] = summary\n",
    "\n",
    "        return mp\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load NYC Open Data catalog ({NYC_DATA_CATALOG_KEY}): {e}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "def load_existing_overview_text(\n",
    "    dataset_key: str,\n",
    "    bucket: str = BUCKET_NAME,\n",
    ") -> Optional[str]:\n",
    "\n",
    "    if dataset_key.startswith(AUCTUS_HAS_DESC_PREFIX):\n",
    "        json_key = _replace_ext(dataset_key, \".json\")\n",
    "        try:\n",
    "            obj = s3.get_object(Bucket=bucket, Key=json_key)\n",
    "            raw = obj[\"Body\"].read().decode(\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "            try:\n",
    "                existing_json = parse_llm_summary(raw)\n",
    "                ov = extract_overview_from_summary(existing_json)\n",
    "                if ov:\n",
    "                    return ov\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            existing_dict = json.loads(raw)\n",
    "            return _extract_overview_from_any_json(existing_dict)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: could not load existing Auctus description for {dataset_key}: {e}\")\n",
    "            return None\n",
    "\n",
    "    if dataset_key.startswith(NYC_OPEN_DATA_PREFIX):\n",
    "        catalog_map = _load_nyc_catalog_map(bucket)\n",
    "        fname = os.path.basename(dataset_key).lower()\n",
    "        return catalog_map.get(fname)\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Rule-based validation\n",
    "# -----------------------\n",
    "REQUIRED_FIELDS = [\"Columns\", \"Key facts\", \"Use-cases\", \"Caveats\", \"Overview\"]\n",
    "\n",
    "def _normalize_type(t: str) -> str:\n",
    "    t = str(t or \"\").lower().strip()\n",
    "    # collapse common synonyms\n",
    "    if t in {\"int\", \"integer\", \"long\"}:\n",
    "        return \"integer\"\n",
    "    if t in {\"float\", \"double\", \"decimal\", \"numeric\", \"number\"}:\n",
    "        return \"numeric\"\n",
    "    if \"date\" in t or \"time\" in t:\n",
    "        return \"datetime\"\n",
    "    if t in {\"bool\", \"boolean\"}:\n",
    "        return \"boolean\"\n",
    "    if t in {\"cat\", \"category\", \"categorical\"}:\n",
    "        return \"categorical\"\n",
    "    if t in {\"text\", \"string\", \"str\"}:\n",
    "        return \"text\"\n",
    "    return t\n",
    "\n",
    "def _profile_columns_map(profile: Dict[str, Any]) -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Try to locate the column-level map in your profiler output.\n",
    "    \"\"\"\n",
    "    if not isinstance(profile, dict):\n",
    "        return {}\n",
    "    cols = profile.get(\"columns\")\n",
    "    if isinstance(cols, dict):\n",
    "        return cols\n",
    "    # fallback: sometimes nested\n",
    "    for k in [\"column_profiles\", \"schema\", \"fields\"]:\n",
    "        v = profile.get(k)\n",
    "        if isinstance(v, dict):\n",
    "            return v\n",
    "    return {}\n",
    "\n",
    "def _profile_type_for(col_profile: Dict[str, Any]) -> str:\n",
    "    for k in [\"semantic_type\", \"type\", \"inferred_type\", \"dtype\"]:\n",
    "        v = col_profile.get(k)\n",
    "        if isinstance(v, str) and v.strip():\n",
    "            return _normalize_type(v)\n",
    "    return \"\"\n",
    "\n",
    "def _extract_generated_columns(gen_json: Dict[str, Any]) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Returns list of (name, type) from the LLM JSON.\n",
    "    Supports:\n",
    "      - list of dicts: [{\"name\":..., \"type\":...}, ...]\n",
    "      - list of strings: [\"colA (numeric)\", ...]  (best-effort)\n",
    "      - dict mapping: {\"colA\":\"numeric\", ...}\n",
    "    \"\"\"\n",
    "    cols = gen_json.get(\"Columns\")\n",
    "    out = []\n",
    "\n",
    "    if isinstance(cols, dict):\n",
    "        for name, t in cols.items():\n",
    "            out.append((str(name).strip(), _normalize_type(t)))\n",
    "        return out\n",
    "\n",
    "    if isinstance(cols, list):\n",
    "        for item in cols:\n",
    "            if isinstance(item, dict):\n",
    "                name = str(item.get(\"name\") or item.get(\"column\") or \"\").strip()\n",
    "                t = item.get(\"type\") or item.get(\"semantic_type\") or \"\"\n",
    "                if name:\n",
    "                    out.append((name, _normalize_type(t)))\n",
    "            elif isinstance(item, str):\n",
    "                # best-effort parse \"name (type)\"\n",
    "                s = item.strip()\n",
    "                if \"(\" in s and s.endswith(\")\"):\n",
    "                    name = s.split(\"(\")[0].strip()\n",
    "                    t = s.split(\"(\")[-1].rstrip(\")\").strip()\n",
    "                    out.append((name, _normalize_type(t)))\n",
    "                else:\n",
    "                    out.append((s, \"\"))\n",
    "    return out\n",
    "\n",
    "def rule_based_validate_summary(\n",
    "    summary_text: str,\n",
    "    profile: Dict[str, Any]\n",
    ") -> Tuple[bool, List[str], Optional[Dict[str, Any]]]:\n",
    "    issues = []\n",
    "\n",
    "    # 1) Parse JSON\n",
    "    try:\n",
    "        gen_json = parse_llm_summary(summary_text) if not isinstance(summary_text, dict) else summary_text\n",
    "    except Exception:\n",
    "        return False, [\"Output is not valid JSON or does not match expected JSON format.\"], None\n",
    "\n",
    "    if not isinstance(gen_json, dict):\n",
    "        return False, [\"Parsed output is not a JSON object.\"], None\n",
    "\n",
    "    # 2) Required fields\n",
    "    for f in REQUIRED_FIELDS:\n",
    "        if f not in gen_json:\n",
    "            issues.append(f\"Missing required field: {f}\")\n",
    "\n",
    "    # 3) Overview length sanity (soft)\n",
    "    ov = gen_json.get(\"Overview\")\n",
    "    if isinstance(ov, str):\n",
    "        wc = len(ov.split())\n",
    "        if wc < 25 or wc > 120:\n",
    "            issues.append(\"Overview length seems outside the intended concise range.\")\n",
    "\n",
    "    # 4) Column name and type consistency\n",
    "    prof_cols = _profile_columns_map(profile)\n",
    "    prof_col_names = {c.strip() for c in prof_cols.keys()}\n",
    "\n",
    "    gen_cols = _extract_generated_columns(gen_json)\n",
    "    if gen_cols:\n",
    "        # name existence checks\n",
    "        for name, _t in gen_cols:\n",
    "            if name and prof_col_names and name not in prof_col_names:\n",
    "                issues.append(f\"Generated column not found in schema: {name}\")\n",
    "\n",
    "        # type checks (best-effort, tolerant)\n",
    "        for name, t in gen_cols:\n",
    "            if not name or not t:\n",
    "                continue\n",
    "            col_prof = prof_cols.get(name)\n",
    "            if isinstance(col_prof, dict):\n",
    "                ptype = _profile_type_for(col_prof)\n",
    "                if ptype and t:\n",
    "                    # allow integer vs numeric compatibility\n",
    "                    if not (\n",
    "                        (ptype == t) or\n",
    "                        (ptype in {\"integer\", \"numeric\"} and t in {\"integer\", \"numeric\"})\n",
    "                    ):\n",
    "                        issues.append(f\"Type mismatch for {name}: generated={t}, profiler={ptype}\")\n",
    "\n",
    "    ok = len(issues) == 0\n",
    "    return ok, issues, gen_json\n",
    "\n",
    "\n",
    "def llm_judge_pass(\n",
    "    profile: Dict[str, Any],\n",
    "    signals: Dict[str, Any],\n",
    "    generated_json: Dict[str, Any]\n",
    ") -> Tuple[bool, List[str]]:\n",
    "    \"\"\"\n",
    "    Call the verify_summary_with_llm() judge function to evaluate the candidate dataset card.\n",
    "\n",
    "    Expected verify_summary_with_llm return format:\n",
    "        {\n",
    "          \"pass\": bool,\n",
    "          \"issues\": [\"issue 1\", \"issue 2\", ...]\n",
    "        }\n",
    "    \"\"\"\n",
    "    if not USE_LLM_JUDGE:\n",
    "        return True, []\n",
    "\n",
    "    try:\n",
    "        # verify_summary_with_llm should be defined in the same file\n",
    "        out = verify_summary_with_llm(profile, generated_json)\n",
    "\n",
    "        if isinstance(out, dict):\n",
    "            passed = bool(out.get(\"pass\", True))\n",
    "            issues = out.get(\"issues\") or []\n",
    "            # Ensure issues are strings\n",
    "            issues = [str(i) for i in issues]\n",
    "            return passed, issues\n",
    "\n",
    "        # If it returns something unexpected, don't block the pipeline\n",
    "        return True, []\n",
    "\n",
    "    except Exception as e:\n",
    "        # Fail-open: do not break the main pipeline if judge fails\n",
    "        print(f\"LLM judge (verify_summary_with_llm) failed: {e}\")\n",
    "        return True, []\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Safe call wrapper for summarize_dataset_with_llm\n",
    "# -----------------------\n",
    "def _call_summarizer(profile, signals, feedback: Optional[str] = None):\n",
    "    fn = summarize_dataset_with_llm\n",
    "    if feedback:\n",
    "        try:\n",
    "            sig = inspect.signature(fn)\n",
    "            for param in [\"feedback\", \"repair_instructions\", \"additional_instructions\", \"extra_instructions\"]:\n",
    "                if param in sig.parameters:\n",
    "                    return fn(profile, signals, **{param: feedback})\n",
    "        except Exception:\n",
    "            pass\n",
    "    return fn(profile, signals)\n",
    "\n",
    "\n",
    "def generate_with_retry(\n",
    "    profile: Dict[str, Any],\n",
    "    signals: Dict[str, Any],\n",
    "    max_retries: int = MAX_RETRIES\n",
    ") -> Tuple[Optional[str], Optional[Dict[str, Any]], List[Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      summary_text, parsed_json, attempts_log\n",
    "    \"\"\"\n",
    "    attempts = []\n",
    "    feedback = None\n",
    "\n",
    "    for attempt_idx in range(max_retries + 1):\n",
    "        summary_text = _call_summarizer(profile, signals, feedback=feedback)\n",
    "\n",
    "        rb_ok, rb_issues, parsed = rule_based_validate_summary(summary_text, profile)\n",
    "        judge_ok, judge_issues = llm_judge_pass(profile, signals, parsed or {})\n",
    "\n",
    "        ok = rb_ok and judge_ok\n",
    "\n",
    "        attempts.append({\n",
    "            \"attempt\": attempt_idx + 1,\n",
    "            \"rule_based_ok\": rb_ok,\n",
    "            \"rule_based_issues\": rb_issues,\n",
    "            \"llm_judge_ok\": judge_ok,\n",
    "            \"llm_judge_issues\": judge_issues,\n",
    "        })\n",
    "\n",
    "        if ok:\n",
    "            return summary_text, parsed, attempts\n",
    "\n",
    "        # Build corrective feedback for next try\n",
    "        all_issues = rb_issues + judge_issues\n",
    "        feedback = (\n",
    "            \"Fix the following issues and regenerate the SAME JSON structure:\\n\"\n",
    "            + \"\\n\".join([f\"- {i}\" for i in all_issues])\n",
    "            + \"\\nDo not add columns not in the schema. Ensure required fields exist.\"\n",
    "        )\n",
    "\n",
    "    return None, None, attempts\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Core runner\n",
    "# -----------------------\n",
    "def _run_single_dataset(\n",
    "    key: str,\n",
    "    bucket_name: str,\n",
    "    run_type: str,\n",
    "    output_prefix: str,\n",
    "    compute_eval: bool\n",
    ") -> bool:\n",
    "\n",
    "    size_mb = _head_size_mb(bucket_name, key)\n",
    "    if size_mb and size_mb > SKIP_IF_FILE_MB:\n",
    "        print(f\"[SKIP huge file] {key} ({size_mb:.1f} MB)\")\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        df = _read_csv_sample_from_s3(bucket_name, key)\n",
    "\n",
    "        if df.shape[1] > MAX_COLS:\n",
    "            df = df.iloc[:, :MAX_COLS]\n",
    "\n",
    "        existing_overview = None\n",
    "        if run_type == \"has_existing_description\":\n",
    "            existing_overview = load_existing_overview_text(key, bucket=bucket_name)\n",
    "\n",
    "        profile = profile_dataframe(df)\n",
    "        signals = build_data_signals(profile)\n",
    "\n",
    "        # --- NEW: generation with retry + validation ---\n",
    "        summary_text, summary_json, attempts_log = generate_with_retry(profile, signals)\n",
    "\n",
    "        if not summary_text or not summary_json:\n",
    "            # Save failure artifact and skip normal persistence\n",
    "            safe_name = _safe_output_name(key)\n",
    "            fail_key = f\"{FAIL_PREFIX}{safe_name}__failed.json\"\n",
    "\n",
    "            fail_obj = {\n",
    "                \"run_type\": run_type,\n",
    "                \"dataset_key\": key,\n",
    "                \"existing_overview\": existing_overview,\n",
    "                \"profile\": profile,\n",
    "                \"signals\": signals,\n",
    "                \"summary\": None,\n",
    "                \"validation_attempts\": attempts_log,\n",
    "                \"processed_at_utc\": utc_now_iso(),\n",
    "                \"sample_rows_used\": SAMPLE_ROWS,\n",
    "                \"max_cols_used\": MAX_COLS,\n",
    "                \"source_file_size_mb\": size_mb,\n",
    "                \"status\": \"failed_after_retries\",\n",
    "            }\n",
    "\n",
    "            s3.put_object(\n",
    "                Bucket=bucket_name,\n",
    "                Key=fail_key,\n",
    "                Body=json.dumps(fail_obj, indent=2, default=str).encode(\"utf-8\"),\n",
    "                ContentType=\"application/json\",\n",
    "            )\n",
    "\n",
    "            print(f\"[FAILED] Saved failure artifact to s3://{bucket_name}/{fail_key}\")\n",
    "\n",
    "            _cleanup(df, profile, signals, fail_obj)\n",
    "            return False\n",
    "\n",
    "        # Normal success result\n",
    "        result = {\n",
    "            \"run_type\": run_type,\n",
    "            \"dataset_key\": key,\n",
    "            \"existing_overview\": existing_overview,\n",
    "            \"profile\": profile,\n",
    "            \"signals\": signals,\n",
    "            \"summary\": summary_text,\n",
    "            \"parsed_summary\": summary_json,  # helpful for debugging/reuse\n",
    "            \"validation_attempts\": attempts_log,\n",
    "            \"eval_vs_existing\": None,  # still computed in a post-run cell\n",
    "            \"processed_at_utc\": utc_now_iso(),\n",
    "            \"sample_rows_used\": SAMPLE_ROWS,\n",
    "            \"max_cols_used\": MAX_COLS,\n",
    "            \"source_file_size_mb\": size_mb,\n",
    "            \"status\": \"success\",\n",
    "        }\n",
    "\n",
    "        safe_name = _safe_output_name(key)\n",
    "        output_key = f\"{output_prefix}{safe_name}__description.json\"\n",
    "\n",
    "        s3.put_object(\n",
    "            Bucket=bucket_name,\n",
    "            Key=output_key,\n",
    "            Body=json.dumps(result, indent=2, default=str).encode(\"utf-8\"),\n",
    "            ContentType=\"application/json\",\n",
    "        )\n",
    "\n",
    "        print(f\"Saved summary to s3://{bucket_name}/{output_key}\")\n",
    "\n",
    "        _cleanup(df, profile, signals, result)\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR processing {key}: {e}\")\n",
    "        _cleanup()\n",
    "        return False\n",
    "\n",
    "\n",
    "def run_pipeline_no_existing_descriptions(\n",
    "    bucket_name: str = BUCKET_NAME,\n",
    "    prefixes: Optional[List[str]] = None,\n",
    "    output_prefix: str = OUTPUT_PREFIX,\n",
    "    max_datasets: Optional[int] = None,\n",
    ") -> Dict[str, int]:\n",
    "\n",
    "    if prefixes is None:\n",
    "        prefixes = [AUCTUS_NO_DESC_PREFIX]\n",
    "\n",
    "    processed = 0\n",
    "    failed = 0\n",
    "    seen = 0\n",
    "\n",
    "    for key in iter_dataset_keys(bucket_name, prefixes=prefixes, suffix=\".csv\"):\n",
    "        if max_datasets is not None and processed >= max_datasets:\n",
    "            break\n",
    "\n",
    "        seen += 1\n",
    "        print(f\"\\n=== [NO DESC] Processing {key} ===\")\n",
    "\n",
    "        ok = _run_single_dataset(\n",
    "            key=key,\n",
    "            bucket_name=bucket_name,\n",
    "            run_type=\"no_existing_description\",\n",
    "            output_prefix=output_prefix,\n",
    "            compute_eval=False\n",
    "        )\n",
    "\n",
    "        if ok:\n",
    "            processed += 1\n",
    "        else:\n",
    "            failed += 1\n",
    "\n",
    "        if seen % PRINT_EVERY == 0:\n",
    "            print(f\"[NO DESC] Progress: processed={processed}, failed={failed}, seen={seen}\")\n",
    "\n",
    "    print(f\"\\n[NO DESC] Done. Processed: {processed}, Failed: {failed}\")\n",
    "    return {\"processed\": processed, \"failed\": failed}\n",
    "\n",
    "\n",
    "def run_pipeline_with_existing_descriptions(\n",
    "    bucket_name: str = BUCKET_NAME,\n",
    "    prefixes: Optional[List[str]] = None,\n",
    "    output_prefix: str = OUTPUT_PREFIX,\n",
    "    max_datasets: Optional[int] = None,\n",
    ") -> Dict[str, int]:\n",
    "\n",
    "    if prefixes is None:\n",
    "        prefixes = [AUCTUS_HAS_DESC_PREFIX, NYC_OPEN_DATA_PREFIX]\n",
    "\n",
    "    processed = 0\n",
    "    failed = 0\n",
    "    seen = 0\n",
    "\n",
    "    for key in iter_dataset_keys(bucket_name, prefixes=prefixes, suffix=\".csv\"):\n",
    "        if max_datasets is not None and processed >= max_datasets:\n",
    "            break\n",
    "\n",
    "        seen += 1\n",
    "        print(f\"\\n=== [HAS DESC] Processing {key} ===\")\n",
    "\n",
    "        ok = _run_single_dataset(\n",
    "            key=key,\n",
    "            bucket_name=bucket_name,\n",
    "            run_type=\"has_existing_description\",\n",
    "            output_prefix=output_prefix,\n",
    "            compute_eval=False\n",
    "        )\n",
    "\n",
    "        if ok:\n",
    "            processed += 1\n",
    "        else:\n",
    "            failed += 1\n",
    "\n",
    "        if seen % PRINT_EVERY == 0:\n",
    "            print(f\"[HAS DESC] Progress: processed={processed}, failed={failed}, seen={seen}\")\n",
    "\n",
    "    print(f\"\\n[HAS DESC] Done. Processed: {processed}, Failed: {failed}\")\n",
    "    return {\"processed\": processed, \"failed\": failed}\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# üîπ Batch runs\n",
    "# -----------------------\n",
    "# Set max_datasets=None to run all.\n",
    "# Using 0 means \"run none\" (kept for safety).\n",
    "\n",
    "run_pipeline_no_existing_descriptions(max_datasets=0)\n",
    "run_pipeline_with_existing_descriptions(max_datasets=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3zebVc-KM8OL"
   },
   "source": [
    "# Search impact evaluation (offline retrieval)\n",
    "\n",
    "This  section measures whether adding our generated descriptions improves\n",
    "top-*k* retrieval performance on a held-out query set.\n",
    " \n",
    "\n",
    "1. Build two searchable corpora:\n",
    "   - **Baseline**: existing descriptions (when available).\n",
    "   - **Augmented**: our generated descriptions (Overview).\n",
    "\n",
    "2. Embed each corpus and run vector retrieval.\n",
    "\n",
    "3. Compute **precision@k** and **recall@k** against a  query set with\n",
    "   manually curated relevant dataset keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z5BPwGW-ObO4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1339 dataset CSV keys across prefixes.\n",
      "Built schema texts for 1339 datasets.\n",
      "Saved 1339 queries to evaluation/query_set.json\n",
      "Uploaded query set to s3://nyu-tandon-cs-gy-6513-project/evaluation/query_set.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'query': 'recreation maps town',\n",
       "  'relevant_datasets': ['nyc_open_data/NYCDEP Recreation Area Maps.csv'],\n",
       "  'source_dataset': 'nyc_open_data/NYCDEP Recreation Area Maps.csv',\n",
       "  'generation_method': 'schema+filename_tfidf'},\n",
       " {'query': 'site site type sample site',\n",
       "  'relevant_datasets': ['nyc_open_data/Watershed Water Quality - Sites.csv'],\n",
       "  'source_dataset': 'nyc_open_data/Watershed Water Quality - Sites.csv',\n",
       "  'generation_method': 'schema+filename_tfidf'},\n",
       " {'query': 'domestic violence partners violence',\n",
       "  'relevant_datasets': ['nyc_open_data/HRA Domestic Violence Partners.csv'],\n",
       "  'source_dataset': 'nyc_open_data/HRA Domestic Violence Partners.csv',\n",
       "  'generation_method': 'schema+filename_tfidf'},\n",
       " {'query': 'fees expenses administrative',\n",
       "  'relevant_datasets': ['nyc_open_data/New York City Deferred Compensation Plan - Summary of Administrative Revenues and Expenses.csv'],\n",
       "  'source_dataset': 'nyc_open_data/New York City Deferred Compensation Plan - Summary of Administrative Revenues and Expenses.csv',\n",
       "  'generation_method': 'schema+filename_tfidf'},\n",
       " {'query': 'enrolled residents resident',\n",
       "  'relevant_datasets': ['nyc_open_data/Resident Economic Empowerment and Sustainability REES for NYCHA Residents - Borough - Local Law 163.csv'],\n",
       "  'source_dataset': 'nyc_open_data/Resident Economic Empowerment and Sustainability REES for NYCHA Residents - Borough - Local Law 163.csv',\n",
       "  'generation_method': 'schema+filename_tfidf'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Generate an UNBIASED synthetic query_set.json for offline retrieval evaluation ---\n",
    "# This version avoids using baseline or generated descriptions.\n",
    "# Queries are derived ONLY from:\n",
    "#   - dataset filename\n",
    "#   - column headers (schema)\n",
    "#\n",
    "# This makes the query set fair for BEFORE vs AFTER description comparisons.\n",
    "\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Config\n",
    "# -----------------------\n",
    "BUCKET = \"nyu-tandon-cs-gy-6513-project\"\n",
    "\n",
    "AUCTUS_NO_DESC_PREFIX = \"auctus/dataset_no_description/\"\n",
    "AUCTUS_HAS_DESC_PREFIX = \"auctus/dataset_has_description/\"\n",
    "NYC_PREFIX = \"nyc_open_data/\"\n",
    "\n",
    "PREFIXES = [AUCTUS_NO_DESC_PREFIX, AUCTUS_HAS_DESC_PREFIX, NYC_PREFIX]\n",
    "\n",
    "LOCAL_OUT_PATH = Path(\"evaluation/query_set.json\")\n",
    "S3_OUT_KEY = \"evaluation/query_set.json\"\n",
    "\n",
    "# How many datasets to sample for query generation\n",
    "MAX_DATASETS = 1500  \n",
    "\n",
    "# Queries per dataset\n",
    "QUERIES_PER_DATASET = 1\n",
    "\n",
    "# TF-IDF settings over schema text\n",
    "NGRAM_RANGE = (1, 2)\n",
    "MAX_FEATURES = 8000\n",
    "STOP_WORDS = \"english\"\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Safety: ignore the NYC catalog file as a dataset\n",
    "NYC_CATALOG_BASENAMES = {\"dataset_catalog.csv\", \"data_catalog.csv\", \"dataset catalog.csv\"}\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Helpers\n",
    "# -----------------------\n",
    "def _get_s3_client():\n",
    "    return boto3.client(\"s3\")\n",
    "\n",
    "\n",
    "def _list_s3_keys(s3, bucket: str, prefix: str) -> List[str]:\n",
    "    keys = []\n",
    "    token = None\n",
    "    while True:\n",
    "        kwargs = {\"Bucket\": bucket, \"Prefix\": prefix}\n",
    "        if token:\n",
    "            kwargs[\"ContinuationToken\"] = token\n",
    "        resp = s3.list_objects_v2(**kwargs)\n",
    "        for obj in resp.get(\"Contents\", []):\n",
    "            keys.append(obj[\"Key\"])\n",
    "        if resp.get(\"IsTruncated\"):\n",
    "            token = resp.get(\"NextContinuationToken\")\n",
    "        else:\n",
    "            break\n",
    "    return keys\n",
    "\n",
    "\n",
    "def iter_dataset_csv_keys(s3, bucket: str, prefixes: List[str]) -> List[str]:\n",
    "    out = []\n",
    "    for p in prefixes:\n",
    "        keys = _list_s3_keys(s3, bucket, p)\n",
    "        for k in keys:\n",
    "            if not k.lower().endswith(\".csv\"):\n",
    "                continue\n",
    "            base = os.path.basename(k).lower()\n",
    "            if base in NYC_CATALOG_BASENAMES:\n",
    "                continue\n",
    "            out.append(k)\n",
    "    # dedupe while preserving order\n",
    "    seen = set()\n",
    "    deduped = []\n",
    "    for k in out:\n",
    "        if k not in seen:\n",
    "            seen.add(k)\n",
    "            deduped.append(k)\n",
    "    return deduped\n",
    "\n",
    "\n",
    "def _clean_text(t: str) -> str:\n",
    "    t = re.sub(r\"[_\\-]+\", \" \", str(t))\n",
    "    t = re.sub(r\"\\s+\", \" \", t)\n",
    "    return t.strip()\n",
    "\n",
    "\n",
    "def _read_csv_header_columns(s3, bucket: str, key: str) -> List[str]:\n",
    "    obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "    df0 = pd.read_csv(obj[\"Body\"], nrows=0)\n",
    "    cols = [str(c) for c in df0.columns]\n",
    "    return cols\n",
    "\n",
    "\n",
    "def build_schema_text(dataset_key: str, columns: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    A neutral text representation of a dataset intended for query synthesis.\n",
    "    \"\"\"\n",
    "    fname = os.path.splitext(os.path.basename(dataset_key))[0]\n",
    "    parts = [fname] + columns\n",
    "    return _clean_text(\" \".join(parts))\n",
    "\n",
    "\n",
    "def _keywords_tfidf(texts: List[str], top_k: int = 6) -> List[List[str]]:\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        stop_words=STOP_WORDS,\n",
    "        ngram_range=NGRAM_RANGE,\n",
    "        max_features=MAX_FEATURES\n",
    "    )\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    vocab = vectorizer.get_feature_names_out()\n",
    "\n",
    "    keywords_per_doc = []\n",
    "    for i in range(X.shape[0]):\n",
    "        row = X.getrow(i)\n",
    "        if row.nnz == 0:\n",
    "            keywords_per_doc.append([])\n",
    "            continue\n",
    "\n",
    "        sorted_idx = row.data.argsort()[::-1]\n",
    "        top_indices = row.indices[sorted_idx][:top_k]\n",
    "        kws = [vocab[j] for j in top_indices]\n",
    "        keywords_per_doc.append(kws)\n",
    "\n",
    "    return keywords_per_doc\n",
    "\n",
    "\n",
    "def _build_query_from_keywords(kws: List[str], variant: int = 0) -> Optional[str]:\n",
    "    if not kws:\n",
    "        return None\n",
    "    if variant == 0:\n",
    "        return \" \".join(kws[:3])\n",
    "    if variant == 1:\n",
    "        return \" \".join(kws[:2])\n",
    "    return kws[0]\n",
    "\n",
    "\n",
    "def generate_unbiased_query_set(\n",
    "    dataset_keys: List[str],\n",
    "    max_datasets: int = MAX_DATASETS,\n",
    "    queries_per_dataset: int = QUERIES_PER_DATASET\n",
    ") -> List[Dict]:\n",
    "\n",
    "    import random\n",
    "    random.seed(RANDOM_SEED)\n",
    "\n",
    "    # shuffle and cap\n",
    "    keys = dataset_keys[:]\n",
    "    random.shuffle(keys)\n",
    "    keys = keys[:max_datasets]\n",
    "\n",
    "    schema_texts = []\n",
    "    kept_keys = []\n",
    "\n",
    "    for k in keys:\n",
    "        try:\n",
    "            cols = _read_csv_header_columns(s3, BUCKET, k)\n",
    "            txt = build_schema_text(k, cols)\n",
    "            if txt:\n",
    "                schema_texts.append(txt)\n",
    "                kept_keys.append(k)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    print(f\"Built schema texts for {len(kept_keys)} datasets.\")\n",
    "\n",
    "    kws_list = _keywords_tfidf(schema_texts, top_k=8)\n",
    "\n",
    "    query_set = []\n",
    "    for dataset_key, kws, schema_txt in zip(kept_keys, kws_list, schema_texts):\n",
    "        for q_idx in range(queries_per_dataset):\n",
    "            q = _build_query_from_keywords(kws, variant=q_idx)\n",
    "            if not q:\n",
    "                # fallback: short chunk of schema text\n",
    "                q = \" \".join(schema_txt.split()[:5])\n",
    "\n",
    "            q = _clean_text(q)\n",
    "\n",
    "            query_set.append({\n",
    "                \"query\": q,\n",
    "                \"relevant_datasets\": [dataset_key],\n",
    "                \"source_dataset\": dataset_key,\n",
    "                \"generation_method\": \"schema+filename_tfidf\"\n",
    "            })\n",
    "\n",
    "    return query_set\n",
    "\n",
    "\n",
    "def save_query_set_local(query_set: List[Dict], path: Path):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(query_set, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"Saved {len(query_set)} queries to {path}\")\n",
    "\n",
    "\n",
    "def upload_query_set_to_s3(s3, bucket: str, key: str, path: Path):\n",
    "    s3.upload_file(str(path), bucket, key)\n",
    "    print(f\"Uploaded query set to s3://{bucket}/{key}\")\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Execute\n",
    "# -----------------------\n",
    "s3 = _get_s3_client()\n",
    "\n",
    "all_dataset_keys = iter_dataset_csv_keys(s3, BUCKET, PREFIXES)\n",
    "print(f\"Found {len(all_dataset_keys)} dataset CSV keys across prefixes.\")\n",
    "\n",
    "query_set = generate_unbiased_query_set(\n",
    "    all_dataset_keys,\n",
    "    max_datasets=MAX_DATASETS,\n",
    "    queries_per_dataset=QUERIES_PER_DATASET\n",
    ")\n",
    "\n",
    "save_query_set_local(query_set, LOCAL_OUT_PATH)\n",
    "\n",
    "try:\n",
    "    upload_query_set_to_s3(s3, BUCKET, S3_OUT_KEY, LOCAL_OUT_PATH)\n",
    "except Exception as e:\n",
    "    print(\"S3 upload skipped/failed:\", repr(e))\n",
    "\n",
    "query_set[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "H0meAndlNJ2t"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result JSON count: 1302\n",
      "Augmented corpus size (robust): 1292\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('auctus/dataset_has_description/2007_-_2008_School_Progress_Reports_-_All_Schools.csv',\n",
       "  '{\\n  \"Columns\": {\\n    \"DBN\": \"text\",\\n    \"DISTRICT\": \"float\",\\n    \"SCHOOL\": \"text\",\\n    \"PRINCIPAL\": \"text\",\\n    \"2007-08 SCHOOL SUPPORT ORGANIZATION\": \"categorical\",\\n    \"PROGRESS REPORT TYPE\": \"categorical\",\\n    \"SCHOOL LEVEL*\": \"categorical\",\\n    \"PEER INDEX*\": \"float\",\\n    \"OVERALL GRADE\": \"categorical\",\\n    \"OVERALL SCORE\": \"float\",\\n    \"ENVIRONMENT CATEGORY SCORE\": \"float\",\\n    \"ENVIRONMENT GRADE\": \"categorical\",\\n    \"PERFORMANCE CATEGORY SCORE\": \"float\",\\n    \"PERFORMANCE GRADE\": \"categorical\",\\n    \"PROGRESS CATEGORY SCORE\": \"float\",\\n    \"PROGRESS GRADE\": \"categorical\",\\n    \"ADDITIONAL CREDIT\": \"float\",\\n    \"2006-07 PROGRESS REPORT GRADE\": \"categorical\",\\n    \"2007-08 QUALITY REVIEW SCORE\": \"categorical\",\\n    \"2006-07 FEDERAL ACCOUNTABILITY STATUS\": \"categorical\"\\n  },\\n  \"Key facts\": [\\n    \"Dataset contains many records, each representing a distinct school.\",\\n    \"Main entities include school, principal, district, and school support organization.\",\\n    \"Columns cover text, categorical, and numeric datatypes.\",\\n    \"Grading and performance measures present across multiple academic and review dimensions.\",\\n    \"No geospatial or date columns are included.\"\\n  ],\\n  \"Use-cases\": [\\n    \"Analyze school performance grades across organizations or school levels.\",\\n    \"Benchmark schools based on categorical ratings and numeric scores.\",\\n    \"Support operational decision-making by comparing accountability status.\",\\n    \"Explore relationship between school support organization and progress.\",\\n    \"Summarize categorical grade distributions for reporting needs.\"\\n  ],\\n  \"Caveats\": [\\n    \"No columns for time periods or geographic coordinates.\",\\n    \"Time coverage is unclear beyond academic years in some fields.\",\\n    \"Exact row count and sample values are not specified.\",\\n    \"Schema balances text, numeric, and categorical but lacks event sequencing.\",\\n    \"Possible limitations interpreting changes over time or across regions.\"\\n  ],\\n  \"Overview\": \"This dataset compiles various school-level records, capturing performance, grading, and organizational details for each institution. Key columns such as DBN, SCHOOL, PRINCIPAL, and DISTRICT identify individual schools and their leadership. Academic standing is assessed through scores and categorical grades like OVERALL GRADE, PROGRESS GRADE, and ENVIRONMENT GRADE. Additional columns include performance and progress scores, school support organization types, and federal accountability statuses. The schema is numeric- and text-heavy, allowing for diverse analyses of educational effectiveness, school organization, and accountability. However, no explicit geographic or temporal identifiers are present beyond references to academic years. The dataset is suitable for comparing school outcomes, organizational impact, and categorical grade distributions, but limitations in time and location coverage should be considered.\"\\n}'),\n",
       " ('auctus/dataset_has_description/Atomic_Polygons.csv',\n",
       "  '{\\n  \"Columns\": {\\n    \"BOROUGH\": \"integer\",\\n    \"CENSUSBLOCK_2000\": \"integer\",\\n    \"CENSUSBLOCK_2000_SUFFIX\": \"categorical\",\\n    \"CENSUSTRACT_2000\": \"integer\",\\n    \"CENSUSBLOCK_2010\": \"integer\",\\n    \"CENSUSBLOCK_2010_SUFFIX\": \"float\",\\n    \"CENSUSTRACT_2010\": \"integer\",\\n    \"CENSUSTRACT_1990\": \"integer\",\\n    \"ADMIN_FIRE\": \"categorical\",\\n    \"WATER_FLAG\": \"integer\",\\n    \"ASSEMDIST\": \"integer\",\\n    \"ELECTDIST\": \"integer\",\\n    \"SCHOOLDIST\": \"integer\",\\n    \"COMMDIST\": \"integer\",\\n    \"SB1_VOLUME\": \"float\",\\n    \"SB1_PAGE\": \"float\",\\n    \"SB2_VOLUME\": \"float\",\\n    \"SB2_PAGE\": \"float\",\\n    \"SB3_VOLUME\": \"float\",\\n    \"SB3_PAGE\": \"float\",\\n    \"ATOMICID\": \"integer\",\\n    \"ATOMIC_NUM\": \"float\",\\n    \"HURRICANE_EVACUATION_ZONE\": \"categorical\",\\n    \"CENSUSTRACT_2020\": \"integer\",\\n    \"CENSUSBLOCK_2020\": \"integer\",\\n    \"CENSUSBLOCK_2020_SUFFIX\": \"float\",\\n    \"COMMERCIAL_WASTE_ZONE\": \"categorical\",\\n    \"Shape_Area\": \"float\",\\n    \"Shape_Length\": \"float\",\\n    \"the_geom\": \"text\"\\n  },\\n  \"Key facts\": [\\n    \"Each row represents a geographic segment with associated demographic and administrative data.\",\\n    \"Includes columns for census tracts and blocks across multiple years for longitudinal analysis.\",\\n    \"Key columns identify borough, election, school, and community districts.\",\\n    \"Contains text-based geographic shapes, and categorical attributes such as fire administration zones.\",\\n    \"Numeric columns dominate, supporting quantitative and statistical analyses by segment.\"\\n  ],\\n  \"Use-cases\": [\\n    \"Analyze population patterns and diversity across boroughs and census tracts.\",\\n    \"Assess service provision by school, election, or administrative district.\",\\n    \"Support fair resource allocation using demographic and spatial boundaries.\",\\n    \"Evaluate disaster preparedness with evacuation and water zone indicators.\",\\n    \"Monitor district changes over census periods to study demographic shifts.\"\\n  ],\\n  \"Caveats\": [\\n    \"No explicit latitude or longitude; geographic fields are text-based shapes.\",\\n    \"Missingness is not reported, but no column is fully missing.\",\\n    \"Time coverage is indirect; census years inferred but not specified directly.\",\\n    \"Potential bias if segments are uneven or do not represent all populations equally.\",\\n    \"Schema lacks detailed population or socioeconomic values.\"\\n  ],\\n  \"Overview\": \"This dataset presents a detailed, segment-level view of geographic, administrative, and categorical attributes relevant to demographic analysis. Each record corresponds to a specific geographic segment, such as a census block or tract, and is annotated with columns like BOROUGH, CENSUSTRACT_2000, CENSUSTRACT_2010, HURRICANE_EVACUATION_ZONE, and ADMIN_FIRE, among others. The data encompasses multiple years of census boundaries, providing granularity for examining population distributions, administrative districts, and service zones. Geospatial information is coded as text (the_geom), while numeric and categorical columns allow for diverse analyses, from tracking demographic change across districts to linking to operational decision-making like disaster planning. There are no explicit population or socioeconomic counts, but the rich segmentation enables analytical, planning, or equity-focused investigations across various district types and time periods.\"\\n}'),\n",
       " ('auctus/dataset_has_description/Ballfields_9K.csv',\n",
       "  '{\\n  \"Columns\": {\\n    \"OBJECTID\": \"integer\",\\n    \"SHAPE_Length\": \"float\",\\n    \"Last Update Date\": \"datetime\",\\n    \"Last Editor\": \"categorical\",\\n    \"GlobalID\": \"text\",\\n    \"CREATED_USER\": \"categorical\",\\n    \"CREATED_DATE\": \"datetime\",\\n    \"LAST_EDITED_USER\": \"categorical\",\\n    \"LAST_EDITED_DATE\": \"datetime\",\\n    \"Name\": \"categorical\",\\n    \"the_geom\": \"text\",\\n    \"Surface\": \"categorical\",\\n    \"SHAPE_Area\": \"float\"\\n  },\\n  \"Key facts\": [\\n    \"Each entry describes a unique entity with surface attributes and time stamps.\",\\n    \"Includes several datetime fields for creation, last update, and editing information.\",\\n    \"Surface, Name, and editor columns provide categorical context to each record.\",\\n    \"Highly structured with no apparent missing values.\",\\n    \"Text and numeric columns offer varied analysis options, but no true geospatial coordinates.\"\\n  ],\\n  \"Use-cases\": [\\n    \"Analyze trends in updates and creation times across entity types.\",\\n    \"Compare surface materials and usage patterns between different named entities.\",\\n    \"Track operational updates by user or over time.\",\\n    \"Support inventory management or facilities planning.\",\\n    \"Assess which editors or users are most active in maintaining records.\"\\n  ],\\n  \"Caveats\": [\\n    \"No latitude or longitude columns; geospatial analysis is limited.\",\\n    \"True time and geographic coverage is unclear from schema.\",\\n    \"All values are non-missing but quality of text fields may vary.\",\\n    \"Limited interpretability if entity context is not known.\",\\n    \"Some fields have few categories, limiting statistical depth.\"\\n  ],\\n  \"Overview\": \"This dataset provides a structured record of entities characterized by categorical labels such as \\'Name\\' and \\'Surface\\', as well as operational metadata such as \\'CREATED_USER\\', \\'Last Update Date\\', and \\'Editor\\' identity fields. Numeric columns like \\'SHAPE_Length\\' and \\'SHAPE_Area\\' suggest measurement attributes. While the presence of \\'the_geom\\' implies spatial information, true latitude and longitude details are absent, making geospatial analysis less direct. Rich in both temporal and text data, each row details one unique entity, tracked through creation, editing, and update timestamps. Its detailed operational and descriptive columns support both analytical and logistical queries about entity maintenance, though higher-level context (such as exact location or broader use-case) must be inferred from categorical field values.\"\\n}'),\n",
       " ('auctus/dataset_has_description/Bike_Ridership_-_Edmonton_Insight_Community.csv',\n",
       "  '{\\n  \"Columns\": {\\n    \"ResponseDate\": \"datetime\",\\n    \"CompletionDate\": \"datetime\",\\n    \"Bike_Ridership\": \"categorical\",\\n    \"Biker_Ridership_Winter\": \"categorical\",\\n    \"Why_Ride_Bike_Recreation or leisure enjoyment\": \"float\",\\n    \"Why_Ride_Bike_To go shopping, dine out, run errands, visit people, go to a movie or similar activities (transportation trips)\": \"float\",\\n    \"Why_Ride_Bike_Commuting to work or school\": \"float\",\\n    \"Why_Ride_Bike_Other reasons\": \"float\",\\n    \"Bike_Agreement_I would like to travel by bike more than I do now\": \"categorical\",\\n    \"Bike_Agreement_I would ride a bike more often if I felt safer riding on the road\": \"categorical\",\\n    \"Bike_Agreement_Many of the places I need to get to regularly are within biking distance of my home\": \"categorical\",\\n    \"Bike_Agreement_There is so much traffic along streets near my home that it would make it difficult or unpleasant to ride a bike.\": \"categorical\",\\n    \"Bike_Agreement_I have been biking more often because of the bike lanes that have been installed.\": \"categorical\",\\n    \"Bike_Agreement_I would like to learn how to ride a bike more safely in traffic\": \"categorical\",\\n    \"Bike_Agreement_I would like to learn how to drive a vehicle more safely with cyclists in traffic\": \"categorical\",\\n    \"Bike_Agreement_I don‚Äôt have time to bike places instead of driving\": \"categorical\",\\n    \"Biking_Comfort_A path or trail separated from the street\": \"categorical\",\\n    \"Biking_Comfort_A quiet residential street\": \"categorical\",\\n    \"Biking_Comfort_A quiet residential street with bike route signs and shared-use lane or sharrow markings?\": \"categorical\",\\n    \"Biking_Comfort_Residential street with bike route signs and shared-use lane or sharrow markings, and things like traffic diverters that slow down and discourage car traffic?\": \"categorical\",\\n    \"Biking_Comfort_Neighbourhood commercial shopping street with one lane in each direction, traffic speeds of 40 to 50 km/hr, on-street car parking and no reserved bike lane?\": \"categorical\",\\n    \"Biking_comfort2_A neighbourhood commercial shopping street with one lane in each direction, traffic speeds of 40 to 50 km/hour, on-street car parking and a reserved bike lane?\": \"categorical\",\\n    \"Biking_comfort2_A major street with two lanes in each direction, on-street parking, traffic speeds of 50 to 60 km/hour, and no reserved bike lane?\": \"categorical\",\\n    \"Biking_comfort2_Major street with two lanes in each direction, on-street parking, traffic speeds of 50 to 60 km/hour, and a reserved bike lane?\": \"categorical\",\\n    \"Biking_comfort2_Major street with two lanes in each direction, on-street parking, traffic speeds of 50 to 60 km/hour, and a bike lane separated from parked cars by a median?\": \"categorical\",\\n    \"OSType\": \"categorical\",\\n    \"DeviceType\": \"categorical\",\\n    \"BrowserType\": \"categorical\",\\n    \"Q3_Own_Rent (Study: Profiling Questionnaire 2014)\": \"categorical\",\\n    \"GENDER (Study: Profiling Questionnaire 2014)\": \"categorical\",\\n    \"Q7_Children (Study: Profiling Questionnaire 2014)\": \"categorical\",\\n    \"Q8_Born_Canada (Study: Profiling Questionnaire 2014)\": \"categorical\",\\n    \"Q9a_Edmonton (Study: Profiling Questionnaire 2014)\": \"categorical\",\\n    \"Q10_Own_Business (Study: Profiling Questionnaire 2014)\": \"categorical\",\\n    \"Q11_Home_Language (Study: Profiling Questionnaire 2014)\": \"categorical\",\\n    \"Q12_Employment_Status (Study: Profiling Questionnaire 2014)\": \"categorical\",\\n    \"Q13_Volunteer (Study: Profiling Questionnaire 2014)\": \"categorical\",\\n    \"Q14a_Primary_Transportation (Study: Profiling Questionnaire 2014)\": \"categorical\",\\n    \"Q14b_Secondary_Transportation (Study: Profiling Questionnaire 2014)\": \"categorical\",\\n    \"Q15_Household_Income (Study: Profiling Questionnaire 2014)\": \"categorical\",\\n    \"Q16_Education (Study: Profiling Questionnaire 2014)\": \"categorical\",\\n    \"Q17_City_Employee (Study: Profiling Questionnaire 2014)\": \"categorical\",\\n    \"FSA (Study: Profiling Questionnaire 2014)\": \"categorical\",\\n    \"Age_Rollup_Detailed (Study: Profiling Questionnaire 2014)\": \"categorical\"\\n  },\\n  \"Key facts\": [\\n    \"Each row is a survey response detailing individual and household demographics.\",\\n    \"Captures bike ridership, comfort, perceptions, and underlying reasons for travel choices.\",\\n    \"Includes categorical, datetime, and binary/float columns linked to demographics and behaviors.\",\\n    \"Contains no latitude/longitude but does include local area designations (FSA codes).\",\\n    \"Records cover distinct time points with detailed respondent metadata.\"\\n  ],\\n  \"Use-cases\": [\\n    \"Analyze cycling frequency by age, gender, or commute mode.\",\\n    \"Identify barriers to cycling among specific population groups.\",\\n    \"Profile household income and education patterns among respondents.\",\\n    \"Evaluate effects of perceived safety on bike ridership.\",\\n    \"Support city planning or infrastructure targeting based on travel and demographic trends.\"\\n  ],\\n  \"Caveats\": [\\n    \"Contains no direct geographic coordinates, only FSA codes.\",\\n    \"Survey-based: may reflect sampling or response biases.\",\\n    \"Unknown completeness; missing value rates are not summarized.\",\\n    \"Time period and coverage may not represent full population diversity.\",\\n    \"Granularity may limit very localized or continuous spatial analysis.\"\\n  ],\\n  \"Overview\": \"This dataset consists of individual-level survey responses that combine detailed travel behavior, cycling attitudes, and demographics. Key columns include bike ridership frequency (Bike_Ridership), perceptions of comfort and safety (Biking_Comfort_*), and agreement with various cycling-related statements (Bike_Agreement_*). Demographic data is recorded in columns like GENDER (Study: Profiling Questionnaire 2014), Q15_Household_Income, Q16_Education, and Age_Rollup_Detailed, along with household characteristics such as home ownership, number of children, and employment status. While it lacks precise geographic coordinates, local area is indicated via FSA codes, allowing for some spatial segmentation. Time-specific fields (ResponseDate, CompletionDate) anchor responses for temporal analyses. This dataset is especially suitable for equity-driven urban planning, monitoring cycling infrastructure impacts, and understanding differences in transportation behavior across population subgroups.\"\\n}'),\n",
       " ('auctus/dataset_has_description/Chicago_Park_District_Activities.csv',\n",
       "  '{\\n  \"Columns\": {\\n    \"Activity ID\": \"integer\",\\n    \"Type\": \"categorical\",\\n    \"Title\": \"text\",\\n    \"Description\": \"text\",\\n    \"Start Date\": \"datetime\",\\n    \"End Date\": \"datetime\",\\n    \"Date Notes\": \"text\",\\n    \"Season\": \"categorical\",\\n    \"Zone\": \"categorical\",\\n    \"Location Facility\": \"text\",\\n    \"Location Notes\": \"categorical\",\\n    \"Age Range\": \"categorical\",\\n    \"Activity Type\": \"categorical\",\\n    \"Category\": \"text\",\\n    \"Fee\": \"float\",\\n    \"Image Link\": \"categorical\",\\n    \"Movie Rating\": \"float\",\\n    \"Movie Title\": \"float\",\\n    \"Restrictions\": \"float\",\\n    \"Information Link\": \"text\",\\n    \"Registration Link\": \"text\",\\n    \"Registration Date\": \"datetime\",\\n    \"Event Cancelled\": \"categorical\",\\n    \"Address\": \"text\",\\n    \"ZIP\": \"float\",\\n    \"Latitude\": \"float\",\\n    \"Longitude\": \"float\",\\n    \"Location\": \"text\"\\n  },\\n  \"Key facts\": [\\n    \"Each row describes a program or event in a city, with related geographic and scheduling information.\",\\n    \"Columns include dates, locations, fees, activity types, and age ranges for urban activities.\",\\n    \"Contains text-heavy descriptions and categorical tags such as Season, Zone, and Activity Type.\",\\n    \"Several columns support geospatial analysis, including ZIP, Latitude, Longitude, and Address.\",\\n    \"Temporal fields allow for tracking by Start Date, End Date, and Registration Date.\"\\n  ],\\n  \"Use-cases\": [\\n    \"Map urban activity access and variety across neighborhoods or ZIP codes.\",\\n    \"Analyze seasonal or temporal trends in program availability and event cancellations.\",\\n    \"Identify age group coverage and gaps in urban recreational opportunities.\",\\n    \"Benchmark urban quality-of-life offerings against other cities‚Äô datasets.\",\\n    \"Support operational scheduling, registration management, and resource planning for city events.\"\\n  ],\\n  \"Caveats\": [\\n    \"Actual number of records and time range covered are unspecified.\",\\n    \"No true city-wide socio-economic or environmental indicators included.\",\\n    \"Possible inconsistency in how geospatial columns are populated or used.\",\\n    \"Movie-related fields are present but seem unused or incomplete.\",\\n    \"Data focuses more on event logistics than direct quality-of-life metrics.\"\\n  ],\\n  \"Overview\": \"This dataset catalogs city programs and events, where each row describes an individual activity held at a specific location and time. Key columns include Activity ID, Type, Title, Description, Start Date, End Date, Age Range, Activity Type, Address, Fee, ZIP, Latitude, and Longitude. Broad geographic coverage is supported at the neighborhood level via latitude, longitude, and ZIP, while temporal fields enable analysis by season or date. The inclusion of text fields like Description and Category allows for nuanced topic or theme analysis. Although not a comprehensive quality-of-life or indicator dataset, it offers a detailed snapshot of urban recreational and cultural opportunities, supporting cross-city comparisons or evaluations of accessibility, diversity, and scheduling of city-sponsored activities.\"\\n}')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Robust corpus builder from dataset_descriptions/ JSONs (schema-agnostic) ---\n",
    "\n",
    "import os\n",
    "import json\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "BUCKET_NAME = \"nyu-tandon-cs-gy-6513-project\"\n",
    "OUTPUT_PREFIX = \"dataset_descriptions/\"\n",
    "\n",
    "AUCTUS_NO_DESC_PREFIX = \"auctus/dataset_no_description/\"\n",
    "AUCTUS_HAS_DESC_PREFIX = \"auctus/dataset_has_description/\"\n",
    "NYC_PREFIX = \"nyc_open_data/\"\n",
    "\n",
    "\n",
    "def list_json_keys(bucket, prefix):\n",
    "    keys = []\n",
    "    token = None\n",
    "    while True:\n",
    "        kwargs = {\"Bucket\": bucket, \"Prefix\": prefix}\n",
    "        if token:\n",
    "            kwargs[\"ContinuationToken\"] = token\n",
    "        resp = s3.list_objects_v2(**kwargs)\n",
    "        for obj in resp.get(\"Contents\", []):\n",
    "            k = obj[\"Key\"]\n",
    "            if k.lower().endswith(\".json\"):\n",
    "                keys.append(k)\n",
    "        if resp.get(\"IsTruncated\"):\n",
    "            token = resp.get(\"NextContinuationToken\")\n",
    "        else:\n",
    "            break\n",
    "    return keys\n",
    "\n",
    "\n",
    "def read_json(bucket, key) -> Dict[str, Any]:\n",
    "    raw = s3.get_object(Bucket=bucket, Key=key)[\"Body\"].read().decode(\"utf-8\", errors=\"ignore\")\n",
    "    return json.loads(raw)\n",
    "\n",
    "\n",
    "def extract_any_overview(obj: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Try many possible shapes:\n",
    "    - top-level Overview/summary/description\n",
    "    - nested generated\n",
    "    - if obj itself is already the parsed LLM summary dict\n",
    "    \"\"\"\n",
    "    if not isinstance(obj, dict):\n",
    "        return \"\"\n",
    "\n",
    "    # 1) Direct keys\n",
    "    for k in [\"Overview\", \"overview\", \"Summary\", \"summary\", \"Description\", \"description\",\n",
    "              \"llm_overview\", \"generated_overview\"]:\n",
    "        v = obj.get(k)\n",
    "        if isinstance(v, str) and v.strip():\n",
    "            return v.strip()\n",
    "\n",
    "    # 2) Nested containers\n",
    "    for container_key in [\"generated\", \"output\", \"result\", \"data\"]:\n",
    "        c = obj.get(container_key)\n",
    "        if isinstance(c, dict):\n",
    "            for k in [\"Overview\", \"overview\", \"Summary\", \"summary\", \"Description\", \"description\"]:\n",
    "                v = c.get(k)\n",
    "                if isinstance(v, str) and v.strip():\n",
    "                    return v.strip()\n",
    "\n",
    "    # 3) If there is a \"summary\" field that looks like a JSON string\n",
    "    s = obj.get(\"summary\")\n",
    "    if isinstance(s, str) and s.strip():\n",
    "        # Try your existing parser if available\n",
    "        try:\n",
    "            j = parse_llm_summary(s)\n",
    "            ov = extract_overview_from_summary(j)\n",
    "            if isinstance(ov, str) and ov.strip():\n",
    "                return ov.strip()\n",
    "        except Exception:\n",
    "            # If summary is plain text, keep it as fallback\n",
    "            if len(s.strip()) > 20:\n",
    "                return s.strip()\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def derive_dataset_key_from_result_key(result_key: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Given:\n",
    "      dataset_descriptions/Foo_description.json\n",
    "    derive likely dataset CSV keys:\n",
    "      nyc_open_data/Foo.csv\n",
    "      auctus/dataset_has_description/Foo.csv\n",
    "      auctus/dataset_no_description/Foo.csv\n",
    "    \"\"\"\n",
    "    base = os.path.basename(result_key)\n",
    "\n",
    "    # Strip suffix patterns\n",
    "    name = base\n",
    "    if name.lower().endswith(\"_description.json\"):\n",
    "        name = name[:-len(\"_description.json\")]\n",
    "    elif name.lower().endswith(\".json\"):\n",
    "        name = name[:-len(\".json\")]\n",
    "\n",
    "    # Re-add csv extension\n",
    "    fname = name + \".csv\"\n",
    "\n",
    "    return [\n",
    "        NYC_PREFIX + fname,\n",
    "        AUCTUS_HAS_DESC_PREFIX + fname,\n",
    "        AUCTUS_NO_DESC_PREFIX + fname,\n",
    "    ]\n",
    "\n",
    "\n",
    "def build_augmented_corpus_from_result_jsons(\n",
    "    bucket=BUCKET_NAME,\n",
    "    prefix=OUTPUT_PREFIX\n",
    ") -> Dict[str, str]:\n",
    "    keys = list_json_keys(bucket, prefix)\n",
    "    print(\"Result JSON count:\", len(keys))\n",
    "\n",
    "    corpus = {}\n",
    "\n",
    "    for rk in keys:\n",
    "        try:\n",
    "            obj = read_json(bucket, rk)\n",
    "\n",
    "            # Prefer explicit dataset_key if present\n",
    "            dkey = obj.get(\"dataset_key\")\n",
    "            overview = extract_any_overview(obj)\n",
    "\n",
    "            if not overview:\n",
    "                continue\n",
    "\n",
    "            if isinstance(dkey, str) and dkey.strip():\n",
    "                corpus[dkey.strip()] = overview\n",
    "                continue\n",
    "\n",
    "            # Otherwise attempt to derive from filename\n",
    "            candidates = derive_dataset_key_from_result_key(rk)\n",
    "            # Store all candidates; downstream you can align by intersection with query_set\n",
    "            for c in candidates:\n",
    "                corpus.setdefault(c, overview)\n",
    "\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    return corpus\n",
    "\n",
    "\n",
    "augmented_corpus = build_augmented_corpus_from_result_jsons()\n",
    "print(\"Augmented corpus size (robust):\", len(augmented_corpus))\n",
    "\n",
    "# show a few\n",
    "list(augmented_corpus.items())[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZIXFep0-u9I0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline corpus size (sources): 1297\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('auctus/dataset_has_description/2007_-_2008_School_Progress_Reports_-_All_Schools.csv',\n",
       "  'Description: 2007/08 Progress Report results for all schools (data as of 1/13/09) Peer indices are calculated differently depending on School Level. Schools are only compared to other schools in the same School Level (e.g., Elementary, K-8, Middle, High) 1) Elementary & K-8 - peer index is a value from 0-100. We use a composite demographic statistic based on % ELL, % SpEd, % Title I free lunch, and % Black/Hispanic. Higher values indicate student populations with higher need. 2) Middle & High - peer index is a value from 1.00-4.50. For middle schools, we use the average 4th grade proficiency ratings in ELA and Math for all their students that have 4th grade test scores. For high schools, we use the average 8th grade proficiency ratings in ELA and Math for all their students that have 8th grade test scores, % SpEd, and % Overage. Lower values indicate student populations with higher need. 3) Schools for Transfer Students - peer index is a value from 1.00-4.50. We use the average 8th grade proficiency ratings in ELA and Math for all their students that have 8th grade test scores and the % Overage/Under credited. Lower values indicate student populations with higher need. Unlike Elementary, Middle, and High School Progress Reports, the Environment Category is only composed of Survey Results.'),\n",
       " ('auctus/dataset_has_description/Atomic_Polygons.csv',\n",
       "  'Description: Atomic polygons serve as a set of basic building blocks for generating the polygons of many of the district types represented in the CSCL database. Feature classes such as election district, school district, census block, FDNY administrative company, and community district can be dissolved by combining the appropriate fields in atomic polygons. All previously released versions of this data are available at BYTES of the BIG APPLE- Archive (https://www1.nyc.gov/site/planning/data-maps/open-data/bytes-archive.page?sorts[year]=0)'),\n",
       " ('auctus/dataset_has_description/Ballfields_9K.csv',\n",
       "  'Description: River and Stream lines that represent flowlines and cartographic features such as stream centerlines and river banks.'),\n",
       " ('auctus/dataset_has_description/Bike_Ridership_-_Edmonton_Insight_Community.csv',\n",
       "  'Description: This was one single topic among many as part of the August 2014 Mixed Topic survey. Test link to view these questions: https://www.edmontoninsightcommunity.ca/R.aspx?a=76&t=1. Open from Aug 19 - 25, 2014. At the time the survey was launched survey invitations were sent to 1029 Insight Community Members. 646 members completed the survey which represents a completion rate of 63%. A total of 816 respondents completed the survey: 646 Insight Community Members and 170 using the anonymous link which will have no demographic info.'),\n",
       " ('auctus/dataset_has_description/Chicago_Park_District_Activities.csv',\n",
       "  'Description: Events and programs hosted by the Chicago Park District. Only activities that have not yet ended are shown.')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Avoids calling any previously-defined _list_s3_keys with mismatched signature.\n",
    "\n",
    "import json\n",
    "import os\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "\n",
    "BUCKET_NAME = \"nyu-tandon-cs-gy-6513-project\"\n",
    "AUCTUS_HAS_DESC_PREFIX = \"auctus/dataset_has_description/\"\n",
    "NYC_PREFIX = \"nyc_open_data/\"\n",
    "NYC_CATALOG_KEY = \"nyc_open_data/dataset_catalog.csv\"\n",
    "\n",
    "\n",
    "def list_s3_keys(bucket: str, prefix: str):\n",
    "    keys = []\n",
    "    token = None\n",
    "    while True:\n",
    "        kwargs = {\"Bucket\": bucket, \"Prefix\": prefix}\n",
    "        if token:\n",
    "            kwargs[\"ContinuationToken\"] = token\n",
    "        resp = s3.list_objects_v2(**kwargs)\n",
    "        for obj in resp.get(\"Contents\", []):\n",
    "            keys.append(obj[\"Key\"])\n",
    "        if resp.get(\"IsTruncated\"):\n",
    "            token = resp.get(\"NextContinuationToken\")\n",
    "        else:\n",
    "            break\n",
    "    return keys\n",
    "\n",
    "\n",
    "def read_s3_text(bucket: str, key: str) -> str:\n",
    "    obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "    return obj[\"Body\"].read().decode(\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "\n",
    "def read_s3_json(bucket: str, key: str):\n",
    "    return json.loads(read_s3_text(bucket, key))\n",
    "\n",
    "\n",
    "def extract_overview_from_any_json(d: dict) -> str:\n",
    "    # handle Auctus paired JSON possibilities\n",
    "    for k in [\"Overview\", \"overview\", \"summary\", \"Summary\", \"description\", \"Description\"]:\n",
    "        v = d.get(k)\n",
    "        if isinstance(v, str) and v.strip():\n",
    "            return v.strip()\n",
    "\n",
    "    if isinstance(d.get(\"generated\"), dict):\n",
    "        for k in [\"Overview\", \"overview\", \"summary\", \"description\"]:\n",
    "            v = d[\"generated\"].get(k)\n",
    "            if isinstance(v, str) and v.strip():\n",
    "                return v.strip()\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def build_auctus_baseline_map():\n",
    "    keys = list_s3_keys(BUCKET_NAME, AUCTUS_HAS_DESC_PREFIX)\n",
    "    key_set = set(keys)\n",
    "    baseline = {}\n",
    "\n",
    "    for csv_key in keys:\n",
    "        if not csv_key.lower().endswith(\".csv\"):\n",
    "            continue\n",
    "\n",
    "        json_key = csv_key.rsplit(\".\", 1)[0] + \".json\"\n",
    "        if json_key not in key_set:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            meta = read_s3_json(BUCKET_NAME, json_key)\n",
    "            ov = extract_overview_from_any_json(meta)\n",
    "            if ov:\n",
    "                baseline[csv_key] = ov\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    return baseline\n",
    "\n",
    "\n",
    "def build_nyc_baseline_map():\n",
    "    text = read_s3_text(BUCKET_NAME, NYC_CATALOG_KEY)\n",
    "    df = pd.read_csv(StringIO(text), header=None)\n",
    "\n",
    "    if df.shape[1] < 3:\n",
    "        raise ValueError(\"dataset_catalog.csv must have >= 3 columns: id, name, summary\")\n",
    "\n",
    "    baseline = {}\n",
    "    for _, row in df.iterrows():\n",
    "        name = str(row[1]).strip()\n",
    "        summary = str(row[2]).strip()\n",
    "\n",
    "        if not name or not summary:\n",
    "            continue\n",
    "\n",
    "        if not name.lower().endswith(\".csv\"):\n",
    "            name += \".csv\"\n",
    "\n",
    "        baseline[f\"{NYC_PREFIX}{name}\"] = summary\n",
    "\n",
    "    return baseline\n",
    "\n",
    "\n",
    "baseline_corpus = {}\n",
    "baseline_corpus.update(build_auctus_baseline_map())\n",
    "baseline_corpus.update(build_nyc_baseline_map())\n",
    "\n",
    "print(\"Baseline corpus size (sources):\", len(baseline_corpus))\n",
    "list(baseline_corpus.items())[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "6Xfd7c1-u_Bt"
   },
   "outputs": [],
   "source": [
    "# === Search impact: Offline retrieval test (robust, aligned baseline vs augmented) ===\n",
    "# Uses FAISS if available; falls back to sklearn if not.\n",
    "# Baseline descriptions are loaded from S3 sources:\n",
    "#   - Auctus: auctus/dataset_has_description/*.json paired with CSV\n",
    "#   - NYC: nyc_open_data/dataset_catalog.csv (col0 id, col1 filename, col2 summary)\n",
    "# Augmented descriptions are generated Overviews from your pipeline outputs.\n",
    "#\n",
    "# This version aligns corpora to the set of datasets you actually generated\n",
    "# so the comparison is fair and baseline isn't accidentally 0.\n",
    "\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "from io import StringIO\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------\n",
    "# Safe defaults (if not defined earlier)\n",
    "# -----------------------\n",
    "try:\n",
    "    BUCKET_NAME\n",
    "except NameError:\n",
    "    BUCKET_NAME = \"nyu-tandon-cs-gy-6513-project\"\n",
    "\n",
    "try:\n",
    "    OUTPUT_PREFIX\n",
    "except NameError:\n",
    "    OUTPUT_PREFIX = \"dataset_descriptions/\"\n",
    "\n",
    "# If you already created s3 earlier, this won't overwrite your client variable usage\n",
    "try:\n",
    "    s3\n",
    "except NameError:\n",
    "    s3 = boto3.client(\"s3\")\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Config\n",
    "# -----------------------\n",
    "QUERY_SET_S3_KEY = \"evaluation/query_set.json\"\n",
    "\n",
    "AUCTUS_HAS_DESC_PREFIX = \"auctus/dataset_has_description/\"\n",
    "NYC_PREFIX = \"nyc_open_data/\"\n",
    "NYC_CATALOG_KEY = \"nyc_open_data/dataset_catalog.csv\"\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Optional retrieval backends\n",
    "# -----------------------\n",
    "USE_FAISS = False\n",
    "try:\n",
    "    import faiss\n",
    "    USE_FAISS = True\n",
    "except Exception:\n",
    "    USE_FAISS = False\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "def build_index_and_search_fn(embeddings: np.ndarray):\n",
    "    \"\"\"\n",
    "    Returns an index object and a search_fn(query_emb, k) -> (idx, scores).\n",
    "    Uses cosine similarity.\n",
    "    \"\"\"\n",
    "    embeddings = embeddings.astype(\"float32\")\n",
    "\n",
    "    if USE_FAISS:\n",
    "        faiss.normalize_L2(embeddings)\n",
    "        index = faiss.IndexFlatIP(embeddings.shape[1])\n",
    "        index.add(embeddings)\n",
    "\n",
    "        def search_fn(query_emb: np.ndarray, k: int):\n",
    "            query_emb = query_emb.astype(\"float32\")\n",
    "            faiss.normalize_L2(query_emb)\n",
    "            scores, idx = index.search(query_emb, k)\n",
    "            return idx, scores\n",
    "\n",
    "        return index, search_fn\n",
    "\n",
    "    # sklearn fallback\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "    norms = np.linalg.norm(embeddings, axis=1, keepdims=True) + 1e-12\n",
    "    emb_norm = embeddings / norms\n",
    "\n",
    "    nn = NearestNeighbors(metric=\"cosine\")\n",
    "    nn.fit(emb_norm)\n",
    "\n",
    "    def search_fn(query_emb: np.ndarray, k: int):\n",
    "        q = query_emb.astype(\"float32\")\n",
    "        q_norm = q / (np.linalg.norm(q, axis=1, keepdims=True) + 1e-12)\n",
    "        distances, idx = nn.kneighbors(q_norm, n_neighbors=min(k, len(emb_norm)))\n",
    "        scores = 1.0 - distances\n",
    "        return idx, scores\n",
    "\n",
    "    return nn, search_fn\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# S3 helpers\n",
    "# -----------------------\n",
    "def _list_s3_keys(bucket: str, prefix: str) -> List[str]:\n",
    "    keys = []\n",
    "    token = None\n",
    "    while True:\n",
    "        kwargs = {\"Bucket\": bucket, \"Prefix\": prefix}\n",
    "        if token:\n",
    "            kwargs[\"ContinuationToken\"] = token\n",
    "        resp = s3.list_objects_v2(**kwargs)\n",
    "        for obj in resp.get(\"Contents\", []):\n",
    "            keys.append(obj[\"Key\"])\n",
    "        if resp.get(\"IsTruncated\"):\n",
    "            token = resp.get(\"NextContinuationToken\")\n",
    "        else:\n",
    "            break\n",
    "    return keys\n",
    "\n",
    "\n",
    "def _read_s3_text(bucket: str, key: str) -> str:\n",
    "    obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "    return obj[\"Body\"].read().decode(\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "\n",
    "def _read_s3_json(bucket: str, key: str) -> Dict[str, Any]:\n",
    "    return json.loads(_read_s3_text(bucket, key))\n",
    "\n",
    "\n",
    "def _clean_text(t: str) -> str:\n",
    "    t = re.sub(r\"\\s+\", \" \", str(t))\n",
    "    return t.strip()\n",
    "\n",
    "\n",
    "def _extract_overview_from_any_json(d: Dict[str, Any]) -> str:\n",
    "    # Handles Auctus paired JSON and possible pipeline variants\n",
    "    for k in [\"Overview\", \"overview\", \"summary\", \"Summary\", \"description\", \"Description\"]:\n",
    "        v = d.get(k)\n",
    "        if isinstance(v, str) and v.strip():\n",
    "            return v.strip()\n",
    "\n",
    "    if isinstance(d.get(\"generated\"), dict):\n",
    "        for k in [\"Overview\", \"overview\", \"summary\", \"description\"]:\n",
    "            v = d[\"generated\"].get(k)\n",
    "            if isinstance(v, str) and v.strip():\n",
    "                return v.strip()\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Load pipeline results\n",
    "# -----------------------\n",
    "def _iter_json_keys(bucket: str, prefix: str) -> List[str]:\n",
    "    keys = _list_s3_keys(bucket, prefix)\n",
    "    return [k for k in keys if k.lower().endswith(\".json\")]\n",
    "\n",
    "\n",
    "def load_pipeline_results(\n",
    "    bucket: str = BUCKET_NAME,\n",
    "    output_prefix: str = OUTPUT_PREFIX\n",
    ") -> List[Dict[str, Any]]:\n",
    "    results = []\n",
    "    json_keys = _iter_json_keys(bucket, output_prefix)\n",
    "\n",
    "    for key in json_keys:\n",
    "        try:\n",
    "            raw = _read_s3_text(bucket, key)\n",
    "            results.append(json.loads(raw))\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: could not read {key}: {e}\")\n",
    "\n",
    "    print(f\"Loaded {len(results)} pipeline result JSON files from {output_prefix}\")\n",
    "    return results\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Build baseline map from S3 sources\n",
    "# -----------------------\n",
    "def build_auctus_baseline_map(\n",
    "    bucket: str = BUCKET_NAME,\n",
    "    prefix: str = AUCTUS_HAS_DESC_PREFIX\n",
    ") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Map: dataset_key (CSV key) -> existing description text\n",
    "    Uses paired JSON with same basename.\n",
    "    \"\"\"\n",
    "    keys = _list_s3_keys(bucket, prefix)\n",
    "    key_set = set(keys)\n",
    "\n",
    "    csv_keys = [k for k in keys if k.lower().endswith(\".csv\")]\n",
    "    baseline = {}\n",
    "\n",
    "    for csv_key in csv_keys:\n",
    "        json_key = csv_key.rsplit(\".\", 1)[0] + \".json\"\n",
    "        if json_key not in key_set:\n",
    "            continue\n",
    "        try:\n",
    "            meta = _read_s3_json(bucket, json_key)\n",
    "            overview = _extract_overview_from_any_json(meta)\n",
    "            if overview:\n",
    "                baseline[csv_key] = _clean_text(overview)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    return baseline\n",
    "\n",
    "\n",
    "def build_nyc_baseline_map(\n",
    "    bucket: str = BUCKET_NAME,\n",
    "    catalog_key: str = NYC_CATALOG_KEY,\n",
    "    prefix: str = NYC_PREFIX\n",
    ") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    NYC catalog schema:\n",
    "      col 0 = id\n",
    "      col 1 = name (same as filename)\n",
    "      col 2 = summary\n",
    "    Map: dataset_key -> summary\n",
    "    \"\"\"\n",
    "    text = _read_s3_text(bucket, catalog_key)\n",
    "    df = pd.read_csv(StringIO(text), header=None)\n",
    "\n",
    "    if df.shape[1] < 3:\n",
    "        raise ValueError(\"dataset_catalog.csv must have at least 3 columns: id, name, summary\")\n",
    "\n",
    "    baseline = {}\n",
    "    for _, row in df.iterrows():\n",
    "        name = _clean_text(row[1])\n",
    "        summary = _clean_text(row[2])\n",
    "        if not name or not summary:\n",
    "            continue\n",
    "\n",
    "        if not name.lower().endswith(\".csv\"):\n",
    "            name = name + \".csv\"\n",
    "\n",
    "        dataset_key = f\"{prefix}{name}\"\n",
    "        baseline[dataset_key] = summary\n",
    "\n",
    "    return baseline\n",
    "\n",
    "\n",
    "def build_baseline_source_map() -> Dict[str, str]:\n",
    "    baseline = {}\n",
    "    baseline.update(build_auctus_baseline_map())\n",
    "    baseline.update(build_nyc_baseline_map())\n",
    "    return baseline\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Build aligned corpora\n",
    "# -----------------------\n",
    "def build_aligned_corpora_from_results(\n",
    "    results: List[Dict[str, Any]]\n",
    ") -> Tuple[Dict[str, str], Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Returns (baseline_corpus, augmented_corpus) aligned to datasets with generated overviews.\n",
    "    Baseline text pulled from S3 sources.\n",
    "    \"\"\"\n",
    "    baseline_sources = build_baseline_source_map()\n",
    "\n",
    "    augmented = {}\n",
    "    for r in results:\n",
    "        dkey = r.get(\"dataset_key\")\n",
    "        if not dkey:\n",
    "            continue\n",
    "\n",
    "        gen_sum = r.get(\"summary\")\n",
    "        try:\n",
    "            # These should exist from your earlier notebook cells:\n",
    "            gen_json = parse_llm_summary(gen_sum) if not isinstance(gen_sum, dict) else gen_sum\n",
    "            gen_overview = extract_overview_from_summary(gen_json)\n",
    "        except Exception:\n",
    "            gen_overview = \"\"\n",
    "\n",
    "        if gen_overview:\n",
    "            augmented[dkey] = _clean_text(gen_overview)\n",
    "\n",
    "    # Align baseline to augmented keys only\n",
    "    baseline = {}\n",
    "    for dkey in augmented.keys():\n",
    "        txt = baseline_sources.get(dkey, \"\")\n",
    "        if txt:\n",
    "            baseline[dkey] = _clean_text(txt)\n",
    "\n",
    "    return baseline, augmented\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Load query set\n",
    "# -----------------------\n",
    "def load_query_set_from_s3(\n",
    "    bucket: str = BUCKET_NAME,\n",
    "    key: str = QUERY_SET_S3_KEY\n",
    ") -> List[Dict[str, Any]]:\n",
    "    try:\n",
    "        raw = _read_s3_text(bucket, key)\n",
    "        qs = json.loads(raw)\n",
    "        print(f\"Loaded query set from s3://{bucket}/{key} with {len(qs)} queries.\")\n",
    "        return qs\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load query set from s3://{bucket}/{key}: {e}\")\n",
    "        print(\"Falling back to a small placeholder template.\")\n",
    "        return [\n",
    "            {\"query\": \"public safety incidents\", \"relevant_datasets\": []},\n",
    "            {\"query\": \"housing and rent\", \"relevant_datasets\": []},\n",
    "        ]\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Evaluation\n",
    "# -----------------------\n",
    "def evaluate_query_set(\n",
    "    corpus: Dict[str, str],\n",
    "    query_set: List[Dict[str, Any]],\n",
    "    model_name: str = \"all-MiniLM-L6-v2\",\n",
    "    k: int = 5\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Macro-averaged precision@k and recall@k across labeled queries.\n",
    "    Always returns num_labeled_queries.\n",
    "    \"\"\"\n",
    "    labeled_queries = [\n",
    "        q for q in query_set\n",
    "        if str(q.get(\"query\", \"\")).strip() and (q.get(\"relevant_datasets\") or [])\n",
    "    ]\n",
    "\n",
    "    if not corpus:\n",
    "        return {\n",
    "            \"model_name\": model_name,\n",
    "            \"k\": k,\n",
    "            \"precision_at_k\": 0.0,\n",
    "            \"recall_at_k\": 0.0,\n",
    "            \"per_query\": [],\n",
    "            \"num_labeled_queries\": 0,\n",
    "            \"num_queries_total\": len(query_set),\n",
    "            \"num_queries_with_labels\": len(labeled_queries),\n",
    "        }\n",
    "\n",
    "    model = SentenceTransformer(model_name)\n",
    "\n",
    "    dataset_keys = list(corpus.keys())\n",
    "    texts = [corpus[k_] for k_ in dataset_keys]\n",
    "\n",
    "    emb = model.encode(texts, convert_to_numpy=True, show_progress_bar=True)\n",
    "    _, search_fn = build_index_and_search_fn(emb)\n",
    "\n",
    "    per_query = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "\n",
    "    for q in query_set:\n",
    "        query = str(q.get(\"query\", \"\")).strip()\n",
    "        relevant = set(q.get(\"relevant_datasets\", []) or [])\n",
    "\n",
    "        if not query:\n",
    "            continue\n",
    "\n",
    "        q_emb = model.encode([query], convert_to_numpy=True)\n",
    "        idx, scores = search_fn(q_emb, k)\n",
    "\n",
    "        retrieved_keys = [dataset_keys[i] for i in idx[0] if i < len(dataset_keys)]\n",
    "        retrieved_set = set(retrieved_keys)\n",
    "\n",
    "        hit_count = len(retrieved_set & relevant)\n",
    "\n",
    "        precision = hit_count / float(k) if k else 0.0\n",
    "        recall = hit_count / float(len(relevant)) if relevant else 0.0\n",
    "\n",
    "        per_query.append({\n",
    "            \"query\": query,\n",
    "            \"k\": k,\n",
    "            \"retrieved\": retrieved_keys,\n",
    "            \"relevant\": list(relevant),\n",
    "            \"precision_at_k\": precision,\n",
    "            \"recall_at_k\": recall,\n",
    "        })\n",
    "\n",
    "        if relevant:\n",
    "            precisions.append(precision)\n",
    "            recalls.append(recall)\n",
    "\n",
    "    precision_macro = float(np.mean(precisions)) if precisions else 0.0\n",
    "    recall_macro = float(np.mean(recalls)) if recalls else 0.0\n",
    "\n",
    "    return {\n",
    "        \"model_name\": model_name,\n",
    "        \"k\": k,\n",
    "        \"precision_at_k\": precision_macro,\n",
    "        \"recall_at_k\": recall_macro,\n",
    "        \"per_query\": per_query,\n",
    "        \"num_labeled_queries\": len(precisions),\n",
    "        \"num_queries_total\": len(query_set),\n",
    "        \"num_queries_with_labels\": len(labeled_queries),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "DvmLdZn46546"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluable labeled queries:\n",
      "  Baseline : 925\n",
      "  Augmented: 1292\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce946817aea84f0faa19440d528c403f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46045f0000aa4084aa6499dad4db9b8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Retrieval Impact Summary ===\n",
      "Baseline:\n",
      "{\n",
      "  \"k\": 5,\n",
      "  \"precision_at_k\": 0.06616216216216217,\n",
      "  \"recall_at_k\": 0.3308108108108108,\n",
      "  \"num_labeled_queries\": 925\n",
      "}\n",
      "Augmented (Generated only):\n",
      "{\n",
      "  \"k\": 5,\n",
      "  \"precision_at_k\": 0.09659442724458205,\n",
      "  \"recall_at_k\": 0.48297213622291024,\n",
      "  \"num_labeled_queries\": 1292\n",
      "}\n",
      "\n",
      "Œî vs Baseline:\n",
      "  Augmented Œî Precision@k: 0.0304\n",
      "  Augmented Œî Recall@k:    0.1522\n"
     ]
    }
   ],
   "source": [
    "def filter_query_set_to_corpus(query_set, corpus):\n",
    "    corpus_keys = set(corpus.keys())\n",
    "    filtered = []\n",
    "    for q in query_set:\n",
    "        rel = set(q.get(\"relevant_datasets\", []) or [])\n",
    "        if not rel:\n",
    "            continue\n",
    "        # keep only queries whose relevant datasets exist in this corpus\n",
    "        if rel & corpus_keys:\n",
    "            filtered.append(q)\n",
    "    return filtered\n",
    "\n",
    "baseline_qs  = filter_query_set_to_corpus(query_set, baseline_corpus)\n",
    "augmented_qs = filter_query_set_to_corpus(query_set, augmented_corpus)\n",
    "\n",
    "print(\"Evaluable labeled queries:\")\n",
    "print(\"  Baseline :\", len(baseline_qs))\n",
    "print(\"  Augmented:\", len(augmented_qs))\n",
    "\n",
    "baseline_metrics  = evaluate_query_set(baseline_corpus,  baseline_qs,  k=5)\n",
    "augmented_metrics = evaluate_query_set(augmented_corpus, augmented_qs, k=5)\n",
    "\n",
    "print(\"\\n=== Retrieval Impact Summary ===\")\n",
    "print(\"Baseline:\")\n",
    "print(json.dumps({\n",
    "    \"k\": baseline_metrics.get(\"k\"),\n",
    "    \"precision_at_k\": baseline_metrics.get(\"precision_at_k\"),\n",
    "    \"recall_at_k\": baseline_metrics.get(\"recall_at_k\"),\n",
    "    \"num_labeled_queries\": baseline_metrics.get(\"num_labeled_queries\", 0),\n",
    "}, indent=2))\n",
    "\n",
    "print(\"Augmented (Generated only):\")\n",
    "print(json.dumps({\n",
    "    \"k\": augmented_metrics.get(\"k\"),\n",
    "    \"precision_at_k\": augmented_metrics.get(\"precision_at_k\"),\n",
    "    \"recall_at_k\": augmented_metrics.get(\"recall_at_k\"),\n",
    "    \"num_labeled_queries\": augmented_metrics.get(\"num_labeled_queries\", 0),\n",
    "}, indent=2))\n",
    "\n",
    "print(\"\\nŒî vs Baseline:\")\n",
    "print(\"  Augmented Œî Precision@k:\", round(\n",
    "    augmented_metrics[\"precision_at_k\"] - baseline_metrics[\"precision_at_k\"], 4\n",
    "))\n",
    "print(\"  Augmented Œî Recall@k:   \", round(\n",
    "    augmented_metrics[\"recall_at_k\"] - baseline_metrics[\"recall_at_k\"], 4\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatic Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ab-nbKycw2cf"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95da4d84303645a68d895e5ab615b88a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Post-run ROUGE/BERTScore:   0%|          | 0/1302 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì evaluated 25 so far...\n",
      "‚úì evaluated 50 so far...\n",
      "‚úì evaluated 75 so far...\n",
      "‚úì evaluated 100 so far...\n",
      "‚úì evaluated 125 so far...\n",
      "‚úì evaluated 150 so far...\n",
      "‚úì evaluated 175 so far...\n",
      "‚úì evaluated 200 so far...\n",
      "‚úì evaluated 225 so far...\n",
      "‚úì evaluated 250 so far...\n",
      "‚úì evaluated 275 so far...\n",
      "‚úì evaluated 300 so far...\n",
      "‚úì evaluated 325 so far...\n",
      "‚úì evaluated 350 so far...\n",
      "‚úì evaluated 375 so far...\n",
      "‚úì evaluated 400 so far...\n",
      "‚úì evaluated 425 so far...\n",
      "‚úì evaluated 450 so far...\n",
      "‚úì evaluated 475 so far...\n",
      "‚úì evaluated 500 so far...\n",
      "‚úì evaluated 525 so far...\n",
      "‚úì evaluated 550 so far...\n",
      "‚úì evaluated 575 so far...\n",
      "‚úì evaluated 600 so far...\n",
      "‚úì evaluated 625 so far...\n",
      "‚úì evaluated 650 so far...\n",
      "‚úì evaluated 675 so far...\n",
      "‚úì evaluated 700 so far...\n",
      "‚úì evaluated 725 so far...\n",
      "‚úì evaluated 750 so far...\n",
      "‚úì evaluated 775 so far...\n",
      "‚úì evaluated 800 so far...\n",
      "‚úì evaluated 825 so far...\n",
      "‚úì evaluated 850 so far...\n",
      "‚úì evaluated 875 so far...\n",
      "Evaluated: 896 | Skipped (missing/errored): 300\n",
      "Average ROUGE-L F1:   0.0959\n",
      "Average BERTScore F1: 0.8108\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'evaluated': 896,\n",
       " 'skipped': 300,\n",
       " 'avg_rougeL_f1': 0.09590355196326665,\n",
       " 'avg_bertscore_f1': 0.8108155442667859}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "from evaluate import load\n",
    "import json\n",
    "import statistics\n",
    "from tqdm.auto import tqdm \n",
    "\n",
    "def _read_json_from_s3(bucket: str, key: str):\n",
    "    obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "    raw = obj[\"Body\"].read().decode(\"utf-8\")\n",
    "    return json.loads(raw)\n",
    "\n",
    "def compute_postrun_avg_rouge_bertscore(\n",
    "    bucket_name: str = BUCKET_NAME,\n",
    "    output_prefix: str = OUTPUT_PREFIX,\n",
    "):\n",
    "    # Collect output json keys\n",
    "    keys = _list_keys(bucket_name, output_prefix)\n",
    "    keys = [k for k in keys if k.lower().endswith(\".json\")]\n",
    "\n",
    "    if not keys:\n",
    "        print(f\"No JSON outputs found under s3://{bucket_name}/{output_prefix}\")\n",
    "        return None\n",
    "\n",
    "    scorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
    "    bertscore = load(\"bertscore\")\n",
    "\n",
    "    rouge_f1s = []\n",
    "    bert_f1s = []\n",
    "    evaluated = 0\n",
    "    skipped = 0\n",
    "\n",
    "    candidate_keys = []\n",
    "    for key in keys:\n",
    "        candidate_keys.append(key)\n",
    "\n",
    "    for i, key in enumerate(tqdm(candidate_keys, desc=\"Post-run ROUGE/BERTScore\")):\n",
    "        try:\n",
    "            data = _read_json_from_s3(bucket_name, key)\n",
    "\n",
    "            # Only evaluate where we actually have an existing reference\n",
    "            if data.get(\"run_type\") != \"has_existing_description\":\n",
    "                continue\n",
    "\n",
    "            reference = (data.get(\"existing_overview\") or \"\").strip()\n",
    "            generated = extract_overview_from_summary(data.get(\"summary\"))\n",
    "\n",
    "            if not reference or not generated:\n",
    "                skipped += 1\n",
    "                continue\n",
    "\n",
    "            # ROUGE-L F1\n",
    "            r = scorer.score(reference, generated)[\"rougeL\"].fmeasure\n",
    "\n",
    "            # BERTScore F1\n",
    "            b = bertscore.compute(\n",
    "                predictions=[generated],\n",
    "                references=[reference],\n",
    "                lang=\"en\",\n",
    "            )\n",
    "            b_f1 = float(b[\"f1\"][0])\n",
    "\n",
    "            rouge_f1s.append(float(r))\n",
    "            bert_f1s.append(b_f1)\n",
    "            evaluated += 1\n",
    "\n",
    "            # Optional: occasional textual progress checkpoint\n",
    "            if evaluated % 25 == 0:\n",
    "                print(f\"‚úì evaluated {evaluated} so far...\")\n",
    "\n",
    "        except Exception as e:\n",
    "            skipped += 1\n",
    "            print(f\"Skip {key} due to error: {e}\")\n",
    "\n",
    "    if evaluated == 0:\n",
    "        print(\"No valid HAS-DESC outputs with both generated + existing Overview found.\")\n",
    "        return None\n",
    "\n",
    "    avg_rouge = statistics.mean(rouge_f1s)\n",
    "    avg_bert = statistics.mean(bert_f1s)\n",
    "\n",
    "    print(f\"Evaluated: {evaluated} | Skipped (missing/errored): {skipped}\")\n",
    "    print(f\"Average ROUGE-L F1:   {avg_rouge:.4f}\")\n",
    "    print(f\"Average BERTScore F1: {avg_bert:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"evaluated\": evaluated,\n",
    "        \"skipped\": skipped,\n",
    "        \"avg_rougeL_f1\": avg_rouge,\n",
    "        \"avg_bertscore_f1\": avg_bert,\n",
    "    }\n",
    "\n",
    "# Run it\n",
    "compute_postrun_avg_rouge_bertscore()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oiwxb533YxjM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import io\n",
    "import contextlib\n",
    "\n",
    "import gradio as gr\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def _load_dataframe_from_file(file_obj) -> pd.DataFrame:\n",
    "    \"\"\"Helper to load a pandas DataFrame from a Gradio File input.\n",
    "    Supports CSV, Parquet, and Excel; defaults to CSV.\"\"\"\n",
    "    if file_obj is None:\n",
    "        raise ValueError(\"No file uploaded.\")\n",
    "\n",
    "    if isinstance(file_obj, dict):\n",
    "        path = file_obj.get(\"name\")\n",
    "    else:\n",
    "        path = getattr(file_obj, \"name\", None)\n",
    "\n",
    "    if path is None:\n",
    "        # Fall back to reading bytes as CSV\n",
    "        return pd.read_csv(io.BytesIO(file_obj.read()))\n",
    "\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "\n",
    "    if ext in [\".csv\", \".txt\"]:\n",
    "        return pd.read_csv(path)\n",
    "    elif ext == \".parquet\":\n",
    "        return pd.read_parquet(path)\n",
    "    elif ext in [\".xlsx\", \".xls\"]:\n",
    "        return pd.read_excel(path)\n",
    "    else:\n",
    "        # Default: try CSV\n",
    "        return pd.read_csv(path)\n",
    "\n",
    "\n",
    "def run_full_pipeline(file_obj):\n",
    "    \"\"\"Run the existing pipeline on an uploaded dataset and\n",
    "    return profiling, signals, summary, and evaluation outputs.\"\"\"\n",
    "    if file_obj is None:\n",
    "        return (\n",
    "            {\"error\": \"Please upload a dataset file.\"},\n",
    "            {},\n",
    "            {},\n",
    "            \"No evaluation (no dataset).\",\n",
    "        )\n",
    "\n",
    "    # 1. Load dataset\n",
    "    df = _load_dataframe_from_file(file_obj)\n",
    "\n",
    "    # 2. Profiling\n",
    "    profile = profile_dataframe(df)\n",
    "\n",
    "    # 3. Build signals\n",
    "    # Wrap the profiler output into a \"dataset\" + \"columns\" structure\n",
    "    profile_for_signals = {\n",
    "        \"dataset\": {\n",
    "            \"num_rows\": profile.get(\"num_rows\"),\n",
    "            \"num_columns\": profile.get(\"num_columns\"),\n",
    "            \"geospatial_hints\": profile.get(\"geospatial_hints\"),\n",
    "            \"primary_key_candidates\": profile.get(\"primary_key_candidates\"),\n",
    "            \"type_counts\": profile.get(\"type_counts\"),\n",
    "            \"time_coverage\": profile.get(\"time_coverage\"),\n",
    "        },\n",
    "        \"columns\": profile.get(\"columns\", {}),\n",
    "    }\n",
    "    signals = build_data_signals(profile_for_signals)\n",
    "\n",
    "    # 4. LLM summary \n",
    "    summary_str = summarize_dataset_with_llm(\n",
    "        profile,\n",
    "        signals,\n",
    "        side_info_text=None,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        summary_json = parse_llm_summary(summary_str)\n",
    "    except Exception:\n",
    "        summary_json = {\"raw_summary\": summary_str}\n",
    "\n",
    "\n",
    "    # Deterministic column/type verification\n",
    "    deterministic_eval = verify_column_names_and_types_deterministic(\n",
    "        ground_truth_profile=profile,\n",
    "        generated_summary_str=summary_str,\n",
    "    )\n",
    "\n",
    "    # ROUGE / BERTScore\n",
    "    eval_text_buf = io.StringIO()\n",
    "    with contextlib.redirect_stdout(eval_text_buf):\n",
    "        try:\n",
    "            evaluate_summary_vs_baseline(summary_json, baseline_summary)\n",
    "        except Exception as e:\n",
    "            print(f\"Error while computing ROUGE/BERTScore: {e}\")\n",
    "    rouge_bert_text = eval_text_buf.getvalue()\n",
    "\n",
    "    # 6. LLM-based verification of the summary against the profile\n",
    "    try:\n",
    "        llm_verification_report = verify_summary_with_llm(\n",
    "            ground_truth_profile=profile,\n",
    "            generated_summary_str=summary_str,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        llm_verification_report = f\"Error in LLM-based verification: {e}\"\n",
    "\n",
    "    # 7. Combine all evaluation outputs into a single text blob\n",
    "    evaluation_text = (\n",
    "        \"=== Deterministic column/type verification ===\\n\"\n",
    "        + json.dumps(deterministic_eval, indent=2, default=str)\n",
    "        + \"\\n\\n=== ROUGE / BERTScore evaluation ===\\n\"\n",
    "        + (rouge_bert_text or \"(no metrics computed)\")\n",
    "        + \"\\n\\n=== LLM-based verification report ===\\n\"\n",
    "        + str(llm_verification_report)\n",
    "    )\n",
    "\n",
    "    # Return:\n",
    "    #  - raw profiler dict\n",
    "    #  - signals dict\n",
    "    #  - parsed LLM summary (JSON-like)\n",
    "    #  - combined evaluation text\n",
    "    return profile, signals, summary_json, evaluation_text\n",
    "\n",
    "\n",
    "with gr.Blocks() as dataset_description_app:\n",
    "    gr.Markdown(\"# LLM Dataset Description Pipeline UI\")\n",
    "    gr.Markdown(\n",
    "        \"Upload a tabular dataset to run the full pipeline: \"\n",
    "        \"profiling ‚Üí signal extraction ‚Üí LLM summary ‚Üí evaluation.\"\n",
    "    )\n",
    "\n",
    "    with gr.Row():\n",
    "        file_input = gr.File(\n",
    "            label=\"Upload dataset (CSV, Parquet, or Excel)\",\n",
    "            file_types=[\".csv\", \".txt\", \".parquet\", \".xlsx\", \".xls\"],\n",
    "        )\n",
    "\n",
    "    run_button = gr.Button(\"Run pipeline\")\n",
    "\n",
    "    with gr.Tab(\"Profiling\"):\n",
    "        profile_output = gr.JSON(label=\"Profiler output\")\n",
    "    with gr.Tab(\"Signals\"):\n",
    "        signals_output = gr.JSON(label=\"Signals output\")\n",
    "    with gr.Tab(\"LLM Summary\"):\n",
    "        summary_output = gr.JSON(label=\"Generated dataset summary\")\n",
    "    with gr.Tab(\"Evaluation\"):\n",
    "        evaluation_output = gr.Textbox(\n",
    "            label=\"Evaluation outputs (deterministic checks + ROUGE/BERTScore + LLM verification)\",\n",
    "            lines=20,\n",
    "        )\n",
    "\n",
    "    run_button.click(\n",
    "        fn=run_full_pipeline,\n",
    "        inputs=[file_input],\n",
    "        outputs=[profile_output, signals_output, summary_output, evaluation_output],\n",
    "    )\n",
    "\n",
    "dataset_description_app.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Compare description lengths by type (existing vs no-existing) ===\n",
    "# For datasets WITH existing descriptions:\n",
    "#   - compare existing_overview vs generated Overview (word count)\n",
    "#   - compute average % change in length\n",
    "# For datasets WITHOUT existing descriptions:\n",
    "#   - report average generated Overview length only\n",
    "\n",
    "import json\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "# Assumes these already exist:\n",
    "#   - s3\n",
    "#   - BUCKET_NAME\n",
    "#   - OUTPUT_PREFIX = \"dataset_descriptions/\"\n",
    "#   - parse_llm_summary\n",
    "#   - extract_overview_from_summary\n",
    "\n",
    "\n",
    "def _iter_json_keys_for_outputs(\n",
    "    bucket: str,\n",
    "    prefix: str\n",
    ") -> List[str]:\n",
    "    \"\"\"List all JSON result keys under the pipeline output prefix.\"\"\"\n",
    "    keys = []\n",
    "    token = None\n",
    "    while True:\n",
    "        kwargs = {\"Bucket\": bucket, \"Prefix\": prefix}\n",
    "        if token:\n",
    "            kwargs[\"ContinuationToken\"] = token\n",
    "        resp = s3.list_objects_v2(**kwargs)\n",
    "        for obj in resp.get(\"Contents\", []):\n",
    "            key = obj[\"Key\"]\n",
    "            if key.lower().endswith(\".json\"):\n",
    "                keys.append(key)\n",
    "        if resp.get(\"IsTruncated\"):\n",
    "            token = resp.get(\"NextContinuationToken\")\n",
    "        else:\n",
    "            break\n",
    "    return keys\n",
    "\n",
    "\n",
    "def load_pipeline_results_for_lengths(\n",
    "    bucket: str = BUCKET_NAME,\n",
    "    output_prefix: str = OUTPUT_PREFIX\n",
    ") -> List[Dict[str, Any]]:\n",
    "    results = []\n",
    "    json_keys = _iter_json_keys_for_outputs(bucket, output_prefix)\n",
    "    print(f\"Found {len(json_keys)} JSON files under {output_prefix}\")\n",
    "    for key in json_keys:\n",
    "        try:\n",
    "            obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "            raw = obj[\"Body\"].read().decode(\"utf-8\")\n",
    "            results.append(json.loads(raw))\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: could not read {key}: {e}\")\n",
    "    print(f\"Loaded {len(results)} pipeline result objects.\")\n",
    "    return results\n",
    "\n",
    "\n",
    "def _extract_generated_overview_any(summary_field: Any) -> str:\n",
    "    \"\"\"\n",
    "    Robustly get the generated Overview text from a summary field.\n",
    "    Handles:\n",
    "      - already-parsed dict\n",
    "      - JSON string in your LLM format\n",
    "      - falls back to your extract_overview_from_summary(...)\n",
    "      - if parsing fails, returns raw string\n",
    "    \"\"\"\n",
    "    if isinstance(summary_field, dict):\n",
    "        try:\n",
    "            ov = extract_overview_from_summary(summary_field)\n",
    "            if isinstance(ov, str) and ov.strip():\n",
    "                return ov.strip()\n",
    "        except Exception:\n",
    "            pass\n",
    "        for k in [\"Overview\", \"overview\", \"summary\", \"Summary\", \"description\", \"Description\"]:\n",
    "            v = summary_field.get(k)\n",
    "            if isinstance(v, str) and v.strip():\n",
    "                return v.strip()\n",
    "        return \"\"\n",
    "\n",
    "    if isinstance(summary_field, str):\n",
    "        text = summary_field.strip()\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        try:\n",
    "            parsed = parse_llm_summary(text)\n",
    "            ov = extract_overview_from_summary(parsed)\n",
    "            if isinstance(ov, str) and ov.strip():\n",
    "                return ov.strip()\n",
    "            for k in [\"Overview\", \"overview\", \"summary\", \"Summary\", \"description\", \"Description\"]:\n",
    "                v = parsed.get(k)\n",
    "                if isinstance(v, str) and v.strip():\n",
    "                    return v.strip()\n",
    "        except Exception:\n",
    "            # last resort: treat whole string as overview\n",
    "            return text\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def compare_description_lengths_by_type(\n",
    "    bucket: str = BUCKET_NAME,\n",
    "    output_prefix: str = OUTPUT_PREFIX,\n",
    "    min_words: int = 3\n",
    ") -> Dict[str, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Returns a dict with three sub-dicts:\n",
    "      {\n",
    "        \"has_existing\": { ... },\n",
    "        \"no_existing\": { ... },\n",
    "        \"overall_generated\": { ... }\n",
    "      }\n",
    "    \"\"\"\n",
    "    results = load_pipeline_results_for_lengths(bucket, output_prefix)\n",
    "\n",
    "    # For has-existing group (baseline vs generated)\n",
    "    has_existing_diffs = []\n",
    "    has_existing_old = []\n",
    "    has_existing_new = []\n",
    "\n",
    "    # For no-existing group (generated only)\n",
    "    no_existing_new = []\n",
    "\n",
    "    # For overall generated lengths (any run_type)\n",
    "    all_generated_new = []\n",
    "\n",
    "    for r in results:\n",
    "        run_type = r.get(\"run_type\", \"\").lower()\n",
    "        existing = r.get(\"existing_overview\")\n",
    "        summary_field = r.get(\"parsed_summary\", None)\n",
    "        if summary_field is None:\n",
    "            summary_field = r.get(\"summary\")\n",
    "\n",
    "        # Always try to get generated overview (for length stats)\n",
    "        gen_overview = _extract_generated_overview_any(summary_field) if summary_field is not None else \"\"\n",
    "        gen_len = len(gen_overview.split()) if gen_overview else 0\n",
    "        if gen_len > 0:\n",
    "            all_generated_new.append(gen_len)\n",
    "\n",
    "        # --- Group: no existing description ---\n",
    "        if run_type == \"no_existing_description\":\n",
    "            if gen_len > 0:\n",
    "                no_existing_new.append(gen_len)\n",
    "            continue\n",
    "\n",
    "        # --- Group: has existing description (only if we actually have a usable baseline) ---\n",
    "        if run_type == \"has_existing_description\":\n",
    "            if not isinstance(existing, str) or not existing.strip():\n",
    "                # no baseline text available, so we treat it as \"generated-only\" in overall stats\n",
    "                continue\n",
    "\n",
    "            old_len = len(existing.split())\n",
    "            if old_len < min_words or gen_len == 0:\n",
    "                continue\n",
    "\n",
    "            pct_change = (gen_len - old_len) / float(old_len) * 100.0\n",
    "\n",
    "            has_existing_old.append(old_len)\n",
    "            has_existing_new.append(gen_len)\n",
    "            has_existing_diffs.append(pct_change)\n",
    "\n",
    "    # Aggregate stats\n",
    "    def _avg(lst):\n",
    "        return sum(lst) / len(lst) if lst else 0.0\n",
    "\n",
    "    stats = {}\n",
    "\n",
    "    # Has-existing group\n",
    "    n_has = len(has_existing_diffs)\n",
    "    stats[\"has_existing\"] = {\n",
    "        \"n_compared\": n_has,\n",
    "        \"avg_existing_len\": _avg(has_existing_old),\n",
    "        \"avg_generated_len\": _avg(has_existing_new),\n",
    "        \"avg_pct_change\": _avg(has_existing_diffs),\n",
    "    }\n",
    "\n",
    "    # No-existing group: only generated length\n",
    "    n_no = len(no_existing_new)\n",
    "    stats[\"no_existing\"] = {\n",
    "        \"n_generated\": n_no,\n",
    "        \"avg_generated_len\": _avg(no_existing_new),\n",
    "    }\n",
    "\n",
    "    # Overall generated (any run type)\n",
    "    n_all = len(all_generated_new)\n",
    "    stats[\"overall_generated\"] = {\n",
    "        \"n_generated\": n_all,\n",
    "        \"avg_generated_len\": _avg(all_generated_new),\n",
    "    }\n",
    "\n",
    "    # Pretty-print\n",
    "    print(\"=== Length comparison by type ===\")\n",
    "    print(f\"[Has existing]    n={n_has}\")\n",
    "    print(f\"  Avg existing length (words):  {_avg(has_existing_old):.2f}\")\n",
    "    print(f\"  Avg generated length (words): {_avg(has_existing_new):.2f}\")\n",
    "    print(f\"  Avg % change in length:       {_avg(has_existing_diffs):+.2f}%\")\n",
    "    print()\n",
    "    print(f\"[No existing]     n_generated={n_no}\")\n",
    "    print(f\"  Avg generated length (words): {_avg(no_existing_new):.2f}\")\n",
    "    print()\n",
    "    print(f\"[Overall]         n_generated={n_all}\")\n",
    "    print(f\"  Avg generated length (words): {_avg(all_generated_new):.2f}\")\n",
    "\n",
    "    return stats\n",
    "\n",
    "\n",
    "# ---- Run grouped comparison ----\n",
    "length_stats_by_type = compare_description_lengths_by_type()\n",
    "length_stats_by_type\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "fJ7vxRaB-CEu",
    "OM9jKlPxZIWU"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
